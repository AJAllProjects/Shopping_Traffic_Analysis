{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-02T16:38:15.294700900Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import plotly.express as px\n",
    "import psutil\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import dask.dataframe as dd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error\n",
    "import lightgbm as lgb\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.spatial import KDTree\n",
    "from prophet import Prophet\n",
    "import polars as pl\n",
    "import time\n",
    "import optuna\n",
    "import itertools\n",
    "from scipy.spatial import ConvexHull\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import calendar\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import traceback\n",
    "import logging\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b76d8de-b11e-487c-af78-694b40af5879",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)  \n",
    "pd.set_option('display.max_colwidth', None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "467e107b8ac1b0ae",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def print_memory_usage():\n",
    "    \"\"\"\n",
    "    Print the current memory usage of the Python process.\n",
    "    Uses psutil if available, otherwise falls back to a more basic method.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        process = psutil.Process()\n",
    "        memory_info = process.memory_info()\n",
    "        memory_mb = memory_info.rss / (1024 * 1024)\n",
    "        print(f\"Current memory usage: {memory_mb:.2f} MB\")\n",
    "    except ImportError:\n",
    "        import sys\n",
    "        memory_mb = sys.getsizeof(locals()) / (1024 * 1024)\n",
    "        print(f\"Approximate memory usage: {memory_mb:.2f} MB (psutil not available for precise measurement)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e74611-2267-4112-8a8c-b052e04de27b",
   "metadata": {},
   "source": [
    "# Data Loading & Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cddf85c16e1ed4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Sales Forecasting Analysis...\n",
      "\n",
      "Loading data...\n",
      "Loaded data: 36550 sales records, 223 stores, 1000 shoppers\n"
     ]
    }
   ],
   "source": [
    "path_sales = \"DataExercise/Sales.csv\"\n",
    "path_store = \"DataExercise/Store.csv\"\n",
    "path_shopper = \"DataExercise/Shopper.csv\"\n",
    "path_loc = \"DataExercise/ShopperLoc.csv\"\n",
    "\n",
    "print(\"Starting Sales Forecasting Analysis...\")\n",
    "\n",
    "print(\"\\nLoading data...\")\n",
    "\n",
    "df_sales = pd.read_csv(path_sales)\n",
    "df_store = pd.read_csv(path_store)\n",
    "df_shopper = pd.read_csv(path_shopper)\n",
    "\n",
    "# For the large ShopperLoc file, we need to use Dask\n",
    "df_loc = dd.read_csv(path_loc, assume_missing=True)\n",
    "\n",
    "print(f\"Loaded data: {len(df_sales)} sales records, {len(df_store)} stores, {len(df_shopper)} shoppers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bb7a2c1-17ac-4999-94d6-e3af3cf7b5dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ShopperID</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-01-01 08:00:00</td>\n",
       "      <td>26405.662417</td>\n",
       "      <td>13325.522001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>339.0</td>\n",
       "      <td>2020-01-01 08:00:00</td>\n",
       "      <td>26429.884756</td>\n",
       "      <td>13328.565292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>339.0</td>\n",
       "      <td>2020-01-01 09:00:00</td>\n",
       "      <td>26420.868732</td>\n",
       "      <td>13311.368993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>624.0</td>\n",
       "      <td>2020-01-01 09:00:00</td>\n",
       "      <td>21698.494509</td>\n",
       "      <td>15084.045257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2020-01-01 10:00:00</td>\n",
       "      <td>26184.784911</td>\n",
       "      <td>13883.214246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ShopperID             Datetime             X             Y\n",
       "0        0.0  2020-01-01 08:00:00  26405.662417  13325.522001\n",
       "1      339.0  2020-01-01 08:00:00  26429.884756  13328.565292\n",
       "2      339.0  2020-01-01 09:00:00  26420.868732  13311.368993\n",
       "3      624.0  2020-01-01 09:00:00  21698.494509  15084.045257\n",
       "4       49.0  2020-01-01 10:00:00  26184.784911  13883.214246"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_loc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "386601ca-a1b0-431d-a58b-3e6fc8999585",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ShopperID</th>\n",
       "      <th>ShopperTypeID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>53</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ShopperID  ShopperTypeID\n",
       "0          0              3\n",
       "1          9              3\n",
       "2         13              3\n",
       "3         18              3\n",
       "4         21              3\n",
       "5         28              3\n",
       "6         31              3\n",
       "7         32              3\n",
       "8         46              3\n",
       "9         53              3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shopper.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d543393-e272-450e-bb43-1cc70d800b5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StoreID</th>\n",
       "      <th>FirmID</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22268.476745</td>\n",
       "      <td>23085.424400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18918.771884</td>\n",
       "      <td>16888.112448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>24665.643151</td>\n",
       "      <td>29919.739741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>27525.639088</td>\n",
       "      <td>17924.360970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>11225.471167</td>\n",
       "      <td>29824.784689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5964.667519</td>\n",
       "      <td>14731.828352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>20326.104450</td>\n",
       "      <td>14524.100305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>15570.504828</td>\n",
       "      <td>26917.750627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>28230.371732</td>\n",
       "      <td>16770.930242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>15376.849412</td>\n",
       "      <td>4165.452042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   StoreID  FirmID             X             Y\n",
       "0        0       0  22268.476745  23085.424400\n",
       "1        1       0  18918.771884  16888.112448\n",
       "2        2       0  24665.643151  29919.739741\n",
       "3        3       0  27525.639088  17924.360970\n",
       "4        4       0  11225.471167  29824.784689\n",
       "5        5       1   5964.667519  14731.828352\n",
       "6        6       1  20326.104450  14524.100305\n",
       "7        7       1  15570.504828  26917.750627\n",
       "8        8       1  28230.371732  16770.930242\n",
       "9        9       1  15376.849412   4165.452042"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_store.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0c8832a-a36d-498e-bec0-9f4735158346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FirmID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2.607345e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>2.256055e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>2.007728e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>3.504672e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>2.634297e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>2.575741e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>2.915166e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>2.039520e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-09</td>\n",
       "      <td>2.586436e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-01-10</td>\n",
       "      <td>3.025750e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FirmID        Date         Sales\n",
       "0       0  2020-01-01  2.607345e+03\n",
       "1       0  2020-01-02  2.256055e+03\n",
       "2       0  2020-01-03  2.007728e+03\n",
       "3       0  2020-01-04  3.504672e+03\n",
       "4       0  2020-01-05  2.634297e+03\n",
       "5       0  2020-01-06  2.575741e+03\n",
       "6       0  2020-01-07  2.915166e+08\n",
       "7       0  2020-01-08  2.039520e+03\n",
       "8       0  2020-01-09  2.586436e+03\n",
       "9       0  2020-01-10  3.025750e+03"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "114a4386-b21b-4d0e-a92d-9b8058db10f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FirmID</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36550.000000</td>\n",
       "      <td>3.515000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24.500000</td>\n",
       "      <td>1.833758e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.431067</td>\n",
       "      <td>6.907250e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.865365e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>7.062059e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24.500000</td>\n",
       "      <td>1.200922e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>1.888080e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>49.000000</td>\n",
       "      <td>6.673288e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             FirmID         Sales\n",
       "count  36550.000000  3.515000e+04\n",
       "mean      24.500000  1.833758e+05\n",
       "std       14.431067  6.907250e+06\n",
       "min        0.000000  3.865365e+01\n",
       "25%       12.000000  7.062059e+02\n",
       "50%       24.500000  1.200922e+03\n",
       "75%       37.000000  1.888080e+03\n",
       "max       49.000000  6.673288e+08"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81795155-e45d-4bbc-8d2e-0f1f8434d40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StoreID</th>\n",
       "      <th>FirmID</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>223.000000</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>223.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>111.000000</td>\n",
       "      <td>24.497758</td>\n",
       "      <td>20015.849371</td>\n",
       "      <td>19588.899424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>64.518731</td>\n",
       "      <td>14.654235</td>\n",
       "      <td>7112.613098</td>\n",
       "      <td>7110.455319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>535.703259</td>\n",
       "      <td>1013.826048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>55.500000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>15192.605148</td>\n",
       "      <td>14701.356380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>111.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>21122.781964</td>\n",
       "      <td>20575.081152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>166.500000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>26300.523429</td>\n",
       "      <td>25794.551801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>222.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>29973.405271</td>\n",
       "      <td>29975.464843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          StoreID      FirmID             X             Y\n",
       "count  223.000000  223.000000    223.000000    223.000000\n",
       "mean   111.000000   24.497758  20015.849371  19588.899424\n",
       "std     64.518731   14.654235   7112.613098   7110.455319\n",
       "min      0.000000    0.000000    535.703259   1013.826048\n",
       "25%     55.500000   12.000000  15192.605148  14701.356380\n",
       "50%    111.000000   24.000000  21122.781964  20575.081152\n",
       "75%    166.500000   37.000000  26300.523429  25794.551801\n",
       "max    222.000000   49.000000  29973.405271  29975.464843"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_store.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "694f0a8b-33e9-4d98-b43b-62f5ff6f3971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ShopperID</th>\n",
       "      <th>ShopperTypeID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>499.500000</td>\n",
       "      <td>1.989000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>288.819436</td>\n",
       "      <td>1.398869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>249.750000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>499.500000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>749.250000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>999.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ShopperID  ShopperTypeID\n",
       "count  1000.000000    1000.000000\n",
       "mean    499.500000       1.989000\n",
       "std     288.819436       1.398869\n",
       "min       0.000000       0.000000\n",
       "25%     249.750000       1.000000\n",
       "50%     499.500000       2.000000\n",
       "75%     749.250000       3.000000\n",
       "max     999.000000       4.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_shopper.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5f157d-dd2f-4f00-a505-465b3ee0e72a",
   "metadata": {},
   "source": [
    "# Clean & Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bda04687-edf0-4cc4-8169-7c6063fbc639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning and exploring data...\n",
      "Found 41 outliers in Sales data\n",
      "Outliers found in 28 firms\n",
      "  Firm 0: Replaced 1 outliers with firm mean: 1997.45\n",
      "  Firm 1: Replaced 2 outliers with firm mean: 1824.54\n",
      "  Firm 2: Replaced 2 outliers with firm mean: 1193.20\n",
      "  Firm 3: Replaced 2 outliers with firm mean: 1631.60\n",
      "  Firm 4: Replaced 2 outliers with firm mean: 1557.47\n",
      "  Firm 5: Replaced 1 outliers with firm mean: 982.83\n",
      "  Firm 6: Replaced 1 outliers with firm mean: 1002.43\n",
      "  Firm 7: Replaced 1 outliers with firm mean: 924.10\n",
      "  Firm 8: Replaced 2 outliers with firm mean: 874.05\n",
      "  Firm 11: Replaced 1 outliers with firm mean: 677.45\n",
      "  Firm 13: Replaced 1 outliers with firm mean: 1124.74\n",
      "  Firm 14: Replaced 1 outliers with firm mean: 735.18\n",
      "  Firm 16: Replaced 2 outliers with firm mean: 5187.06\n",
      "  Firm 18: Replaced 1 outliers with firm mean: 3611.89\n",
      "  Firm 20: Replaced 2 outliers with firm mean: 512.40\n",
      "  Firm 23: Replaced 1 outliers with firm mean: 1776.05\n",
      "  Firm 24: Replaced 1 outliers with firm mean: 1445.58\n",
      "  Firm 27: Replaced 1 outliers with firm mean: 1796.03\n",
      "  Firm 28: Replaced 1 outliers with firm mean: 1643.82\n",
      "  Firm 29: Replaced 1 outliers with firm mean: 1106.41\n",
      "  Firm 30: Replaced 2 outliers with firm mean: 449.09\n",
      "  Firm 32: Replaced 1 outliers with firm mean: 577.08\n",
      "  Firm 37: Replaced 1 outliers with firm mean: 837.66\n",
      "  Firm 38: Replaced 1 outliers with firm mean: 2587.46\n",
      "  Firm 43: Replaced 2 outliers with firm mean: 1133.41\n",
      "  Firm 44: Replaced 5 outliers with firm mean: 1426.70\n",
      "  Firm 45: Replaced 1 outliers with firm mean: 2184.40\n",
      "  Firm 46: Replaced 1 outliers with firm mean: 1380.47\n",
      "Found missing values in Sales data. Backfilling missing values for each firm...\n",
      "Created sample of 783298 shopper locations for radius estimation\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCleaning and exploring data...\")\n",
    "\n",
    "outlier_threshold = 1e6 \n",
    "mask_outlier = df_sales['Sales'] > outlier_threshold\n",
    "\n",
    "if mask_outlier.any():\n",
    "    print(f\"Found {mask_outlier.sum()} outliers in Sales data\")\n",
    "    \n",
    "    firms_with_outliers = df_sales.loc[mask_outlier, 'FirmID'].unique()\n",
    "    print(f\"Outliers found in {len(firms_with_outliers)} firms\")\n",
    "    \n",
    "    # For each firm with outliers, replace with that firm's mean (excluding outliers) as we don't have a fundamental understanding of why these outliers are occurring and if they are event driven or anything along those lines given the nature of the data and given the size of the data relative to outliers, thresholding by a hrad cap shouldn't introduce any data issues\n",
    "    for firm in firms_with_outliers:\n",
    "        firm_mask = df_sales['FirmID'] == firm\n",
    "        firm_outlier_mask = firm_mask & mask_outlier\n",
    "        \n",
    "        firm_mean = df_sales.loc[firm_mask & ~mask_outlier, 'Sales'].mean()\n",
    "        \n",
    "        if pd.isna(firm_mean):\n",
    "            firm_mean = df_sales.loc[~mask_outlier, 'Sales'].median()\n",
    "            print(f\"  Firm {firm} has all outliers, using global median: {firm_mean:.2f}\")\n",
    "        else:\n",
    "            print(f\"  Firm {firm}: Replaced {firm_outlier_mask.sum()} outliers with firm mean: {firm_mean:.2f}\")\n",
    "        \n",
    "        df_sales.loc[firm_outlier_mask, 'Sales'] = firm_mean\n",
    "\n",
    "# Test Linear Imputation for outlier removal\n",
    "\n",
    "# if not pd.api.types.is_datetime64_dtype(df_sales['Date']):\n",
    "#     print(\"Converting Date column to datetime format...\")\n",
    "#     df_sales['Date'] = pd.to_datetime(df_sales['Date'])\n",
    "\n",
    "# if df_sales['Sales'].isna().any():\n",
    "#     print(\"Found missing values in Sales data. Performing backwards linear imputation...\")\n",
    "    \n",
    "#     df_sales_imputed = df_sales.copy()\n",
    "    \n",
    "#     for firm_id in df_sales['FirmID'].unique():\n",
    "#         firm_data = df_sales[df_sales['FirmID'] == firm_id].copy().sort_values('Date')\n",
    "        \n",
    "#         if not firm_data['Sales'].isna().any():\n",
    "#             continue\n",
    "            \n",
    "#         if firm_data['Sales'].notna().sum() < 2:\n",
    "#             print(f\"  Skipping Firm {firm_id}: Not enough non-missing values for imputation\")\n",
    "#             continue\n",
    "            \n",
    "#         firm_data['DateNum'] = (firm_data['Date'] - firm_data['Date'].min()).dt.days\n",
    "        \n",
    "#         missing_indices = firm_data.index[firm_data['Sales'].isna()]\n",
    "#         for idx in missing_indices:\n",
    "#             missing_date = firm_data.loc[idx, 'Date']\n",
    "#             missing_datenum = firm_data.loc[idx, 'DateNum']\n",
    "            \n",
    "#             future_data = firm_data[(firm_data['Date'] > missing_date) & \n",
    "#                                    (firm_data['Sales'].notna())].copy()\n",
    "            \n",
    "#             if len(future_data) >= 2:\n",
    "                \n",
    "#                 model = LinearRegression()\n",
    "#                 X = future_data[['DateNum']].values\n",
    "#                 y = future_data['Sales'].values\n",
    "#                 model.fit(X, y)\n",
    "                \n",
    "#                 imputed_value = model.predict([[missing_datenum]])[0]\n",
    "                \n",
    "#                 imputed_value = max(0, imputed_value)\n",
    "                \n",
    "#                 df_sales_imputed.loc[idx, 'Sales'] = imputed_value\n",
    "                \n",
    "#             else:\n",
    "#                 next_valid = firm_data[firm_data['Date'] > missing_date]['Sales'].first_valid_index()\n",
    "#                 if next_valid is not None:\n",
    "#                     df_sales_imputed.loc[idx, 'Sales'] = firm_data.loc[next_valid, 'Sales']\n",
    "                    \n",
    "#         print(f\"  Imputed {firm_data['Sales'].isna().sum()} values for Firm {firm_id}\")\n",
    "    \n",
    "#     df_sales = df_sales_imputed\n",
    "    \n",
    "#     remaining_na = df_sales['Sales'].isna().sum()\n",
    "#     if remaining_na > 0:\n",
    "#         print(f\"Warning: {remaining_na} Sales values still missing after imputation\")\n",
    "#     else:\n",
    "#         print(\"Successfully imputed all missing Sales values\")\n",
    "\n",
    "if df_sales['Sales'].isna().any():\n",
    "    print(\"Found missing values in Sales data. Backfilling missing values for each firm...\")\n",
    "    df_sales['Sales'] = df_sales.groupby('FirmID')['Sales'].transform(lambda x: x.backfill())\n",
    "\n",
    "sample_fraction = 0.1\n",
    "df_loc_sample = df_loc.sample(frac=sample_fraction, random_state=42).compute()\n",
    "print(f\"Created sample of {len(df_loc_sample)} shopper locations for radius estimation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f31295a2-4c72-4b36-9b10-90fd1bbaf2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ShopperID</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1093580</th>\n",
       "      <td>890.0</td>\n",
       "      <td>2020-04-11 17:00:00</td>\n",
       "      <td>27311.959048</td>\n",
       "      <td>7106.265198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730008</th>\n",
       "      <td>682.0</td>\n",
       "      <td>2021-09-04 14:00:00</td>\n",
       "      <td>18152.418106</td>\n",
       "      <td>18755.832178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99204</th>\n",
       "      <td>680.0</td>\n",
       "      <td>2021-07-19 13:00:00</td>\n",
       "      <td>20843.511437</td>\n",
       "      <td>19965.719211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827740</th>\n",
       "      <td>393.0</td>\n",
       "      <td>2021-08-20 15:00:00</td>\n",
       "      <td>27180.620774</td>\n",
       "      <td>17067.426915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881384</th>\n",
       "      <td>663.0</td>\n",
       "      <td>2021-04-26 14:00:00</td>\n",
       "      <td>10407.426732</td>\n",
       "      <td>21386.226674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ShopperID             Datetime             X             Y\n",
       "1093580      890.0  2020-04-11 17:00:00  27311.959048   7106.265198\n",
       "730008       682.0  2021-09-04 14:00:00  18152.418106  18755.832178\n",
       "99204        680.0  2021-07-19 13:00:00  20843.511437  19965.719211\n",
       "827740       393.0  2021-08-20 15:00:00  27180.620774  17067.426915\n",
       "881384       663.0  2021-04-26 14:00:00  10407.426732  21386.226674"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_loc_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "126603b3-85b9-489d-85ce-d3b71dca34f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ShopperID</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>783298.000000</td>\n",
       "      <td>783298.000000</td>\n",
       "      <td>783298.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>499.187413</td>\n",
       "      <td>19885.789462</td>\n",
       "      <td>19445.009704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>288.404488</td>\n",
       "      <td>6594.359258</td>\n",
       "      <td>6574.795435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>525.310772</td>\n",
       "      <td>1001.564195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>250.000000</td>\n",
       "      <td>15575.173862</td>\n",
       "      <td>14815.212758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>499.000000</td>\n",
       "      <td>20305.875021</td>\n",
       "      <td>20013.454095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>749.000000</td>\n",
       "      <td>25464.963663</td>\n",
       "      <td>24717.005449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>999.000000</td>\n",
       "      <td>29990.209250</td>\n",
       "      <td>30001.766046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ShopperID              X              Y\n",
       "count  783298.000000  783298.000000  783298.000000\n",
       "mean      499.187413   19885.789462   19445.009704\n",
       "std       288.404488    6594.359258    6574.795435\n",
       "min         0.000000     525.310772    1001.564195\n",
       "25%       250.000000   15575.173862   14815.212758\n",
       "50%       499.000000   20305.875021   20013.454095\n",
       "75%       749.000000   25464.963663   24717.005449\n",
       "max       999.000000   29990.209250   30001.766046"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_loc_sample.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7a08f4-295b-439d-980a-69595e9284d3",
   "metadata": {},
   "source": [
    "# Data-Driven Store Radius Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57bc88c5-0872-4411-8874-169a764eaac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimating store radii...\n",
      "Estimating store radii using percentile and density-based methods...\n",
      "Store 0.0: Percentile radius = 8.53, Density radius = 7.65\n",
      "Store 1.0: Percentile radius = 8.01, Density radius = 7.12\n",
      "Store 2.0: Percentile radius = 13.15, Density radius = 13.03\n",
      "Store 3.0: Percentile radius = 24.41, Density radius = 23.93\n",
      "Store 4.0: Percentile radius = 24.17, Density radius = 23.81\n",
      "Store 20.0: Percentile radius = 16.52, Density radius = 16.35\n",
      "Store 40.0: Percentile radius = 11.73, Density radius = 10.70\n",
      "Store 60.0: Percentile radius = 12.82, Density radius = 11.46\n",
      "Store 80.0: Percentile radius = 99.91, Density radius = 9.43\n",
      "Store 100.0: Percentile radius = 23.54, Density radius = 23.05\n",
      "Store 120.0: Percentile radius = 21.77, Density radius = 21.24\n",
      "Store 140.0: Percentile radius = 96.82, Density radius = 9.24\n",
      "Store 160.0: Percentile radius = 20.49, Density radius = 20.29\n",
      "Store 180.0: Percentile radius = 8.32, Density radius = 7.91\n",
      "Store 200.0: Percentile radius = 16.66, Density radius = 16.45\n",
      "Store 220.0: Percentile radius = 90.60, Density radius = 23.10\n",
      "Completed radius estimation. Average radius: 17.78\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEstimating store radii...\")\n",
    "\n",
    "def estimate_radius_percentile(store_center, candidate_points, percentile=95):\n",
    "    \"\"\"\n",
    "    Estimate the store's radius using the percentile method.\n",
    "    \n",
    "    Args:\n",
    "        store_center: Tuple or array (x, y) of the store center\n",
    "        candidate_points: np.array of shape (n_points, 2) containing shopper coordinates\n",
    "        percentile: Percentile to use (e.g., 90 or 95)\n",
    "        \n",
    "    Returns:\n",
    "        radius_est: The estimated radius\n",
    "    \"\"\"\n",
    "    if len(candidate_points) == 0:\n",
    "        return 15.0 \n",
    "        \n",
    "    distances = np.linalg.norm(candidate_points - np.array(store_center), axis=1)\n",
    "    radius_est = np.percentile(distances, percentile)\n",
    "    return max(radius_est, 5.0)\n",
    "\n",
    "def estimate_radius_density(store_center, candidate_points, percentile=95, eps=10, min_samples=5):\n",
    "    \"\"\"\n",
    "    Estimate the store's radius using a density/clustering approach.\n",
    "    \n",
    "    This function uses DBSCAN to cluster candidate points. It then selects\n",
    "    the cluster that is most likely to represent the \"core\" of the store traffic\n",
    "    (the largest cluster) and computes the given percentile of distances\n",
    "    within that cluster.\n",
    "    \n",
    "    Args:\n",
    "        store_center: Tuple or array (x, y) of the store center\n",
    "        candidate_points: np.array of shape (n_points, 2) containing shopper coordinates\n",
    "        percentile: Percentile to use (e.g., 90 or 95)\n",
    "        eps: The maximum distance between two samples for DBSCAN clustering\n",
    "        min_samples: The number of samples in a neighborhood for a core point in DBSCAN\n",
    "    \n",
    "    Returns:\n",
    "        radius_est: The estimated radius based on the dense cluster\n",
    "    \"\"\"\n",
    "    if len(candidate_points) < min_samples + 1:\n",
    "        return estimate_radius_percentile(store_center, candidate_points, percentile)\n",
    "\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = db.fit_predict(candidate_points)\n",
    "    \n",
    "    if np.all(labels == -1):\n",
    "        return estimate_radius_percentile(store_center, candidate_points, percentile)\n",
    "    \n",
    "    unique_labels = np.unique(labels[labels != -1])\n",
    "    if len(unique_labels) == 0:\n",
    "        return estimate_radius_percentile(store_center, candidate_points, percentile)\n",
    "        \n",
    "    counts = np.array([np.sum(labels == label) for label in unique_labels])\n",
    "    core_label = unique_labels[np.argmax(counts)]\n",
    "    \n",
    "    core_points = candidate_points[labels == core_label]\n",
    "    \n",
    "    distances = np.linalg.norm(core_points - np.array(store_center), axis=1)\n",
    "    radius_est = np.percentile(distances, percentile)\n",
    "    return max(radius_est, 5.0)\n",
    "\n",
    "CAPTURE_RADIUS = 200.0 \n",
    "store_radius_percentile = {}\n",
    "store_radius_density = {}\n",
    "\n",
    "print(\"Estimating store radii using percentile and density-based methods...\")\n",
    "\n",
    "for idx, store in df_store.iterrows():\n",
    "    store_id = store['StoreID']\n",
    "    store_center = np.array([store['X'], store['Y']])\n",
    "    \n",
    "    dists = np.sqrt(((df_loc_sample[['X', 'Y']].values - store_center)**2).sum(axis=1))\n",
    "    candidate_points = df_loc_sample[['X', 'Y']].values[dists <= CAPTURE_RADIUS]\n",
    "    \n",
    "    if len(candidate_points) > 0:\n",
    "        radius_p = estimate_radius_percentile(store_center, candidate_points, percentile=95)\n",
    "        \n",
    "        if len(candidate_points) > 100:\n",
    "            eps = 5\n",
    "            min_samples = 10\n",
    "        else:\n",
    "            eps = 10\n",
    "            min_samples = 5\n",
    "            \n",
    "        radius_d = estimate_radius_density(\n",
    "            store_center, candidate_points, percentile=95, \n",
    "            eps=eps, min_samples=min_samples\n",
    "        )\n",
    "    else:\n",
    "        radius_p = radius_d = 15.0\n",
    "    \n",
    "    store_radius_percentile[store_id] = radius_p\n",
    "    store_radius_density[store_id] = radius_d\n",
    "    \n",
    "    if idx < 5 or idx % 20 == 0:\n",
    "        print(f\"Store {store_id}: Percentile radius = {radius_p:.2f}, Density radius = {radius_d:.2f}\")\n",
    "\n",
    "store_radius_dict = {}\n",
    "for store_id in df_store['StoreID']:\n",
    "    r_p = store_radius_percentile[store_id]\n",
    "    r_d = store_radius_density[store_id]\n",
    "    \n",
    "    if r_d < 0.7 * r_p:\n",
    "        store_radius_dict[store_id] = r_d\n",
    "    elif r_p < 0.7 * r_d:\n",
    "        store_radius_dict[store_id] = r_p\n",
    "    else:\n",
    "        store_radius_dict[store_id] = (r_p + r_d) / 2\n",
    "\n",
    "df_store['EstimatedRadius'] = df_store['StoreID'].map(store_radius_dict)\n",
    "\n",
    "df_store['EstimatedRadius'] = df_store['EstimatedRadius'].fillna(15.0)\n",
    "\n",
    "print(f\"Completed radius estimation. Average radius: {df_store['EstimatedRadius'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7860cbc-9360-43c4-b316-ca37d8ab7b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created combined visualization of all stores and shopper points.\n",
      "Features of the visualization:\n",
      "- Red stars represent store locations\n",
      "- Red circles show the estimated radius for each store\n",
      "- Light blue points show sampled shopper locations\n",
      "- Blue heatmap indicates areas of high shopper density\n",
      "- Orange dashed line shows the overall coverage area of all stores\n",
      "- Interactive buttons allow toggling different elements\n",
      "- Hover over stores to see details including Store ID and radius\n",
      "- Visualization saved to 'output/all_stores_and_shoppers.html'\n",
      "\n",
      "Also created visualization by firm:\n",
      "- Stores and their radii are color-coded by firm\n",
      "- Solid colored lines show the territory covered by each firm\n",
      "- Visualization saved to 'output/stores_by_firm.html'\n"
     ]
    }
   ],
   "source": [
    "def create_combined_visualization(store_df, shopper_sample_df):\n",
    "    \"\"\"\n",
    "    Create a single plot showing all stores, their estimated radii, and shopper points.\n",
    "    \n",
    "    Args:\n",
    "        store_df: DataFrame with store information including coordinates and radii\n",
    "        shopper_sample_df: DataFrame with shopper coordinates\n",
    "    \n",
    "    Returns:\n",
    "        Plotly figure object\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=shopper_sample_df['X'],\n",
    "            y=shopper_sample_df['Y'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                color='lightblue',\n",
    "                size=3,\n",
    "                opacity=0.3\n",
    "            ),\n",
    "            name='Shopper Locations'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    for _, store in store_df.iterrows():\n",
    "        store_x, store_y = store['X'], store['Y']\n",
    "        store_radius = store['EstimatedRadius']\n",
    "        store_id = store['StoreID']\n",
    "        firm_id = store['FirmID'] if 'FirmID' in store else None\n",
    "        \n",
    "        theta = np.linspace(0, 2*np.pi, 100)\n",
    "        circle_x = store_radius * np.cos(theta) + store_x\n",
    "        circle_y = store_radius * np.sin(theta) + store_y\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=circle_x,\n",
    "                y=circle_y,\n",
    "                mode='lines',\n",
    "                line=dict(color='rgba(255, 0, 0, 0.5)', width=1),\n",
    "                name=f'Store {store_id} Radius',\n",
    "                legendgroup=f'store_{store_id}',\n",
    "                showlegend=False,\n",
    "                hoverinfo='skip'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        hover_text = f'Store ID: {store_id}'\n",
    "        if firm_id is not None:\n",
    "            hover_text += f'<br>Firm ID: {firm_id}'\n",
    "        hover_text += f'<br>Radius: {store_radius:.2f}'\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[store_x],\n",
    "                y=[store_y],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    color='red',\n",
    "                    size=8,\n",
    "                    symbol='star'\n",
    "                ),\n",
    "                name=f'Store {store_id}',\n",
    "                legendgroup=f'store_{store_id}',\n",
    "                showlegend=True,\n",
    "                hovertext=hover_text,\n",
    "                hoverinfo='text'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        store_points = store_df[['X', 'Y']].values\n",
    "        if len(store_points) >= 3: \n",
    "            hull = ConvexHull(store_points)\n",
    "            hull_x = store_points[hull.vertices, 0].tolist() + [store_points[hull.vertices[0], 0]]\n",
    "            hull_y = store_points[hull.vertices, 1].tolist() + [store_points[hull.vertices[0], 1]]\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=hull_x,\n",
    "                    y=hull_y,\n",
    "                    fill='toself',\n",
    "                    fillcolor='rgba(255, 165, 0, 0.1)',\n",
    "                    line=dict(color='orange', width=2, dash='dash'),\n",
    "                    name='Store Coverage Area'\n",
    "                )\n",
    "            )\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Histogram2dContour(\n",
    "            x=shopper_sample_df['X'],\n",
    "            y=shopper_sample_df['Y'],\n",
    "            colorscale='Blues',\n",
    "            opacity=0.3,\n",
    "            showscale=False,\n",
    "            hoverinfo='skip',\n",
    "            name='Shopper Density'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Store Locations, Radii, and Shopper Distribution',\n",
    "        xaxis_title='X Coordinate',\n",
    "        yaxis_title='Y Coordinate',\n",
    "        legend_title='Legend',\n",
    "        height=800,\n",
    "        width=1000,\n",
    "        hovermode='closest',\n",
    "        showlegend=True,\n",
    "        yaxis=dict(\n",
    "            scaleanchor=\"x\",\n",
    "            scaleratio=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    updatemenus = [\n",
    "        dict(\n",
    "            type=\"buttons\",\n",
    "            direction=\"left\",\n",
    "            buttons=[\n",
    "                dict(\n",
    "                    args=[{\"visible\": [True, True, True, True]},\n",
    "                          {\"title\": \"All Elements Visible\"}],\n",
    "                    label=\"Show All\",\n",
    "                    method=\"update\"\n",
    "                ),\n",
    "                dict(\n",
    "                    args=[{\"visible\": [False, True, True, False]},\n",
    "                          {\"title\": \"Stores and Radii Only\"}],\n",
    "                    label=\"Stores Only\",\n",
    "                    method=\"update\"\n",
    "                ),\n",
    "                dict(\n",
    "                    args=[{\"visible\": [True, False, False, False]},\n",
    "                          {\"title\": \"Shoppers Only\"}],\n",
    "                    label=\"Shoppers Only\",\n",
    "                    method=\"update\"\n",
    "                ),\n",
    "                dict(\n",
    "                    args=[{\"visible\": [True, True, True, False]},\n",
    "                          {\"title\": \"Hide Density Heatmap\"}],\n",
    "                    label=\"Hide Heatmap\",\n",
    "                    method=\"update\"\n",
    "                )\n",
    "            ],\n",
    "            pad={\"r\": 10, \"t\": 10},\n",
    "            showactive=True,\n",
    "            x=0.11,\n",
    "            xanchor=\"left\",\n",
    "            y=1.1,\n",
    "            yanchor=\"top\"\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    fig.update_layout(updatemenus=updatemenus)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_firms_visualization(store_df, shopper_sample_df):\n",
    "    \"\"\"\n",
    "    Create a visualization color-coded by firm, showing stores and their radii.\n",
    "    \n",
    "    Args:\n",
    "        store_df: DataFrame with store information including coordinates, radii, and firm IDs\n",
    "        shopper_sample_df: DataFrame with shopper coordinates\n",
    "    \n",
    "    Returns:\n",
    "        Plotly figure object\n",
    "    \"\"\"\n",
    "    if 'FirmID' not in store_df.columns:\n",
    "        return None\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=shopper_sample_df['X'],\n",
    "            y=shopper_sample_df['Y'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                color='lightgray',\n",
    "                size=3,\n",
    "                opacity=0.2\n",
    "            ),\n",
    "            name='Shopper Locations'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    firms = store_df['FirmID'].unique()\n",
    "    \n",
    "    colors = px.colors.qualitative.Plotly\n",
    "    \n",
    "    for firm_idx, firm_id in enumerate(firms):\n",
    "        firm_color = colors[firm_idx % len(colors)]\n",
    "        firm_stores = store_df[store_df['FirmID'] == firm_id]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=firm_stores['X'],\n",
    "                y=firm_stores['Y'],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    color=firm_color,\n",
    "                    size=10,\n",
    "                    symbol='star',\n",
    "                    line=dict(width=1, color='black')\n",
    "                ),\n",
    "                name=f'Firm {firm_id}',\n",
    "                legendgroup=f'firm_{firm_id}',\n",
    "                text=[f'Store {s}<br>Firm {f}<br>Radius: {r:.2f}' \n",
    "                      for s, f, r in zip(firm_stores['StoreID'], firm_stores['FirmID'], firm_stores['EstimatedRadius'])],\n",
    "                hoverinfo='text'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        for _, store in firm_stores.iterrows():\n",
    "            store_x, store_y = store['X'], store['Y']\n",
    "            store_radius = store['EstimatedRadius']\n",
    "            \n",
    "            theta = np.linspace(0, 2*np.pi, 100)\n",
    "            circle_x = store_radius * np.cos(theta) + store_x\n",
    "            circle_y = store_radius * np.sin(theta) + store_y\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=circle_x,\n",
    "                    y=circle_y,\n",
    "                    mode='lines',\n",
    "                    line=dict(color=firm_color, width=1, dash='dot'),\n",
    "                    name=f'Firm {firm_id} Radii',\n",
    "                    legendgroup=f'firm_{firm_id}',\n",
    "                    showlegend=False,\n",
    "                    hoverinfo='skip'\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            firm_points = firm_stores[['X', 'Y']].values\n",
    "            if len(firm_points) >= 3: \n",
    "                hull = ConvexHull(firm_points)\n",
    "                hull_x = firm_points[hull.vertices, 0].tolist() + [firm_points[hull.vertices[0], 0]]\n",
    "                hull_y = firm_points[hull.vertices, 1].tolist() + [firm_points[hull.vertices[0], 1]]\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=hull_x,\n",
    "                        y=hull_y,\n",
    "                        fill='toself',\n",
    "                        fillcolor=f'rgba{tuple(int(firm_color.lstrip(\"#\")[i:i+2], 16) for i in (0, 2, 4)) + (0.1,)}',\n",
    "                        line=dict(color=firm_color, width=2),\n",
    "                        name=f'Firm {firm_id} Area',\n",
    "                        legendgroup=f'firm_{firm_id}',\n",
    "                        showlegend=True\n",
    "                    )\n",
    "                )\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Stores and Estimated Radii by Firm',\n",
    "        xaxis_title='X Coordinate',\n",
    "        yaxis_title='Y Coordinate',\n",
    "        legend_title='Firms',\n",
    "        height=800,\n",
    "        width=1000,\n",
    "        hovermode='closest',\n",
    "        yaxis=dict(\n",
    "            scaleanchor=\"x\",\n",
    "            scaleratio=1\n",
    "        ),\n",
    "        legend=dict(\n",
    "            groupclick=\"toggleitem\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "combined_fig = create_combined_visualization(df_store, df_loc_sample)\n",
    "firms_fig = create_firms_visualization(df_store, df_loc_sample)\n",
    "\n",
    "combined_fig.write_html('all_stores_and_shoppers.html')\n",
    "if firms_fig is not None:\n",
    "    firms_fig.write_html('stores_by_firm.html')\n",
    "\n",
    "print(\"Created combined visualization of all stores and shopper points.\")\n",
    "print(\"Features of the visualization:\")\n",
    "print(\"- Red stars represent store locations\")\n",
    "print(\"- Red circles show the estimated radius for each store\")\n",
    "print(\"- Light blue points show sampled shopper locations\")\n",
    "print(\"- Blue heatmap indicates areas of high shopper density\")\n",
    "print(\"- Orange dashed line shows the overall coverage area of all stores\")\n",
    "print(\"- Interactive buttons allow toggling different elements\")\n",
    "print(\"- Hover over stores to see details including Store ID and radius\")\n",
    "print(\"- Visualization saved to 'output/all_stores_and_shoppers.html'\")\n",
    "\n",
    "if firms_fig is not None:\n",
    "    print(\"\\nAlso created visualization by firm:\")\n",
    "    print(\"- Stores and their radii are color-coded by firm\")\n",
    "    print(\"- Solid colored lines show the territory covered by each firm\")\n",
    "    print(\"- Visualization saved to 'output/stores_by_firm.html'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab880b6f-c4d8-4349-a709-a57e61dc4a91",
   "metadata": {},
   "source": [
    "# Match Customers to Stores - Chunked Processing with Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0385c000-222d-4a12-a568-dab3264990d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matching shoppers to stores using full dataset...\n",
      "Converting store data to Polars format...\n",
      "Using single store assignment method\n",
      "Loading full ShopperLoc data file...\n",
      "Data prepared for streaming in 0.01 seconds\n",
      "Processing approximately 7,833,000 shopper records in chunks\n",
      "Processing in smaller chunks for memory efficiency\n",
      "Collecting data into memory - this might take some time for large datasets...\n",
      "Data collected, total rows: 7833000\n",
      "Processing chunk 1 with 500,000 rows (total: 500,000), memory: 933.95 MB\n",
      "  Chunk 1 processed in 20.49 seconds (24398.6 rows/second)\n",
      "Processing chunk 2 with 500,000 rows (total: 1,000,000), memory: 1095.68 MB\n",
      "  Chunk 2 processed in 22.56 seconds (22164.2 rows/second)\n",
      "Processing chunk 3 with 500,000 rows (total: 1,500,000), memory: 1095.22 MB\n",
      "  Chunk 3 processed in 16.84 seconds (29683.8 rows/second)\n",
      "Processing chunk 4 with 500,000 rows (total: 2,000,000), memory: 1156.27 MB\n",
      "  Chunk 4 processed in 14.42 seconds (34665.3 rows/second)\n",
      "Processing chunk 5 with 500,000 rows (total: 2,500,000), memory: 931.45 MB\n",
      "  Chunk 5 processed in 14.79 seconds (33806.3 rows/second)\n",
      "  Saved intermediate results, memory: 975.33 MB\n",
      "Processing chunk 6 with 500,000 rows (total: 3,000,000), memory: 975.34 MB\n",
      "  Chunk 6 processed in 14.27 seconds (35040.9 rows/second)\n",
      "Processing chunk 7 with 500,000 rows (total: 3,500,000), memory: 1013.13 MB\n",
      "  Chunk 7 processed in 14.22 seconds (35173.4 rows/second)\n",
      "Processing chunk 8 with 500,000 rows (total: 4,000,000), memory: 546.07 MB\n",
      "  Chunk 8 processed in 14.02 seconds (35653.5 rows/second)\n",
      "Processing chunk 9 with 500,000 rows (total: 4,500,000), memory: 603.91 MB\n",
      "  Chunk 9 processed in 14.03 seconds (35632.4 rows/second)\n",
      "Processing chunk 10 with 500,000 rows (total: 5,000,000), memory: 657.17 MB\n",
      "  Chunk 10 processed in 13.80 seconds (36230.5 rows/second)\n",
      "  Saved intermediate results, memory: 719.03 MB\n",
      "Processing chunk 11 with 500,000 rows (total: 5,500,000), memory: 719.03 MB\n",
      "  Chunk 11 processed in 14.17 seconds (35282.2 rows/second)\n",
      "Processing chunk 12 with 500,000 rows (total: 6,000,000), memory: 774.53 MB\n",
      "  Chunk 12 processed in 14.01 seconds (35682.2 rows/second)\n",
      "Processing chunk 13 with 500,000 rows (total: 6,500,000), memory: 830.40 MB\n",
      "  Chunk 13 processed in 14.15 seconds (35332.2 rows/second)\n",
      "Processing chunk 14 with 500,000 rows (total: 7,000,000), memory: 886.96 MB\n",
      "  Chunk 14 processed in 13.76 seconds (36340.1 rows/second)\n",
      "Processing chunk 15 with 500,000 rows (total: 7,500,000), memory: 946.13 MB\n",
      "  Chunk 15 processed in 13.92 seconds (35930.8 rows/second)\n",
      "  Saved intermediate results, memory: 1001.94 MB\n",
      "Processing chunk 16 with 333,000 rows (total: 7,833,000), memory: 1001.94 MB\n",
      "  Chunk 16 processed in 9.24 seconds (36041.3 rows/second)\n",
      "\n",
      "Assignment complete!\n",
      "Processed 7,833,000 shopper records in 239.90 seconds\n",
      "Average processing speed: 32651.0 records/second\n",
      "Converting results to Pandas for further processing...\n",
      "\n",
      "Assignment statistics:\n",
      "  Total shopper locations: 7,833,000\n",
      "  Assigned to stores: 5,675,845 (72.46%)\n",
      "  Unassigned: 2,157,155 (27.54%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMatching shoppers to stores using full dataset...\")\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Return the memory usage in a human-readable format.\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem = process.memory_info().rss / 1024 / 1024 \n",
    "    return f\"{mem:.2f} MB\"\n",
    "\n",
    "def process_shopper_chunk(shopper_chunk, store_df, tree, store_centers, store_ids, store_radii, \n",
    "                         multiple_assignment=False):\n",
    "    \"\"\"\n",
    "    Process a chunk of shopper data to assign to stores.\n",
    "    \n",
    "    Args:\n",
    "        shopper_chunk: Chunk of shopper data as a Polars DataFrame\n",
    "        store_df: DataFrame with store information\n",
    "        tree: KDTree for store locations\n",
    "        store_centers: Numpy array of store coordinates\n",
    "        store_ids: Array of store IDs\n",
    "        store_radii: Array of store radii\n",
    "        multiple_assignment: Whether to assign shoppers to multiple stores\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with assignments\n",
    "    \"\"\"\n",
    "    shopper_ids = shopper_chunk.get_column(\"ShopperID\").to_numpy()\n",
    "    datetimes = shopper_chunk.get_column(\"Datetime\").to_numpy()\n",
    "\n",
    "    x_coords = shopper_chunk.get_column(\"X\").to_numpy()\n",
    "    y_coords = shopper_chunk.get_column(\"Y\").to_numpy()\n",
    "    shopper_points = np.column_stack((x_coords, y_coords))\n",
    "    \n",
    "    max_radius = np.max(store_radii)\n",
    "    \n",
    "    assignments = []\n",
    "    \n",
    "    for i in range(len(shopper_points)):\n",
    "        nearby_indices = tree.query_ball_point(shopper_points[i], max_radius)\n",
    "        \n",
    "        if len(nearby_indices) == 0:\n",
    "            assignments.append({\n",
    "                'ShopperID': shopper_ids[i],\n",
    "                'Datetime': datetimes[i],\n",
    "                'StoreID': None,\n",
    "                'Distance': None\n",
    "            })\n",
    "            continue\n",
    "            \n",
    "        nearby_centers = store_centers[nearby_indices]\n",
    "        distances = np.linalg.norm(shopper_points[i] - nearby_centers, axis=1)\n",
    "        \n",
    "        nearby_radii = store_radii[nearby_indices]\n",
    "        valid_mask = distances <= nearby_radii\n",
    "        \n",
    "        if not np.any(valid_mask):\n",
    "            assignments.append({\n",
    "                'ShopperID': shopper_ids[i],\n",
    "                'Datetime': datetimes[i],\n",
    "                'StoreID': None,\n",
    "                'Distance': None\n",
    "            })\n",
    "        else:\n",
    "            valid_indices = np.array(nearby_indices)[valid_mask]\n",
    "            valid_distances = distances[valid_mask]\n",
    "            valid_store_ids = store_ids[valid_indices]\n",
    "            \n",
    "            if multiple_assignment:\n",
    "                for j in range(len(valid_store_ids)):\n",
    "                    assignments.append({\n",
    "                        'ShopperID': shopper_ids[i],\n",
    "                        'Datetime': datetimes[i],\n",
    "                        'StoreID': valid_store_ids[j],\n",
    "                        'Distance': valid_distances[j]\n",
    "                    })\n",
    "            else:\n",
    "                min_idx = np.argmin(valid_distances)\n",
    "                assignments.append({\n",
    "                    'ShopperID': shopper_ids[i],\n",
    "                    'Datetime': datetimes[i],\n",
    "                    'StoreID': valid_store_ids[min_idx],\n",
    "                    'Distance': valid_distances[min_idx]\n",
    "                })\n",
    "    \n",
    "    return pl.DataFrame(assignments)\n",
    "\n",
    "def assign_full_dataset(store_df, shopper_lazyframe, multiple_assignment=False, chunk_size=500000):\n",
    "    \"\"\"\n",
    "    Process the full dataset in chunks using Polars.\n",
    "    \n",
    "    Args:\n",
    "        store_df: Polars DataFrame with store information\n",
    "        shopper_lazyframe: Polars LazyFrame for shopper data\n",
    "        multiple_assignment: Whether to assign to multiple stores\n",
    "        chunk_size: Size of chunks to process\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with all assignments\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    store_centers = store_df.select(['X', 'Y']).to_numpy()\n",
    "    store_ids = store_df.select('StoreID').to_numpy().flatten()\n",
    "    store_radii = store_df.select('EstimatedRadius').to_numpy().flatten()\n",
    "    \n",
    "    tree = KDTree(store_centers)\n",
    "    \n",
    "    try:\n",
    "        total_rows = shopper_lazyframe.select(pl.count()).collect().item()\n",
    "        print(f\"Processing approximately {total_rows:,} shopper records in chunks\")\n",
    "    except:\n",
    "        print(f\"Processing unknown number of shopper records in chunks\")\n",
    "        total_rows = None\n",
    "    \n",
    "    all_assignments = []\n",
    "    processed_rows = 0\n",
    "    \n",
    "    chunk_size = min(chunk_size, 500000)\n",
    "    print(f\"Processing in smaller chunks for memory efficiency\")\n",
    "    \n",
    "    print(\"Collecting data into memory - this might take some time for large datasets...\")\n",
    "    \n",
    "    try:\n",
    "        full_df = shopper_lazyframe.collect()\n",
    "        print(f\"Data collected, total rows: {len(full_df)}\")\n",
    "        \n",
    "        total_rows = len(full_df)\n",
    "        for i in range(0, total_rows, chunk_size):\n",
    "            chunk_start_time = time.time()\n",
    "            \n",
    "            end_idx = min(i + chunk_size, total_rows)\n",
    "            chunk = full_df.slice(i, end_idx - i)\n",
    "            chunk_size_actual = len(chunk)\n",
    "            processed_rows += chunk_size_actual\n",
    "            \n",
    "            print(f\"Processing chunk {i//chunk_size + 1} with {chunk_size_actual:,} rows (total: {processed_rows:,}), memory: {get_memory_usage()}\")\n",
    "            \n",
    "            assignments = process_shopper_chunk(\n",
    "                chunk, store_df, tree, store_centers, store_ids, store_radii, \n",
    "                multiple_assignment=multiple_assignment\n",
    "            )\n",
    "\n",
    "            all_assignments.append(assignments)\n",
    "            \n",
    "            chunk_time = time.time() - chunk_start_time\n",
    "            print(f\"  Chunk {i//chunk_size + 1} processed in {chunk_time:.2f} seconds ({chunk_size_actual/chunk_time:.1f} rows/second)\")\n",
    "            \n",
    "            if (i//chunk_size + 1) % 5 == 0 and len(all_assignments) > 1:\n",
    "                intermediate_df = pl.concat(all_assignments)\n",
    "                all_assignments = [intermediate_df]  \n",
    "                print(f\"  Saved intermediate results, memory: {get_memory_usage()}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error collecting full dataset: {e}\")\n",
    "        print(\"Switching to streaming with manual chunking...\")\n",
    "        \n",
    "        chunk_buffer = []\n",
    "        current_chunk_size = 0\n",
    "        chunk_counter = 1\n",
    "        \n",
    "        for row in shopper_lazyframe.collect(streaming=True):\n",
    "            chunk_buffer.append(row)\n",
    "            current_chunk_size += 1\n",
    "            \n",
    "            if current_chunk_size >= chunk_size:\n",
    "                chunk_start_time = time.time()\n",
    "                \n",
    "                chunk = pl.DataFrame(chunk_buffer)\n",
    "                processed_rows += current_chunk_size\n",
    "                \n",
    "                print(f\"Processing chunk {chunk_counter} with {current_chunk_size:,} rows (total: {processed_rows:,}), memory: {get_memory_usage()}\")\n",
    "                \n",
    "                assignments = process_shopper_chunk(\n",
    "                    chunk, store_df, tree, store_centers, store_ids, store_radii, \n",
    "                    multiple_assignment=multiple_assignment\n",
    "                )\n",
    "\n",
    "                all_assignments.append(assignments)\n",
    "                \n",
    "                chunk_time = time.time() - chunk_start_time\n",
    "                print(f\"  Chunk {chunk_counter} processed in {chunk_time:.2f} seconds ({current_chunk_size/chunk_time:.1f} rows/second)\")\n",
    "                \n",
    "                chunk_buffer = []\n",
    "                current_chunk_size = 0\n",
    "                chunk_counter += 1\n",
    "                \n",
    "                if chunk_counter % 5 == 0 and len(all_assignments) > 1:\n",
    "                    intermediate_df = pl.concat(all_assignments)\n",
    "                    all_assignments = [intermediate_df]\n",
    "                    print(f\"  Saved intermediate results, memory: {get_memory_usage()}\")\n",
    "        \n",
    "        if chunk_buffer:\n",
    "            chunk = pl.DataFrame(chunk_buffer)\n",
    "            processed_rows += len(chunk)\n",
    "            \n",
    "            print(f\"Processing final chunk with {len(chunk):,} rows (total: {processed_rows:,})\")\n",
    "            \n",
    "            assignments = process_shopper_chunk(\n",
    "                chunk, store_df, tree, store_centers, store_ids, store_radii, \n",
    "                multiple_assignment=multiple_assignment\n",
    "            )\n",
    "            \n",
    "            all_assignments.append(assignments)\n",
    "    \n",
    "    if all_assignments:\n",
    "        result_df = pl.concat(all_assignments)\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\nAssignment complete!\")\n",
    "        print(f\"Processed {processed_rows:,} shopper records in {total_time:.2f} seconds\")\n",
    "        print(f\"Average processing speed: {processed_rows/total_time:.1f} records/second\")\n",
    "        \n",
    "        return result_df\n",
    "    else:\n",
    "        print(\"No assignments found!\")\n",
    "        sample_assignment = {\n",
    "            'ShopperID': None,\n",
    "            'Datetime': None,\n",
    "            'StoreID': None,\n",
    "            'Distance': None\n",
    "        }\n",
    "        return pl.DataFrame([sample_assignment])\n",
    "\n",
    "path_loc = \"DataExercise/ShopperLoc.csv\"\n",
    "\n",
    "print(\"Converting store data to Polars format...\")\n",
    "store_df_pl = pl.from_pandas(df_store)\n",
    "\n",
    "MULTIPLE_ASSIGNMENT = False\n",
    "print(f\"Using {'multiple' if MULTIPLE_ASSIGNMENT else 'single'} store assignment method\")\n",
    "\n",
    "print(\"Loading full ShopperLoc data file...\")\n",
    "start_time = time.time()\n",
    "\n",
    "shopper_lf = pl.scan_csv(\n",
    "    path_loc\n",
    ")\n",
    "\n",
    "print(f\"Data prepared for streaming in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "df_assign_pl = assign_full_dataset(\n",
    "    store_df_pl,\n",
    "    shopper_lf,\n",
    "    multiple_assignment=MULTIPLE_ASSIGNMENT,\n",
    "    chunk_size=500000 \n",
    ")\n",
    "\n",
    "print(\"Converting results to Pandas for further processing...\")\n",
    "df_assign = df_assign_pl.to_pandas()\n",
    "\n",
    "total_shoppers = len(df_assign)\n",
    "assigned_shoppers = df_assign['StoreID'].notna().sum()\n",
    "assignment_rate = assigned_shoppers / total_shoppers * 100\n",
    "\n",
    "print(f\"\\nAssignment statistics:\")\n",
    "print(f\"  Total shopper locations: {total_shoppers:,}\")\n",
    "print(f\"  Assigned to stores: {assigned_shoppers:,} ({assignment_rate:.2f}%)\")\n",
    "print(f\"  Unassigned: {total_shoppers - assigned_shoppers:,} ({100 - assignment_rate:.2f}%)\")\n",
    "\n",
    "if assignment_rate < 70:\n",
    "    print(\"Warning: Low assignment rate. Consider increasing store radii or checking for data issues.\")\n",
    "\n",
    "df_assign['Date'] = pd.to_datetime(df_assign['Datetime']).dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6beef7-a9da-4436-94b3-46b62c9c2d9f",
   "metadata": {},
   "source": [
    "# Aggregate Daily Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0657b2be-2247-4153-8fb9-3a0ce001f57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in df_assign: ['ShopperID', 'Datetime', 'StoreID', 'Distance', 'Date']\n",
      "Sample of first few rows:\n",
      "   ShopperID             Datetime  StoreID   Distance        Date\n",
      "0          0  2020-01-01 08:00:00    149.0  20.733549  2020-01-01\n",
      "1        339  2020-01-01 08:00:00    149.0   7.882423  2020-01-01\n",
      "2        339  2020-01-01 09:00:00    149.0  11.538913  2020-01-01\n",
      "3        624  2020-01-01 09:00:00      NaN        NaN  2020-01-01\n",
      "4         49  2020-01-01 10:00:00      NaN        NaN  2020-01-01\n",
      "Number of rows with StoreID: 5675845\n",
      "Daily store traffic shape: (162980, 3)\n",
      "\n",
      "After merge with store data shape: (162980, 4)\n",
      "Sample after merge:\n",
      "   StoreID        Date  DailyStoreTraffic  FirmID\n",
      "0      0.0  2020-01-01                 17       0\n",
      "1      0.0  2020-01-02                 12       0\n",
      "2      0.0  2020-01-03                 10       0\n",
      "3      0.0  2020-01-04                 13       0\n",
      "4      0.0  2020-01-05                 17       0\n",
      "\n",
      "Firm daily traffic shape: (36550, 3)\n",
      "Sample of firm traffic:\n",
      "   FirmID        Date  FirmDailyTraffic\n",
      "0       0  2020-01-01               114\n",
      "1       0  2020-01-02               109\n",
      "2       0  2020-01-03               104\n",
      "3       0  2020-01-04                99\n",
      "4       0  2020-01-05               121\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns in df_assign:\", df_assign.columns.tolist())\n",
    "print(\"Sample of first few rows:\")\n",
    "print(df_assign.head())\n",
    "\n",
    "print(\"Number of rows with StoreID:\", df_assign['StoreID'].notna().sum())\n",
    "\n",
    "if 'Date' not in df_assign.columns and 'Datetime' in df_assign.columns:\n",
    "    print(\"Creating Date column from Datetime...\")\n",
    "    df_assign['Date'] = pd.to_datetime(df_assign['Datetime']).dt.date\n",
    "\n",
    "df_daily_store_traffic = (\n",
    "    df_assign.dropna(subset=['StoreID'])\n",
    "    .groupby(['StoreID', 'Date'])['ShopperID']\n",
    "    .nunique()\n",
    "    .reset_index(name='DailyStoreTraffic')\n",
    ")\n",
    "\n",
    "print(\"Daily store traffic shape:\", df_daily_store_traffic.shape)\n",
    "\n",
    "df_daily_store_traffic = df_daily_store_traffic.merge(\n",
    "    df_store[['StoreID', 'FirmID']], \n",
    "    on='StoreID', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"After merge with store data shape:\", df_daily_store_traffic.shape)\n",
    "print(\"Sample after merge:\")\n",
    "print(df_daily_store_traffic.head())\n",
    "print()\n",
    "\n",
    "df_daily_firm_traffic = (\n",
    "    df_daily_store_traffic\n",
    "    .groupby(['FirmID', 'Date'])['DailyStoreTraffic']\n",
    "    .sum()\n",
    "    .reset_index(name='FirmDailyTraffic')\n",
    ")\n",
    "\n",
    "print(\"Firm daily traffic shape:\", df_daily_firm_traffic.shape)\n",
    "print(\"Sample of firm traffic:\")\n",
    "print(df_daily_firm_traffic.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3687c9b-0ad8-44e5-9ee2-4eaf19afcb37",
   "metadata": {},
   "source": [
    "# Merge Traffic with Sales & Build Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6bfea46-74af-44c1-8eef-6b6c83d3036b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building features for modeling...\n",
      "df_sales dtypes:\n",
      "FirmID     int64\n",
      "Date      object\n",
      "dtype: object\n",
      "\n",
      "df_daily_firm_traffic dtypes:\n",
      "FirmID     int64\n",
      "Date      object\n",
      "dtype: object\n",
      "\n",
      "df_sales sample:\n",
      "   FirmID        Date\n",
      "0       0  2020-01-01\n",
      "1       0  2020-01-02\n",
      "2       0  2020-01-03\n",
      "3       0  2020-01-04\n",
      "4       0  2020-01-05\n",
      "\n",
      "df_daily_firm_traffic sample:\n",
      "   FirmID        Date\n",
      "0       0  2020-01-01\n",
      "1       0  2020-01-02\n",
      "2       0  2020-01-03\n",
      "3       0  2020-01-04\n",
      "4       0  2020-01-05\n",
      "\n",
      "Converting Date columns to consistent datetime format...\n",
      "Converting FirmID columns to consistent numeric format...\n",
      "\n",
      "Checking for overlapping values...\n",
      "Firms in sales data: 50\n",
      "Firms in traffic data: 50\n",
      "Common firms: 50\n",
      "Dates in sales data: 731\n",
      "Dates in traffic data: 731\n",
      "Common dates: 731\n",
      "\n",
      "Performing merge with consistent data types...\n",
      "\n",
      "Merge results:\n",
      "_merge\n",
      "both          36550\n",
      "left_only         0\n",
      "right_only        0\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Found 1400 rows with missing sales values\n",
      "These rows will form our test/forecast set\n",
      "Training set: 35150 rows\n",
      "Test set: 1400 rows\n",
      "\n",
      "Filling missing traffic values using only past data...\n",
      "\n",
      "Feature engineering for training set...\n",
      "\n",
      "Feature engineering for test set...\n",
      "Test set contains 28 unique dates\n",
      "\n",
      "Final feature set for training:\n",
      "  - FirmDailyTraffic\n",
      "  - TrafficPctChange\n",
      "  - DayOfWeek\n",
      "  - Month\n",
      "  - DayOfMonth\n",
      "  - IsWeekend\n",
      "  - FirmDailyTraffic_lag1\n",
      "  - FirmDailyTraffic_lag2\n",
      "  - FirmDailyTraffic_lag7\n",
      "  - Traffic_7day_avg\n",
      "  - Sales_lag1\n",
      "  - Sales_lag7\n",
      "  - Sales_7day_avg\n",
      "  - Sales_7day_std\n",
      "  - SalesPctChange\n",
      "  - Sales_normalized\n",
      "  - Sales_robust_scaled\n",
      "\n",
      "Final feature set for testing:\n",
      "  - FirmDailyTraffic\n",
      "  - TrafficPctChange\n",
      "  - DayOfWeek\n",
      "  - Month\n",
      "  - DayOfMonth\n",
      "  - IsWeekend\n",
      "  - FirmDailyTraffic_lag1\n",
      "  - FirmDailyTraffic_lag2\n",
      "  - FirmDailyTraffic_lag7\n",
      "  - Traffic_7day_avg\n",
      "  - Sales_lag1\n",
      "  - Sales_lag7\n",
      "  - Sales_7day_avg\n",
      "  - Sales_7day_std\n",
      "  - SalesPctChange\n",
      "  - Sales_normalized\n",
      "  - Sales_robust_scaled\n",
      "\n",
      "Training set shape: (35150, 21)\n",
      "Test set shape: (1400, 21)\n",
      "\n",
      "Feature engineering completed successfully\n",
      "Final dataset shape: (36550, 5)\n",
      "Available features for modeling:\n",
      "  - FirmDailyTraffic\n",
      "  - Sales_is_missing\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBuilding features for modeling...\")\n",
    "\n",
    "print(\"df_sales dtypes:\")\n",
    "print(df_sales[['FirmID', 'Date']].dtypes)\n",
    "print(\"\\ndf_daily_firm_traffic dtypes:\")\n",
    "print(df_daily_firm_traffic[['FirmID', 'Date']].dtypes)\n",
    "\n",
    "print(\"\\ndf_sales sample:\")\n",
    "print(df_sales[['FirmID', 'Date']].head())\n",
    "print(\"\\ndf_daily_firm_traffic sample:\")\n",
    "print(df_daily_firm_traffic[['FirmID', 'Date']].head())\n",
    "\n",
    "print(\"\\nConverting Date columns to consistent datetime format...\")\n",
    "if not pd.api.types.is_datetime64_dtype(df_sales['Date']):\n",
    "    df_sales['Date'] = pd.to_datetime(df_sales['Date'])\n",
    "if not pd.api.types.is_datetime64_dtype(df_daily_firm_traffic['Date']):\n",
    "    df_daily_firm_traffic['Date'] = pd.to_datetime(df_daily_firm_traffic['Date'])\n",
    "\n",
    "print(\"Converting FirmID columns to consistent numeric format...\")\n",
    "df_sales['FirmID'] = df_sales['FirmID'].astype(float)\n",
    "df_daily_firm_traffic['FirmID'] = df_daily_firm_traffic['FirmID'].astype(float)\n",
    "\n",
    "print(\"\\nChecking for overlapping values...\")\n",
    "sales_firms = set(df_sales['FirmID'].unique())\n",
    "traffic_firms = set(df_daily_firm_traffic['FirmID'].unique())\n",
    "common_firms = sales_firms.intersection(traffic_firms)\n",
    "print(f\"Firms in sales data: {len(sales_firms)}\")\n",
    "print(f\"Firms in traffic data: {len(traffic_firms)}\")\n",
    "print(f\"Common firms: {len(common_firms)}\")\n",
    "\n",
    "sales_dates = set(df_sales['Date'].dt.date)\n",
    "traffic_dates = set(df_daily_firm_traffic['Date'].dt.date)\n",
    "common_dates = sales_dates.intersection(traffic_dates)\n",
    "print(f\"Dates in sales data: {len(sales_dates)}\")\n",
    "print(f\"Dates in traffic data: {len(traffic_dates)}\")\n",
    "print(f\"Common dates: {len(common_dates)}\")\n",
    "\n",
    "print(\"\\nPerforming merge with consistent data types...\")\n",
    "df_model = df_sales.merge(\n",
    "    df_daily_firm_traffic,\n",
    "    on=['FirmID', 'Date'],\n",
    "    how='left',\n",
    "    indicator=True \n",
    ")\n",
    "\n",
    "print(\"\\nMerge results:\")\n",
    "print(df_model['_merge'].value_counts())\n",
    "\n",
    "if (df_model['_merge'] == 'left_only').any():\n",
    "    print(\"\\nSample of unmatched rows:\")\n",
    "    print(df_model[df_model['_merge'] == 'left_only'][['FirmID', 'Date', 'Sales']].head())\n",
    "\n",
    "df_model = df_model.drop(columns=['_merge'])\n",
    "\n",
    "df_model = df_model.sort_values(['FirmID', 'Date']).reset_index(drop=True)\n",
    "\n",
    "df_model['Sales_is_missing'] = df_model['Sales'].isna()\n",
    "\n",
    "print(f\"\\nFound {df_model['Sales_is_missing'].sum()} rows with missing sales values\")\n",
    "print(f\"These rows will form our test/forecast set\")\n",
    "\n",
    "df_train = df_model[~df_model['Sales_is_missing']].copy()\n",
    "df_test = df_model[df_model['Sales_is_missing']].copy()\n",
    "\n",
    "print(f\"Training set: {len(df_train)} rows\")\n",
    "print(f\"Test set: {len(df_test)} rows\")\n",
    "\n",
    "print(\"\\nFilling missing traffic values using only past data...\")\n",
    "\n",
    "for firm in df_train['FirmID'].unique():\n",
    "    firm_mask = df_train['FirmID'] == firm\n",
    "    firm_data = df_train[firm_mask].copy().sort_values('Date')\n",
    "    \n",
    "    for i in range(len(firm_data)):\n",
    "        if pd.isna(firm_data.iloc[i]['FirmDailyTraffic']):\n",
    "            past_values = firm_data.iloc[:i]['FirmDailyTraffic'].dropna()\n",
    "            if len(past_values) > 0:\n",
    "                firm_data.iloc[i, firm_data.columns.get_loc('FirmDailyTraffic')] = past_values.mean()\n",
    "            else:\n",
    "                firm_data.iloc[i, firm_data.columns.get_loc('FirmDailyTraffic')] = 0\n",
    "    \n",
    "    df_train.loc[firm_mask, 'FirmDailyTraffic'] = firm_data['FirmDailyTraffic'].values\n",
    "\n",
    "for firm in df_test['FirmID'].unique():\n",
    "    hist_traffic = df_train[df_train['FirmID'] == firm]['FirmDailyTraffic'].dropna()\n",
    "    \n",
    "    if len(hist_traffic) > 0:\n",
    "        hist_mean = hist_traffic.mean()\n",
    "    else:\n",
    "        hist_mean = df_train['FirmDailyTraffic'].dropna().mean()\n",
    "    \n",
    "    test_firm_mask = df_test['FirmID'] == firm\n",
    "    df_test.loc[test_firm_mask & df_test['FirmDailyTraffic'].isna(), 'FirmDailyTraffic'] = hist_mean\n",
    "\n",
    "print(\"\\nFeature engineering for training set...\")\n",
    "\n",
    "df_train['TrafficPctChange'] = df_train.groupby('FirmID')['FirmDailyTraffic'].pct_change().fillna(0)\n",
    "\n",
    "df_train['DayOfWeek'] = df_train['Date'].dt.dayofweek\n",
    "df_train['Month'] = df_train['Date'].dt.month\n",
    "df_train['DayOfMonth'] = df_train['Date'].dt.day\n",
    "df_train['IsWeekend'] = df_train['DayOfWeek'].isin([5, 6]).astype(int)\n",
    "\n",
    "df_train['FirmDailyTraffic_lag1'] = df_train.groupby('FirmID')['FirmDailyTraffic'].shift(1).fillna(0)\n",
    "df_train['FirmDailyTraffic_lag2'] = df_train.groupby('FirmID')['FirmDailyTraffic'].shift(2).fillna(0)\n",
    "df_train['FirmDailyTraffic_lag7'] = df_train.groupby('FirmID')['FirmDailyTraffic'].shift(7).fillna(0)\n",
    "\n",
    "df_train['Traffic_7day_avg'] = df_train.groupby('FirmID')['FirmDailyTraffic'].transform(\n",
    "    lambda x: x.rolling(window=7, min_periods=1).mean()\n",
    ")\n",
    "\n",
    "df_train['Sales_lag1'] = df_train.groupby('FirmID')['Sales'].shift(1).fillna(0)\n",
    "df_train['Sales_lag7'] = df_train.groupby('FirmID')['Sales'].shift(7).fillna(0)\n",
    "\n",
    "df_train['Sales_7day_avg'] = df_train.groupby('FirmID')['Sales'].transform(\n",
    "    lambda x: x.rolling(window=7, min_periods=1).mean()\n",
    ")\n",
    "\n",
    "df_train['Sales_7day_std'] = df_train.groupby('FirmID')['Sales'].transform(\n",
    "    lambda x: x.rolling(window=7, min_periods=3).std()\n",
    ").fillna(0)\n",
    "\n",
    "df_train['SalesPctChange'] = df_train.groupby('FirmID')['Sales'].pct_change().fillna(0)\n",
    "\n",
    "df_train['Sales_normalized'] = df_train.groupby('FirmID')['Sales'].transform(\n",
    "    lambda x: (x - x.mean()) / (x.std() if x.std() != 0 else 1)\n",
    ").fillna(0)\n",
    "\n",
    "for firm in df_train['FirmID'].unique():\n",
    "    firm_mask = df_train['FirmID'] == firm\n",
    "    \n",
    "    sales_data = df_train.loc[firm_mask, 'Sales'].values\n",
    "    if len(sales_data) > 5 and np.std(sales_data) > 0: \n",
    "        try:\n",
    "            scaler = RobustScaler()\n",
    "            scaled_sales = scaler.fit_transform(sales_data.reshape(-1, 1)).flatten()\n",
    "            df_train.loc[firm_mask, 'Sales_robust_scaled'] = scaled_sales\n",
    "        except Exception as e:\n",
    "            print(f\"Error scaling data for firm {firm}: {e}\")\n",
    "            df_train.loc[firm_mask, 'Sales_robust_scaled'] = 0\n",
    "    else:\n",
    "        df_train.loc[firm_mask, 'Sales_robust_scaled'] = 0\n",
    "\n",
    "print(\"\\nFeature engineering for test set...\")\n",
    "\n",
    "df_test['DayOfWeek'] = df_test['Date'].dt.dayofweek\n",
    "df_test['Month'] = df_test['Date'].dt.month\n",
    "df_test['DayOfMonth'] = df_test['Date'].dt.day\n",
    "df_test['IsWeekend'] = df_test['DayOfWeek'].isin([5, 6]).astype(int)\n",
    "\n",
    "df_test = df_test.sort_values(['Date', 'FirmID']).reset_index(drop=True)\n",
    "\n",
    "unique_test_dates = df_test['Date'].unique()\n",
    "print(f\"Test set contains {len(unique_test_dates)} unique dates\")\n",
    "\n",
    "df_combined = pd.concat([df_train, df_test]).sort_values(['FirmID', 'Date']).reset_index(drop=True)\n",
    "\n",
    "df_combined['TrafficPctChange'] = df_combined.groupby('FirmID')['FirmDailyTraffic'].pct_change().fillna(0)\n",
    "df_combined['FirmDailyTraffic_lag1'] = df_combined.groupby('FirmID')['FirmDailyTraffic'].shift(1).fillna(0)\n",
    "df_combined['FirmDailyTraffic_lag2'] = df_combined.groupby('FirmID')['FirmDailyTraffic'].shift(2).fillna(0)\n",
    "df_combined['FirmDailyTraffic_lag7'] = df_combined.groupby('FirmID')['FirmDailyTraffic'].shift(7).fillna(0)\n",
    "\n",
    "df_combined['Traffic_7day_avg'] = df_combined.groupby('FirmID')['FirmDailyTraffic'].transform(\n",
    "    lambda x: x.rolling(window=7, min_periods=1).mean()\n",
    ")\n",
    "\n",
    "df_test = df_combined[df_combined['Sales_is_missing']].copy()\n",
    "\n",
    "for firm in df_test['FirmID'].unique():\n",
    "    firm_train = df_train[df_train['FirmID'] == firm]\n",
    "    \n",
    "    if len(firm_train) > 0:\n",
    "        last_sales = firm_train.sort_values('Date', ascending=False)['Sales'].iloc[0]\n",
    "        last_date = firm_train.sort_values('Date', ascending=False)['Date'].iloc[0]\n",
    "        \n",
    "        if len(firm_train) >= 7:\n",
    "            last_7day_avg = firm_train.sort_values('Date', ascending=False).head(7)['Sales'].mean()\n",
    "            last_7day_std = firm_train.sort_values('Date', ascending=False).head(7)['Sales'].std()\n",
    "        else:\n",
    "            last_7day_avg = firm_train['Sales'].mean()\n",
    "            last_7day_std = firm_train['Sales'].std() if len(firm_train) > 1 else 0\n",
    "            \n",
    "        if len(firm_train) > 1:\n",
    "            last_pct_change = (last_sales / firm_train.sort_values('Date', ascending=False)['Sales'].iloc[1] - 1) if firm_train.sort_values('Date', ascending=False)['Sales'].iloc[1] != 0 else 0\n",
    "        else:\n",
    "            last_pct_change = 0\n",
    "            \n",
    "        for test_date in df_test[df_test['FirmID'] == firm]['Date'].unique():\n",
    "            days_diff = (test_date - last_date).days\n",
    "\n",
    "            if days_diff == 1:\n",
    "                df_test.loc[(df_test['FirmID'] == firm) & (df_test['Date'] == test_date), 'Sales_lag1'] = last_sales\n",
    "            else:\n",
    "                df_test.loc[(df_test['FirmID'] == firm) & (df_test['Date'] == test_date), 'Sales_lag1'] = 0\n",
    "                \n",
    "            if days_diff == 7:\n",
    "                df_test.loc[(df_test['FirmID'] == firm) & (df_test['Date'] == test_date), 'Sales_lag7'] = last_sales\n",
    "            else:\n",
    "                df_test.loc[(df_test['FirmID'] == firm) & (df_test['Date'] == test_date), 'Sales_lag7'] = 0\n",
    "                \n",
    "            df_test.loc[(df_test['FirmID'] == firm) & (df_test['Date'] == test_date), 'Sales_7day_avg'] = last_7day_avg\n",
    "            df_test.loc[(df_test['FirmID'] == firm) & (df_test['Date'] == test_date), 'Sales_7day_std'] = last_7day_std\n",
    "            df_test.loc[(df_test['FirmID'] == firm) & (df_test['Date'] == test_date), 'SalesPctChange'] = last_pct_change\n",
    "            \n",
    "            mean_sales = firm_train['Sales'].mean()\n",
    "            std_sales = firm_train['Sales'].std() if firm_train['Sales'].std() != 0 else 1\n",
    "            df_test.loc[(df_test['FirmID'] == firm) & (df_test['Date'] == test_date), 'Sales_normalized'] = (last_sales - mean_sales) / std_sales\n",
    "\n",
    "            df_test.loc[(df_test['FirmID'] == firm) & (df_test['Date'] == test_date), 'Sales_robust_scaled'] = 0\n",
    "\n",
    "for col in df_test.columns:\n",
    "    if col not in ['Sales', 'Date', 'FirmID', 'Sales_is_missing'] and df_test[col].isna().any():\n",
    "        if pd.api.types.is_numeric_dtype(df_test[col]):\n",
    "            df_test[col] = df_test[col].fillna(0)\n",
    "            print(f\"Filled {df_test[col].isna().sum()} missing values in {col} with 0\")\n",
    "\n",
    "print(\"\\nFinal feature set for training:\")\n",
    "for col in df_train.columns:\n",
    "    if col not in ['Date', 'FirmID', 'Sales', 'Sales_is_missing']:\n",
    "        print(f\"  - {col}\")\n",
    "\n",
    "print(\"\\nFinal feature set for testing:\")\n",
    "for col in df_test.columns:\n",
    "    if col not in ['Date', 'FirmID', 'Sales', 'Sales_is_missing']:\n",
    "        print(f\"  - {col}\")\n",
    "\n",
    "print(f\"\\nTraining set shape: {df_train.shape}\")\n",
    "print(f\"Test set shape: {df_test.shape}\")\n",
    "\n",
    "train_nan_cols = [col for col in df_train.columns if df_train[col].isna().any() and col != 'Sales']\n",
    "if train_nan_cols:\n",
    "    print(f\"Warning: NaN values found in training set columns: {train_nan_cols}\")\n",
    "    \n",
    "test_nan_cols = [col for col in df_test.columns if df_test[col].isna().any() and col != 'Sales']\n",
    "if test_nan_cols:\n",
    "    print(f\"Warning: NaN values found in test set columns: {test_nan_cols}\")\n",
    "\n",
    "print(\"\\nFeature engineering completed successfully\")\n",
    "print(f\"Final dataset shape: {df_model.shape}\")\n",
    "print(\"Available features for modeling:\")\n",
    "for col in df_model.columns:\n",
    "    if col not in ['Date', 'FirmID', 'Sales']:\n",
    "        print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6e03d5b-10f3-4f21-bcd9-edeb57a33747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FirmID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sales</th>\n",
       "      <th>FirmDailyTraffic</th>\n",
       "      <th>Sales_is_missing</th>\n",
       "      <th>TrafficPctChange</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Month</th>\n",
       "      <th>DayOfMonth</th>\n",
       "      <th>IsWeekend</th>\n",
       "      <th>FirmDailyTraffic_lag1</th>\n",
       "      <th>FirmDailyTraffic_lag2</th>\n",
       "      <th>FirmDailyTraffic_lag7</th>\n",
       "      <th>Traffic_7day_avg</th>\n",
       "      <th>Sales_lag1</th>\n",
       "      <th>Sales_lag7</th>\n",
       "      <th>Sales_7day_avg</th>\n",
       "      <th>Sales_7day_std</th>\n",
       "      <th>SalesPctChange</th>\n",
       "      <th>Sales_normalized</th>\n",
       "      <th>Sales_robust_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35817</th>\n",
       "      <td>48.0</td>\n",
       "      <td>2021-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.082192</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>67.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>903.689493</td>\n",
       "      <td>69.178750</td>\n",
       "      <td>-0.115128</td>\n",
       "      <td>-0.94845</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35818</th>\n",
       "      <td>48.0</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.029851</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>68.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>903.689493</td>\n",
       "      <td>69.178750</td>\n",
       "      <td>-0.115128</td>\n",
       "      <td>-0.94845</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36522</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>112.571429</td>\n",
       "      <td>1397.808867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36523</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>108.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>113.714286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36524</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.045045</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>112.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36525</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>114.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.075472</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>111.571429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36526</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.078947</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>108.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36527</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36528</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>107.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1397.808867</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36529</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>126.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.188679</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>106.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>110.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36530</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>115.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.087302</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>126.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36531</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.034783</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>111.714286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36532</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.027027</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>110.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36533</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.009259</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>111.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36534</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.082569</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>110.714286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36535</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>115.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36536</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.147826</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>115.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36537</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>98.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36538</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.101852</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36539</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.144330</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>105.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36540</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>121.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.090090</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>107.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36541</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>107.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.115702</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>108.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36542</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>107.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36543</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.037383</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>107.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>107.714286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36544</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.009709</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>103.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>107.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36545</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>119.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.144231</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>110.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36546</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>123.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.033613</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36547</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>117.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.048780</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>111.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36548</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.094017</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>111.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36549</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.113208</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>112.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.05055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       FirmID       Date  Sales  FirmDailyTraffic  Sales_is_missing  \\\n",
       "35817    48.0 2021-12-30    NaN              67.0              True   \n",
       "35818    48.0 2021-12-31    NaN              65.0              True   \n",
       "36522    49.0 2021-12-04    NaN             108.0              True   \n",
       "36523    49.0 2021-12-05    NaN             111.0              True   \n",
       "36524    49.0 2021-12-06    NaN             106.0              True   \n",
       "36525    49.0 2021-12-07    NaN             114.0              True   \n",
       "36526    49.0 2021-12-08    NaN             105.0              True   \n",
       "36527    49.0 2021-12-09    NaN             105.0              True   \n",
       "36528    49.0 2021-12-10    NaN             106.0              True   \n",
       "36529    49.0 2021-12-11    NaN             126.0              True   \n",
       "36530    49.0 2021-12-12    NaN             115.0              True   \n",
       "36531    49.0 2021-12-13    NaN             111.0              True   \n",
       "36532    49.0 2021-12-14    NaN             108.0              True   \n",
       "36533    49.0 2021-12-15    NaN             109.0              True   \n",
       "36534    49.0 2021-12-16    NaN             100.0              True   \n",
       "36535    49.0 2021-12-17    NaN             115.0              True   \n",
       "36536    49.0 2021-12-18    NaN              98.0              True   \n",
       "36537    49.0 2021-12-19    NaN             108.0              True   \n",
       "36538    49.0 2021-12-20    NaN              97.0              True   \n",
       "36539    49.0 2021-12-21    NaN             111.0              True   \n",
       "36540    49.0 2021-12-22    NaN             121.0              True   \n",
       "36541    49.0 2021-12-23    NaN             107.0              True   \n",
       "36542    49.0 2021-12-24    NaN             107.0              True   \n",
       "36543    49.0 2021-12-25    NaN             103.0              True   \n",
       "36544    49.0 2021-12-26    NaN             104.0              True   \n",
       "36545    49.0 2021-12-27    NaN             119.0              True   \n",
       "36546    49.0 2021-12-28    NaN             123.0              True   \n",
       "36547    49.0 2021-12-29    NaN             117.0              True   \n",
       "36548    49.0 2021-12-30    NaN             106.0              True   \n",
       "36549    49.0 2021-12-31    NaN             118.0              True   \n",
       "\n",
       "       TrafficPctChange  DayOfWeek  Month  DayOfMonth  IsWeekend  \\\n",
       "35817         -0.082192          3     12          30          0   \n",
       "35818         -0.029851          4     12          31          0   \n",
       "36522          0.080000          5     12           4          1   \n",
       "36523          0.027778          6     12           5          1   \n",
       "36524         -0.045045          0     12           6          0   \n",
       "36525          0.075472          1     12           7          0   \n",
       "36526         -0.078947          2     12           8          0   \n",
       "36527          0.000000          3     12           9          0   \n",
       "36528          0.009524          4     12          10          0   \n",
       "36529          0.188679          5     12          11          1   \n",
       "36530         -0.087302          6     12          12          1   \n",
       "36531         -0.034783          0     12          13          0   \n",
       "36532         -0.027027          1     12          14          0   \n",
       "36533          0.009259          2     12          15          0   \n",
       "36534         -0.082569          3     12          16          0   \n",
       "36535          0.150000          4     12          17          0   \n",
       "36536         -0.147826          5     12          18          1   \n",
       "36537          0.102041          6     12          19          1   \n",
       "36538         -0.101852          0     12          20          0   \n",
       "36539          0.144330          1     12          21          0   \n",
       "36540          0.090090          2     12          22          0   \n",
       "36541         -0.115702          3     12          23          0   \n",
       "36542          0.000000          4     12          24          0   \n",
       "36543         -0.037383          5     12          25          1   \n",
       "36544          0.009709          6     12          26          1   \n",
       "36545          0.144231          0     12          27          0   \n",
       "36546          0.033613          1     12          28          0   \n",
       "36547         -0.048780          2     12          29          0   \n",
       "36548         -0.094017          3     12          30          0   \n",
       "36549          0.113208          4     12          31          0   \n",
       "\n",
       "       FirmDailyTraffic_lag1  FirmDailyTraffic_lag2  FirmDailyTraffic_lag7  \\\n",
       "35817                   73.0                   66.0                   73.0   \n",
       "35818                   67.0                   73.0                   62.0   \n",
       "36522                  100.0                  118.0                   99.0   \n",
       "36523                  108.0                  100.0                  103.0   \n",
       "36524                  111.0                  108.0                  115.0   \n",
       "36525                  106.0                  111.0                  120.0   \n",
       "36526                  114.0                  106.0                  124.0   \n",
       "36527                  105.0                  114.0                  118.0   \n",
       "36528                  105.0                  105.0                  100.0   \n",
       "36529                  106.0                  105.0                  108.0   \n",
       "36530                  126.0                  106.0                  111.0   \n",
       "36531                  115.0                  126.0                  106.0   \n",
       "36532                  111.0                  115.0                  114.0   \n",
       "36533                  108.0                  111.0                  105.0   \n",
       "36534                  109.0                  108.0                  105.0   \n",
       "36535                  100.0                  109.0                  106.0   \n",
       "36536                  115.0                  100.0                  126.0   \n",
       "36537                   98.0                  115.0                  115.0   \n",
       "36538                  108.0                   98.0                  111.0   \n",
       "36539                   97.0                  108.0                  108.0   \n",
       "36540                  111.0                   97.0                  109.0   \n",
       "36541                  121.0                  111.0                  100.0   \n",
       "36542                  107.0                  121.0                  115.0   \n",
       "36543                  107.0                  107.0                   98.0   \n",
       "36544                  103.0                  107.0                  108.0   \n",
       "36545                  104.0                  103.0                   97.0   \n",
       "36546                  119.0                  104.0                  111.0   \n",
       "36547                  123.0                  119.0                  121.0   \n",
       "36548                  117.0                  123.0                  107.0   \n",
       "36549                  106.0                  117.0                  107.0   \n",
       "\n",
       "       Traffic_7day_avg   Sales_lag1   Sales_lag7  Sales_7day_avg  \\\n",
       "35817         67.857143     0.000000     0.000000      903.689493   \n",
       "35818         68.285714     0.000000     0.000000      903.689493   \n",
       "36522        112.571429  1397.808867     0.000000     1739.179452   \n",
       "36523        113.714286     0.000000     0.000000     1739.179452   \n",
       "36524        112.428571     0.000000     0.000000     1739.179452   \n",
       "36525        111.571429     0.000000     0.000000     1739.179452   \n",
       "36526        108.857143     0.000000     0.000000     1739.179452   \n",
       "36527        107.000000     0.000000     0.000000     1739.179452   \n",
       "36528        107.857143     0.000000  1397.808867     1739.179452   \n",
       "36529        110.428571     0.000000     0.000000     1739.179452   \n",
       "36530        111.000000     0.000000     0.000000     1739.179452   \n",
       "36531        111.714286     0.000000     0.000000     1739.179452   \n",
       "36532        110.857143     0.000000     0.000000     1739.179452   \n",
       "36533        111.428571     0.000000     0.000000     1739.179452   \n",
       "36534        110.714286     0.000000     0.000000     1739.179452   \n",
       "36535        112.000000     0.000000     0.000000     1739.179452   \n",
       "36536        108.000000     0.000000     0.000000     1739.179452   \n",
       "36537        107.000000     0.000000     0.000000     1739.179452   \n",
       "36538        105.000000     0.000000     0.000000     1739.179452   \n",
       "36539        105.428571     0.000000     0.000000     1739.179452   \n",
       "36540        107.142857     0.000000     0.000000     1739.179452   \n",
       "36541        108.142857     0.000000     0.000000     1739.179452   \n",
       "36542        107.000000     0.000000     0.000000     1739.179452   \n",
       "36543        107.714286     0.000000     0.000000     1739.179452   \n",
       "36544        107.142857     0.000000     0.000000     1739.179452   \n",
       "36545        110.285714     0.000000     0.000000     1739.179452   \n",
       "36546        112.000000     0.000000     0.000000     1739.179452   \n",
       "36547        111.428571     0.000000     0.000000     1739.179452   \n",
       "36548        111.285714     0.000000     0.000000     1739.179452   \n",
       "36549        112.857143     0.000000     0.000000     1739.179452   \n",
       "\n",
       "       Sales_7day_std  SalesPctChange  Sales_normalized  Sales_robust_scaled  \n",
       "35817       69.178750       -0.115128          -0.94845                  0.0  \n",
       "35818       69.178750       -0.115128          -0.94845                  0.0  \n",
       "36522      575.572498        0.324032          -1.05055                  0.0  \n",
       "36523      575.572498        0.324032          -1.05055                  0.0  \n",
       "36524      575.572498        0.324032          -1.05055                  0.0  \n",
       "36525      575.572498        0.324032          -1.05055                  0.0  \n",
       "36526      575.572498        0.324032          -1.05055                  0.0  \n",
       "36527      575.572498        0.324032          -1.05055                  0.0  \n",
       "36528      575.572498        0.324032          -1.05055                  0.0  \n",
       "36529      575.572498        0.324032          -1.05055                  0.0  \n",
       "36530      575.572498        0.324032          -1.05055                  0.0  \n",
       "36531      575.572498        0.324032          -1.05055                  0.0  \n",
       "36532      575.572498        0.324032          -1.05055                  0.0  \n",
       "36533      575.572498        0.324032          -1.05055                  0.0  \n",
       "36534      575.572498        0.324032          -1.05055                  0.0  \n",
       "36535      575.572498        0.324032          -1.05055                  0.0  \n",
       "36536      575.572498        0.324032          -1.05055                  0.0  \n",
       "36537      575.572498        0.324032          -1.05055                  0.0  \n",
       "36538      575.572498        0.324032          -1.05055                  0.0  \n",
       "36539      575.572498        0.324032          -1.05055                  0.0  \n",
       "36540      575.572498        0.324032          -1.05055                  0.0  \n",
       "36541      575.572498        0.324032          -1.05055                  0.0  \n",
       "36542      575.572498        0.324032          -1.05055                  0.0  \n",
       "36543      575.572498        0.324032          -1.05055                  0.0  \n",
       "36544      575.572498        0.324032          -1.05055                  0.0  \n",
       "36545      575.572498        0.324032          -1.05055                  0.0  \n",
       "36546      575.572498        0.324032          -1.05055                  0.0  \n",
       "36547      575.572498        0.324032          -1.05055                  0.0  \n",
       "36548      575.572498        0.324032          -1.05055                  0.0  \n",
       "36549      575.572498        0.324032          -1.05055                  0.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71406cb8-66c4-4262-a657-bca8ab311b3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FirmID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sales</th>\n",
       "      <th>FirmDailyTraffic</th>\n",
       "      <th>Sales_is_missing</th>\n",
       "      <th>TrafficPctChange</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Month</th>\n",
       "      <th>DayOfMonth</th>\n",
       "      <th>IsWeekend</th>\n",
       "      <th>FirmDailyTraffic_lag1</th>\n",
       "      <th>FirmDailyTraffic_lag2</th>\n",
       "      <th>FirmDailyTraffic_lag7</th>\n",
       "      <th>Traffic_7day_avg</th>\n",
       "      <th>Sales_lag1</th>\n",
       "      <th>Sales_lag7</th>\n",
       "      <th>Sales_7day_avg</th>\n",
       "      <th>Sales_7day_std</th>\n",
       "      <th>SalesPctChange</th>\n",
       "      <th>Sales_normalized</th>\n",
       "      <th>Sales_robust_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36492</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-11-04</td>\n",
       "      <td>1047.599175</td>\n",
       "      <td>94</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.168142</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>104.428571</td>\n",
       "      <td>1688.620569</td>\n",
       "      <td>1309.311880</td>\n",
       "      <td>1713.610822</td>\n",
       "      <td>948.212110</td>\n",
       "      <td>-0.379612</td>\n",
       "      <td>-1.443398</td>\n",
       "      <td>-0.852448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36493</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-11-05</td>\n",
       "      <td>1831.061661</td>\n",
       "      <td>120</td>\n",
       "      <td>False</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>104.857143</td>\n",
       "      <td>1047.599175</td>\n",
       "      <td>1237.982355</td>\n",
       "      <td>1798.336437</td>\n",
       "      <td>924.838769</td>\n",
       "      <td>0.747865</td>\n",
       "      <td>-0.564548</td>\n",
       "      <td>-0.261601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36494</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-11-06</td>\n",
       "      <td>2074.301601</td>\n",
       "      <td>97</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.191667</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>120.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>101.714286</td>\n",
       "      <td>1831.061661</td>\n",
       "      <td>3670.787213</td>\n",
       "      <td>1570.267064</td>\n",
       "      <td>472.217442</td>\n",
       "      <td>0.132841</td>\n",
       "      <td>-0.291694</td>\n",
       "      <td>-0.078162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36495</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-11-07</td>\n",
       "      <td>2834.257179</td>\n",
       "      <td>111</td>\n",
       "      <td>False</td>\n",
       "      <td>0.144330</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>97.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>104.285714</td>\n",
       "      <td>2074.301601</td>\n",
       "      <td>2124.056406</td>\n",
       "      <td>1671.724317</td>\n",
       "      <td>652.797727</td>\n",
       "      <td>0.366367</td>\n",
       "      <td>0.560788</td>\n",
       "      <td>0.494956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36496</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-11-08</td>\n",
       "      <td>1199.787564</td>\n",
       "      <td>105</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.054054</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>107.285714</td>\n",
       "      <td>2834.257179</td>\n",
       "      <td>1157.966670</td>\n",
       "      <td>1677.698730</td>\n",
       "      <td>647.481886</td>\n",
       "      <td>-0.576684</td>\n",
       "      <td>-1.272681</td>\n",
       "      <td>-0.737675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36497</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-11-09</td>\n",
       "      <td>1270.488626</td>\n",
       "      <td>116</td>\n",
       "      <td>False</td>\n",
       "      <td>0.104762</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>1199.787564</td>\n",
       "      <td>1068.263364</td>\n",
       "      <td>1706.588053</td>\n",
       "      <td>619.672388</td>\n",
       "      <td>0.058928</td>\n",
       "      <td>-1.193372</td>\n",
       "      <td>-0.684356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36498</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-11-10</td>\n",
       "      <td>1263.127509</td>\n",
       "      <td>106</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.086207</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>1270.488626</td>\n",
       "      <td>1688.620569</td>\n",
       "      <td>1645.803331</td>\n",
       "      <td>642.188206</td>\n",
       "      <td>-0.005794</td>\n",
       "      <td>-1.201629</td>\n",
       "      <td>-0.689907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36499</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-11-11</td>\n",
       "      <td>1604.580515</td>\n",
       "      <td>110</td>\n",
       "      <td>False</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>109.285714</td>\n",
       "      <td>1263.127509</td>\n",
       "      <td>1047.599175</td>\n",
       "      <td>1725.372093</td>\n",
       "      <td>587.929552</td>\n",
       "      <td>0.270323</td>\n",
       "      <td>-0.818604</td>\n",
       "      <td>-0.432401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36500</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-11-12</td>\n",
       "      <td>930.441300</td>\n",
       "      <td>101</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.081818</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>106.571429</td>\n",
       "      <td>1604.580515</td>\n",
       "      <td>1831.061661</td>\n",
       "      <td>1596.712042</td>\n",
       "      <td>655.596137</td>\n",
       "      <td>-0.420134</td>\n",
       "      <td>-1.574820</td>\n",
       "      <td>-0.940802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36501</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-11-13</td>\n",
       "      <td>3828.806906</td>\n",
       "      <td>119</td>\n",
       "      <td>False</td>\n",
       "      <td>0.178218</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>101.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>109.714286</td>\n",
       "      <td>930.441300</td>\n",
       "      <td>2074.301601</td>\n",
       "      <td>1847.355657</td>\n",
       "      <td>1071.854927</td>\n",
       "      <td>3.115044</td>\n",
       "      <td>1.676425</td>\n",
       "      <td>1.244994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36502</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-11-14</td>\n",
       "      <td>2528.014564</td>\n",
       "      <td>105</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.117647</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>119.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>108.857143</td>\n",
       "      <td>3828.806906</td>\n",
       "      <td>2834.257179</td>\n",
       "      <td>1803.606712</td>\n",
       "      <td>1030.304328</td>\n",
       "      <td>-0.339738</td>\n",
       "      <td>0.217260</td>\n",
       "      <td>0.264004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36503</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-11-15</td>\n",
       "      <td>991.613348</td>\n",
       "      <td>108</td>\n",
       "      <td>False</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>109.285714</td>\n",
       "      <td>2528.014564</td>\n",
       "      <td>1199.787564</td>\n",
       "      <td>1773.867538</td>\n",
       "      <td>1053.383974</td>\n",
       "      <td>-0.607750</td>\n",
       "      <td>-1.506201</td>\n",
       "      <td>-0.894669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36504</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-11-16</td>\n",
       "      <td>1028.487910</td>\n",
       "      <td>113</td>\n",
       "      <td>False</td>\n",
       "      <td>0.046296</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>108.857143</td>\n",
       "      <td>991.613348</td>\n",
       "      <td>1270.488626</td>\n",
       "      <td>1739.296007</td>\n",
       "      <td>1076.378257</td>\n",
       "      <td>0.037186</td>\n",
       "      <td>-1.464836</td>\n",
       "      <td>-0.866860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36505</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-11-17</td>\n",
       "      <td>1123.827926</td>\n",
       "      <td>101</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.106195</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>108.142857</td>\n",
       "      <td>1028.487910</td>\n",
       "      <td>1263.127509</td>\n",
       "      <td>1719.396067</td>\n",
       "      <td>1087.875098</td>\n",
       "      <td>0.092699</td>\n",
       "      <td>-1.357889</td>\n",
       "      <td>-0.794960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36506</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-11-18</td>\n",
       "      <td>998.488475</td>\n",
       "      <td>109</td>\n",
       "      <td>False</td>\n",
       "      <td>0.079208</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>1123.827926</td>\n",
       "      <td>1604.580515</td>\n",
       "      <td>1632.811490</td>\n",
       "      <td>1122.117069</td>\n",
       "      <td>-0.111529</td>\n",
       "      <td>-1.498488</td>\n",
       "      <td>-0.889484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36507</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-11-19</td>\n",
       "      <td>1454.753945</td>\n",
       "      <td>115</td>\n",
       "      <td>False</td>\n",
       "      <td>0.055046</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>998.488475</td>\n",
       "      <td>930.441300</td>\n",
       "      <td>1707.713296</td>\n",
       "      <td>1084.280786</td>\n",
       "      <td>0.456956</td>\n",
       "      <td>-0.986672</td>\n",
       "      <td>-0.545393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36508</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-11-20</td>\n",
       "      <td>2826.577011</td>\n",
       "      <td>106</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.078261</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>115.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>108.142857</td>\n",
       "      <td>1454.753945</td>\n",
       "      <td>3828.806906</td>\n",
       "      <td>1564.537597</td>\n",
       "      <td>781.378257</td>\n",
       "      <td>0.942993</td>\n",
       "      <td>0.552172</td>\n",
       "      <td>0.489164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36509</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-11-21</td>\n",
       "      <td>1506.617705</td>\n",
       "      <td>109</td>\n",
       "      <td>False</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>106.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>108.714286</td>\n",
       "      <td>2826.577011</td>\n",
       "      <td>2528.014564</td>\n",
       "      <td>1418.623760</td>\n",
       "      <td>656.930082</td>\n",
       "      <td>-0.466982</td>\n",
       "      <td>-0.928494</td>\n",
       "      <td>-0.506280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36510</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-11-22</td>\n",
       "      <td>1022.299111</td>\n",
       "      <td>108</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.009174</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>108.714286</td>\n",
       "      <td>1506.617705</td>\n",
       "      <td>991.613348</td>\n",
       "      <td>1423.007440</td>\n",
       "      <td>653.700188</td>\n",
       "      <td>-0.321461</td>\n",
       "      <td>-1.471779</td>\n",
       "      <td>-0.871528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36511</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-11-23</td>\n",
       "      <td>1300.719408</td>\n",
       "      <td>121</td>\n",
       "      <td>False</td>\n",
       "      <td>0.120370</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>109.857143</td>\n",
       "      <td>1022.299111</td>\n",
       "      <td>1028.487910</td>\n",
       "      <td>1461.897654</td>\n",
       "      <td>634.122120</td>\n",
       "      <td>0.272347</td>\n",
       "      <td>-1.159460</td>\n",
       "      <td>-0.661557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36512</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-11-24</td>\n",
       "      <td>1721.963479</td>\n",
       "      <td>120</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.008264</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>112.571429</td>\n",
       "      <td>1300.719408</td>\n",
       "      <td>1123.827926</td>\n",
       "      <td>1547.345590</td>\n",
       "      <td>621.141264</td>\n",
       "      <td>0.323855</td>\n",
       "      <td>-0.686929</td>\n",
       "      <td>-0.343877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36513</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-11-25</td>\n",
       "      <td>921.238078</td>\n",
       "      <td>118</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.016667</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>113.857143</td>\n",
       "      <td>1721.963479</td>\n",
       "      <td>998.488475</td>\n",
       "      <td>1536.309819</td>\n",
       "      <td>633.089355</td>\n",
       "      <td>-0.465007</td>\n",
       "      <td>-1.585144</td>\n",
       "      <td>-0.947742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36514</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-11-26</td>\n",
       "      <td>770.015986</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.152542</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>111.714286</td>\n",
       "      <td>921.238078</td>\n",
       "      <td>1454.753945</td>\n",
       "      <td>1438.490111</td>\n",
       "      <td>697.422254</td>\n",
       "      <td>-0.164151</td>\n",
       "      <td>-1.754778</td>\n",
       "      <td>-1.061786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36515</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-11-27</td>\n",
       "      <td>2874.955304</td>\n",
       "      <td>99</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>110.714286</td>\n",
       "      <td>770.015986</td>\n",
       "      <td>2826.577011</td>\n",
       "      <td>1445.401296</td>\n",
       "      <td>713.524054</td>\n",
       "      <td>2.733631</td>\n",
       "      <td>0.606441</td>\n",
       "      <td>0.525649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36516</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-11-28</td>\n",
       "      <td>1617.118150</td>\n",
       "      <td>103</td>\n",
       "      <td>False</td>\n",
       "      <td>0.040404</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>99.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>109.857143</td>\n",
       "      <td>2874.955304</td>\n",
       "      <td>1506.617705</td>\n",
       "      <td>1461.187074</td>\n",
       "      <td>716.320965</td>\n",
       "      <td>-0.437515</td>\n",
       "      <td>-0.804540</td>\n",
       "      <td>-0.422946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36517</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-11-29</td>\n",
       "      <td>1715.156925</td>\n",
       "      <td>115</td>\n",
       "      <td>False</td>\n",
       "      <td>0.116505</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>110.857143</td>\n",
       "      <td>1617.118150</td>\n",
       "      <td>1022.299111</td>\n",
       "      <td>1560.166761</td>\n",
       "      <td>693.060067</td>\n",
       "      <td>0.060626</td>\n",
       "      <td>-0.694565</td>\n",
       "      <td>-0.349010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36518</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-11-30</td>\n",
       "      <td>1535.418567</td>\n",
       "      <td>120</td>\n",
       "      <td>False</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>110.714286</td>\n",
       "      <td>1715.156925</td>\n",
       "      <td>1300.719408</td>\n",
       "      <td>1593.695213</td>\n",
       "      <td>684.035085</td>\n",
       "      <td>-0.104794</td>\n",
       "      <td>-0.896186</td>\n",
       "      <td>-0.484560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36519</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-01</td>\n",
       "      <td>1978.076991</td>\n",
       "      <td>124</td>\n",
       "      <td>False</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>111.285714</td>\n",
       "      <td>1535.418567</td>\n",
       "      <td>1721.963479</td>\n",
       "      <td>1630.282857</td>\n",
       "      <td>698.730992</td>\n",
       "      <td>0.288298</td>\n",
       "      <td>-0.399634</td>\n",
       "      <td>-0.150730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36520</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-02</td>\n",
       "      <td>1055.721361</td>\n",
       "      <td>118</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.048387</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>111.285714</td>\n",
       "      <td>1978.076991</td>\n",
       "      <td>921.238078</td>\n",
       "      <td>1649.494755</td>\n",
       "      <td>677.512944</td>\n",
       "      <td>-0.466289</td>\n",
       "      <td>-1.434287</td>\n",
       "      <td>-0.846322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36521</th>\n",
       "      <td>49.0</td>\n",
       "      <td>2021-12-03</td>\n",
       "      <td>1397.808867</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.152542</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>111.285714</td>\n",
       "      <td>1055.721361</td>\n",
       "      <td>770.015986</td>\n",
       "      <td>1739.179452</td>\n",
       "      <td>575.572498</td>\n",
       "      <td>0.324032</td>\n",
       "      <td>-1.050550</td>\n",
       "      <td>-0.588338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       FirmID       Date        Sales  FirmDailyTraffic  Sales_is_missing  \\\n",
       "36492    49.0 2021-11-04  1047.599175                94             False   \n",
       "36493    49.0 2021-11-05  1831.061661               120             False   \n",
       "36494    49.0 2021-11-06  2074.301601                97             False   \n",
       "36495    49.0 2021-11-07  2834.257179               111             False   \n",
       "36496    49.0 2021-11-08  1199.787564               105             False   \n",
       "36497    49.0 2021-11-09  1270.488626               116             False   \n",
       "36498    49.0 2021-11-10  1263.127509               106             False   \n",
       "36499    49.0 2021-11-11  1604.580515               110             False   \n",
       "36500    49.0 2021-11-12   930.441300               101             False   \n",
       "36501    49.0 2021-11-13  3828.806906               119             False   \n",
       "36502    49.0 2021-11-14  2528.014564               105             False   \n",
       "36503    49.0 2021-11-15   991.613348               108             False   \n",
       "36504    49.0 2021-11-16  1028.487910               113             False   \n",
       "36505    49.0 2021-11-17  1123.827926               101             False   \n",
       "36506    49.0 2021-11-18   998.488475               109             False   \n",
       "36507    49.0 2021-11-19  1454.753945               115             False   \n",
       "36508    49.0 2021-11-20  2826.577011               106             False   \n",
       "36509    49.0 2021-11-21  1506.617705               109             False   \n",
       "36510    49.0 2021-11-22  1022.299111               108             False   \n",
       "36511    49.0 2021-11-23  1300.719408               121             False   \n",
       "36512    49.0 2021-11-24  1721.963479               120             False   \n",
       "36513    49.0 2021-11-25   921.238078               118             False   \n",
       "36514    49.0 2021-11-26   770.015986               100             False   \n",
       "36515    49.0 2021-11-27  2874.955304                99             False   \n",
       "36516    49.0 2021-11-28  1617.118150               103             False   \n",
       "36517    49.0 2021-11-29  1715.156925               115             False   \n",
       "36518    49.0 2021-11-30  1535.418567               120             False   \n",
       "36519    49.0 2021-12-01  1978.076991               124             False   \n",
       "36520    49.0 2021-12-02  1055.721361               118             False   \n",
       "36521    49.0 2021-12-03  1397.808867               100             False   \n",
       "\n",
       "       TrafficPctChange  DayOfWeek  Month  DayOfMonth  IsWeekend  \\\n",
       "36492         -0.168142          3     11           4          0   \n",
       "36493          0.276596          4     11           5          0   \n",
       "36494         -0.191667          5     11           6          1   \n",
       "36495          0.144330          6     11           7          1   \n",
       "36496         -0.054054          0     11           8          0   \n",
       "36497          0.104762          1     11           9          0   \n",
       "36498         -0.086207          2     11          10          0   \n",
       "36499          0.037736          3     11          11          0   \n",
       "36500         -0.081818          4     11          12          0   \n",
       "36501          0.178218          5     11          13          1   \n",
       "36502         -0.117647          6     11          14          1   \n",
       "36503          0.028571          0     11          15          0   \n",
       "36504          0.046296          1     11          16          0   \n",
       "36505         -0.106195          2     11          17          0   \n",
       "36506          0.079208          3     11          18          0   \n",
       "36507          0.055046          4     11          19          0   \n",
       "36508         -0.078261          5     11          20          1   \n",
       "36509          0.028302          6     11          21          1   \n",
       "36510         -0.009174          0     11          22          0   \n",
       "36511          0.120370          1     11          23          0   \n",
       "36512         -0.008264          2     11          24          0   \n",
       "36513         -0.016667          3     11          25          0   \n",
       "36514         -0.152542          4     11          26          0   \n",
       "36515         -0.010000          5     11          27          1   \n",
       "36516          0.040404          6     11          28          1   \n",
       "36517          0.116505          0     11          29          0   \n",
       "36518          0.043478          1     11          30          0   \n",
       "36519          0.033333          2     12           1          0   \n",
       "36520         -0.048387          3     12           2          0   \n",
       "36521         -0.152542          4     12           3          0   \n",
       "\n",
       "       FirmDailyTraffic_lag1  FirmDailyTraffic_lag2  FirmDailyTraffic_lag7  \\\n",
       "36492                  113.0                  111.0                  109.0   \n",
       "36493                   94.0                  113.0                  117.0   \n",
       "36494                  120.0                   94.0                  119.0   \n",
       "36495                   97.0                  120.0                   93.0   \n",
       "36496                  111.0                   97.0                   84.0   \n",
       "36497                  105.0                  111.0                  111.0   \n",
       "36498                  116.0                  105.0                  113.0   \n",
       "36499                  106.0                  116.0                   94.0   \n",
       "36500                  110.0                  106.0                  120.0   \n",
       "36501                  101.0                  110.0                   97.0   \n",
       "36502                  119.0                  101.0                  111.0   \n",
       "36503                  105.0                  119.0                  105.0   \n",
       "36504                  108.0                  105.0                  116.0   \n",
       "36505                  113.0                  108.0                  106.0   \n",
       "36506                  101.0                  113.0                  110.0   \n",
       "36507                  109.0                  101.0                  101.0   \n",
       "36508                  115.0                  109.0                  119.0   \n",
       "36509                  106.0                  115.0                  105.0   \n",
       "36510                  109.0                  106.0                  108.0   \n",
       "36511                  108.0                  109.0                  113.0   \n",
       "36512                  121.0                  108.0                  101.0   \n",
       "36513                  120.0                  121.0                  109.0   \n",
       "36514                  118.0                  120.0                  115.0   \n",
       "36515                  100.0                  118.0                  106.0   \n",
       "36516                   99.0                  100.0                  109.0   \n",
       "36517                  103.0                   99.0                  108.0   \n",
       "36518                  115.0                  103.0                  121.0   \n",
       "36519                  120.0                  115.0                  120.0   \n",
       "36520                  124.0                  120.0                  118.0   \n",
       "36521                  118.0                  124.0                  100.0   \n",
       "\n",
       "       Traffic_7day_avg   Sales_lag1   Sales_lag7  Sales_7day_avg  \\\n",
       "36492        104.428571  1688.620569  1309.311880     1713.610822   \n",
       "36493        104.857143  1047.599175  1237.982355     1798.336437   \n",
       "36494        101.714286  1831.061661  3670.787213     1570.267064   \n",
       "36495        104.285714  2074.301601  2124.056406     1671.724317   \n",
       "36496        107.285714  2834.257179  1157.966670     1677.698730   \n",
       "36497        108.000000  1199.787564  1068.263364     1706.588053   \n",
       "36498        107.000000  1270.488626  1688.620569     1645.803331   \n",
       "36499        109.285714  1263.127509  1047.599175     1725.372093   \n",
       "36500        106.571429  1604.580515  1831.061661     1596.712042   \n",
       "36501        109.714286   930.441300  2074.301601     1847.355657   \n",
       "36502        108.857143  3828.806906  2834.257179     1803.606712   \n",
       "36503        109.285714  2528.014564  1199.787564     1773.867538   \n",
       "36504        108.857143   991.613348  1270.488626     1739.296007   \n",
       "36505        108.142857  1028.487910  1263.127509     1719.396067   \n",
       "36506        108.000000  1123.827926  1604.580515     1632.811490   \n",
       "36507        110.000000   998.488475   930.441300     1707.713296   \n",
       "36508        108.142857  1454.753945  3828.806906     1564.537597   \n",
       "36509        108.714286  2826.577011  2528.014564     1418.623760   \n",
       "36510        108.714286  1506.617705   991.613348     1423.007440   \n",
       "36511        109.857143  1022.299111  1028.487910     1461.897654   \n",
       "36512        112.571429  1300.719408  1123.827926     1547.345590   \n",
       "36513        113.857143  1721.963479   998.488475     1536.309819   \n",
       "36514        111.714286   921.238078  1454.753945     1438.490111   \n",
       "36515        110.714286   770.015986  2826.577011     1445.401296   \n",
       "36516        109.857143  2874.955304  1506.617705     1461.187074   \n",
       "36517        110.857143  1617.118150  1022.299111     1560.166761   \n",
       "36518        110.714286  1715.156925  1300.719408     1593.695213   \n",
       "36519        111.285714  1535.418567  1721.963479     1630.282857   \n",
       "36520        111.285714  1978.076991   921.238078     1649.494755   \n",
       "36521        111.285714  1055.721361   770.015986     1739.179452   \n",
       "\n",
       "       Sales_7day_std  SalesPctChange  Sales_normalized  Sales_robust_scaled  \n",
       "36492      948.212110       -0.379612         -1.443398            -0.852448  \n",
       "36493      924.838769        0.747865         -0.564548            -0.261601  \n",
       "36494      472.217442        0.132841         -0.291694            -0.078162  \n",
       "36495      652.797727        0.366367          0.560788             0.494956  \n",
       "36496      647.481886       -0.576684         -1.272681            -0.737675  \n",
       "36497      619.672388        0.058928         -1.193372            -0.684356  \n",
       "36498      642.188206       -0.005794         -1.201629            -0.689907  \n",
       "36499      587.929552        0.270323         -0.818604            -0.432401  \n",
       "36500      655.596137       -0.420134         -1.574820            -0.940802  \n",
       "36501     1071.854927        3.115044          1.676425             1.244994  \n",
       "36502     1030.304328       -0.339738          0.217260             0.264004  \n",
       "36503     1053.383974       -0.607750         -1.506201            -0.894669  \n",
       "36504     1076.378257        0.037186         -1.464836            -0.866860  \n",
       "36505     1087.875098        0.092699         -1.357889            -0.794960  \n",
       "36506     1122.117069       -0.111529         -1.498488            -0.889484  \n",
       "36507     1084.280786        0.456956         -0.986672            -0.545393  \n",
       "36508      781.378257        0.942993          0.552172             0.489164  \n",
       "36509      656.930082       -0.466982         -0.928494            -0.506280  \n",
       "36510      653.700188       -0.321461         -1.471779            -0.871528  \n",
       "36511      634.122120        0.272347         -1.159460            -0.661557  \n",
       "36512      621.141264        0.323855         -0.686929            -0.343877  \n",
       "36513      633.089355       -0.465007         -1.585144            -0.947742  \n",
       "36514      697.422254       -0.164151         -1.754778            -1.061786  \n",
       "36515      713.524054        2.733631          0.606441             0.525649  \n",
       "36516      716.320965       -0.437515         -0.804540            -0.422946  \n",
       "36517      693.060067        0.060626         -0.694565            -0.349010  \n",
       "36518      684.035085       -0.104794         -0.896186            -0.484560  \n",
       "36519      698.730992        0.288298         -0.399634            -0.150730  \n",
       "36520      677.512944       -0.466289         -1.434287            -0.846322  \n",
       "36521      575.572498        0.324032         -1.050550            -0.588338  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcd4b207-9404-42d5-9216-d8cdab99ebf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FirmID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sales</th>\n",
       "      <th>FirmDailyTraffic</th>\n",
       "      <th>TrafficPctChange</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Month</th>\n",
       "      <th>DayOfMonth</th>\n",
       "      <th>IsWeekend</th>\n",
       "      <th>FirmDailyTraffic_lag1</th>\n",
       "      <th>FirmDailyTraffic_lag2</th>\n",
       "      <th>FirmDailyTraffic_lag7</th>\n",
       "      <th>Traffic_7day_avg</th>\n",
       "      <th>Sales_lag1</th>\n",
       "      <th>Sales_lag7</th>\n",
       "      <th>Sales_7day_avg</th>\n",
       "      <th>Sales_7day_std</th>\n",
       "      <th>SalesPctChange</th>\n",
       "      <th>Sales_normalized</th>\n",
       "      <th>Sales_robust_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1400.000000</td>\n",
       "      <td>1400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>1400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24.500000</td>\n",
       "      <td>2021-12-17 12:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.515714</td>\n",
       "      <td>0.014253</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>84.436429</td>\n",
       "      <td>84.468571</td>\n",
       "      <td>84.478571</td>\n",
       "      <td>84.494388</td>\n",
       "      <td>34.711545</td>\n",
       "      <td>34.711545</td>\n",
       "      <td>1083.719559</td>\n",
       "      <td>395.184396</td>\n",
       "      <td>0.122171</td>\n",
       "      <td>-0.930213</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2021-12-04 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>-0.412844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>39.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>159.604587</td>\n",
       "      <td>65.761851</td>\n",
       "      <td>-0.775568</td>\n",
       "      <td>-1.841337</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>2021-12-10 18:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>-0.090909</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.571429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>618.900137</td>\n",
       "      <td>191.522829</td>\n",
       "      <td>-0.123845</td>\n",
       "      <td>-1.314320</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24.500000</td>\n",
       "      <td>2021-12-17 12:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>78.571429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>903.942966</td>\n",
       "      <td>334.098426</td>\n",
       "      <td>-0.008975</td>\n",
       "      <td>-1.014584</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>2021-12-24 06:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>24.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.250000</td>\n",
       "      <td>100.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1346.753125</td>\n",
       "      <td>538.285625</td>\n",
       "      <td>0.294203</td>\n",
       "      <td>-0.582764</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>49.000000</td>\n",
       "      <td>2021-12-31 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>178.285714</td>\n",
       "      <td>4547.372847</td>\n",
       "      <td>4547.372847</td>\n",
       "      <td>3902.408719</td>\n",
       "      <td>1307.906912</td>\n",
       "      <td>3.242085</td>\n",
       "      <td>0.518328</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.436026</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.573158</td>\n",
       "      <td>0.161959</td>\n",
       "      <td>2.000715</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.080634</td>\n",
       "      <td>0.451915</td>\n",
       "      <td>26.732524</td>\n",
       "      <td>26.761390</td>\n",
       "      <td>26.911118</td>\n",
       "      <td>25.309319</td>\n",
       "      <td>239.090808</td>\n",
       "      <td>239.090808</td>\n",
       "      <td>752.222875</td>\n",
       "      <td>276.203836</td>\n",
       "      <td>0.611977</td>\n",
       "      <td>0.525829</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            FirmID                 Date  Sales  FirmDailyTraffic  \\\n",
       "count  1400.000000                 1400    0.0       1400.000000   \n",
       "mean     24.500000  2021-12-17 12:00:00    NaN         84.515714   \n",
       "min       0.000000  2021-12-04 00:00:00    NaN         31.000000   \n",
       "25%      12.000000  2021-12-10 18:00:00    NaN         65.000000   \n",
       "50%      24.500000  2021-12-17 12:00:00    NaN         80.000000   \n",
       "75%      37.000000  2021-12-24 06:00:00    NaN        101.000000   \n",
       "max      49.000000  2021-12-31 00:00:00    NaN        198.000000   \n",
       "std      14.436026                  NaN    NaN         26.573158   \n",
       "\n",
       "       TrafficPctChange    DayOfWeek   Month   DayOfMonth    IsWeekend  \\\n",
       "count       1400.000000  1400.000000  1400.0  1400.000000  1400.000000   \n",
       "mean           0.014253     3.000000    12.0    17.500000     0.285714   \n",
       "min           -0.412844     0.000000    12.0     4.000000     0.000000   \n",
       "25%           -0.090909     1.000000    12.0    10.750000     0.000000   \n",
       "50%            0.000000     3.000000    12.0    17.500000     0.000000   \n",
       "75%            0.111111     5.000000    12.0    24.250000     1.000000   \n",
       "max            0.690476     6.000000    12.0    31.000000     1.000000   \n",
       "std            0.161959     2.000715     0.0     8.080634     0.451915   \n",
       "\n",
       "       FirmDailyTraffic_lag1  FirmDailyTraffic_lag2  FirmDailyTraffic_lag7  \\\n",
       "count            1400.000000            1400.000000            1400.000000   \n",
       "mean               84.436429              84.468571              84.478571   \n",
       "min                31.000000              31.000000              34.000000   \n",
       "25%                65.000000              65.000000              65.000000   \n",
       "50%                80.000000              79.000000              79.000000   \n",
       "75%               100.000000             100.000000             100.250000   \n",
       "max               198.000000             198.000000             198.000000   \n",
       "std                26.732524              26.761390              26.911118   \n",
       "\n",
       "       Traffic_7day_avg   Sales_lag1   Sales_lag7  Sales_7day_avg  \\\n",
       "count       1400.000000  1400.000000  1400.000000     1400.000000   \n",
       "mean          84.494388    34.711545    34.711545     1083.719559   \n",
       "min           39.142857     0.000000     0.000000      159.604587   \n",
       "25%           65.571429     0.000000     0.000000      618.900137   \n",
       "50%           78.571429     0.000000     0.000000      903.942966   \n",
       "75%          100.142857     0.000000     0.000000     1346.753125   \n",
       "max          178.285714  4547.372847  4547.372847     3902.408719   \n",
       "std           25.309319   239.090808   239.090808      752.222875   \n",
       "\n",
       "       Sales_7day_std  SalesPctChange  Sales_normalized  Sales_robust_scaled  \n",
       "count     1400.000000     1400.000000       1400.000000               1400.0  \n",
       "mean       395.184396        0.122171         -0.930213                  0.0  \n",
       "min         65.761851       -0.775568         -1.841337                  0.0  \n",
       "25%        191.522829       -0.123845         -1.314320                  0.0  \n",
       "50%        334.098426       -0.008975         -1.014584                  0.0  \n",
       "75%        538.285625        0.294203         -0.582764                  0.0  \n",
       "max       1307.906912        3.242085          0.518328                  0.0  \n",
       "std        276.203836        0.611977          0.525829                  0.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb4d443c32e451dc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FirmID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sales</th>\n",
       "      <th>FirmDailyTraffic</th>\n",
       "      <th>TrafficPctChange</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Month</th>\n",
       "      <th>DayOfMonth</th>\n",
       "      <th>IsWeekend</th>\n",
       "      <th>FirmDailyTraffic_lag1</th>\n",
       "      <th>FirmDailyTraffic_lag2</th>\n",
       "      <th>FirmDailyTraffic_lag7</th>\n",
       "      <th>Traffic_7day_avg</th>\n",
       "      <th>Sales_lag1</th>\n",
       "      <th>Sales_lag7</th>\n",
       "      <th>Sales_7day_avg</th>\n",
       "      <th>Sales_7day_std</th>\n",
       "      <th>SalesPctChange</th>\n",
       "      <th>Sales_normalized</th>\n",
       "      <th>Sales_robust_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35150.000000</td>\n",
       "      <td>35150</td>\n",
       "      <td>35150.000000</td>\n",
       "      <td>35150.000000</td>\n",
       "      <td>35150.000000</td>\n",
       "      <td>35150.000000</td>\n",
       "      <td>35150.000000</td>\n",
       "      <td>35150.000000</td>\n",
       "      <td>35150.000000</td>\n",
       "      <td>35150.000000</td>\n",
       "      <td>35150.000000</td>\n",
       "      <td>35150.000000</td>\n",
       "      <td>35150.000000</td>\n",
       "      <td>35150.000000</td>\n",
       "      <td>35150.000000</td>\n",
       "      <td>35150.000000</td>\n",
       "      <td>35150.000000</td>\n",
       "      <td>35150.000000</td>\n",
       "      <td>3.515000e+04</td>\n",
       "      <td>35150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24.500000</td>\n",
       "      <td>2020-12-17 00:00:00</td>\n",
       "      <td>1452.387267</td>\n",
       "      <td>84.580284</td>\n",
       "      <td>0.013095</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.301565</td>\n",
       "      <td>15.668563</td>\n",
       "      <td>0.284495</td>\n",
       "      <td>84.458407</td>\n",
       "      <td>84.334708</td>\n",
       "      <td>83.736728</td>\n",
       "      <td>84.578366</td>\n",
       "      <td>1451.004730</td>\n",
       "      <td>1441.596318</td>\n",
       "      <td>1456.305806</td>\n",
       "      <td>400.772430</td>\n",
       "      <td>0.095109</td>\n",
       "      <td>-1.778884e-17</td>\n",
       "      <td>0.113955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "      <td>38.653651</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>-0.534483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>113.041813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.872923</td>\n",
       "      <td>-2.800831e+00</td>\n",
       "      <td>-2.074553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>2020-06-24 00:00:00</td>\n",
       "      <td>705.852250</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>65.285714</td>\n",
       "      <td>703.458865</td>\n",
       "      <td>692.458995</td>\n",
       "      <td>772.632402</td>\n",
       "      <td>213.581798</td>\n",
       "      <td>-0.233334</td>\n",
       "      <td>-7.617291e-01</td>\n",
       "      <td>-0.430934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24.500000</td>\n",
       "      <td>2020-12-17 00:00:00</td>\n",
       "      <td>1199.841163</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>1199.116230</td>\n",
       "      <td>1192.529214</td>\n",
       "      <td>1275.431632</td>\n",
       "      <td>340.595978</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>-1.565672e-01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>2021-06-11 00:00:00</td>\n",
       "      <td>1885.013224</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.428571</td>\n",
       "      <td>1884.072940</td>\n",
       "      <td>1879.711164</td>\n",
       "      <td>1832.466989</td>\n",
       "      <td>538.647444</td>\n",
       "      <td>0.307695</td>\n",
       "      <td>6.361991e-01</td>\n",
       "      <td>0.567331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>49.000000</td>\n",
       "      <td>2021-12-03 00:00:00</td>\n",
       "      <td>10282.143694</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>1.178571</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>186.142857</td>\n",
       "      <td>10282.143694</td>\n",
       "      <td>10282.143694</td>\n",
       "      <td>7738.411583</td>\n",
       "      <td>2224.002526</td>\n",
       "      <td>6.463783</td>\n",
       "      <td>5.187815e+00</td>\n",
       "      <td>4.410478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.431075</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1093.988957</td>\n",
       "      <td>27.006681</td>\n",
       "      <td>0.164840</td>\n",
       "      <td>1.996469</td>\n",
       "      <td>3.336144</td>\n",
       "      <td>8.824456</td>\n",
       "      <td>0.451180</td>\n",
       "      <td>27.168445</td>\n",
       "      <td>27.332043</td>\n",
       "      <td>28.142533</td>\n",
       "      <td>25.526295</td>\n",
       "      <td>1094.761503</td>\n",
       "      <td>1099.423886</td>\n",
       "      <td>1002.122141</td>\n",
       "      <td>261.346133</td>\n",
       "      <td>0.505268</td>\n",
       "      <td>9.993027e-01</td>\n",
       "      <td>0.721814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             FirmID                 Date         Sales  FirmDailyTraffic  \\\n",
       "count  35150.000000                35150  35150.000000      35150.000000   \n",
       "mean      24.500000  2020-12-17 00:00:00   1452.387267         84.580284   \n",
       "min        0.000000  2020-01-01 00:00:00     38.653651         25.000000   \n",
       "25%       12.000000  2020-06-24 00:00:00    705.852250         65.000000   \n",
       "50%       24.500000  2020-12-17 00:00:00   1199.841163         80.000000   \n",
       "75%       37.000000  2021-06-11 00:00:00   1885.013224        100.000000   \n",
       "max       49.000000  2021-12-03 00:00:00  10282.143694        222.000000   \n",
       "std       14.431075                  NaN   1093.988957         27.006681   \n",
       "\n",
       "       TrafficPctChange     DayOfWeek         Month    DayOfMonth  \\\n",
       "count      35150.000000  35150.000000  35150.000000  35150.000000   \n",
       "mean           0.013095      3.000000      6.301565     15.668563   \n",
       "min           -0.534483      0.000000      1.000000      1.000000   \n",
       "25%           -0.100000      1.000000      3.000000      8.000000   \n",
       "50%            0.000000      3.000000      6.000000     16.000000   \n",
       "75%            0.111111      5.000000      9.000000     23.000000   \n",
       "max            1.178571      6.000000     12.000000     31.000000   \n",
       "std            0.164840      1.996469      3.336144      8.824456   \n",
       "\n",
       "          IsWeekend  FirmDailyTraffic_lag1  FirmDailyTraffic_lag2  \\\n",
       "count  35150.000000           35150.000000           35150.000000   \n",
       "mean       0.284495              84.458407              84.334708   \n",
       "min        0.000000               0.000000               0.000000   \n",
       "25%        0.000000              65.000000              65.000000   \n",
       "50%        0.000000              80.000000              80.000000   \n",
       "75%        1.000000             100.000000             100.000000   \n",
       "max        1.000000             222.000000             222.000000   \n",
       "std        0.451180              27.168445              27.332043   \n",
       "\n",
       "       FirmDailyTraffic_lag7  Traffic_7day_avg    Sales_lag1    Sales_lag7  \\\n",
       "count           35150.000000      35150.000000  35150.000000  35150.000000   \n",
       "mean               83.736728         84.578366   1451.004730   1441.596318   \n",
       "min                 0.000000         34.857143      0.000000      0.000000   \n",
       "25%                64.000000         65.285714    703.458865    692.458995   \n",
       "50%                79.000000         79.000000   1199.116230   1192.529214   \n",
       "75%               100.000000        100.428571   1884.072940   1879.711164   \n",
       "max               222.000000        186.142857  10282.143694  10282.143694   \n",
       "std                28.142533         25.526295   1094.761503   1099.423886   \n",
       "\n",
       "       Sales_7day_avg  Sales_7day_std  SalesPctChange  Sales_normalized  \\\n",
       "count    35150.000000    35150.000000    35150.000000      3.515000e+04   \n",
       "mean      1456.305806      400.772430        0.095109     -1.778884e-17   \n",
       "min        113.041813        0.000000       -0.872923     -2.800831e+00   \n",
       "25%        772.632402      213.581798       -0.233334     -7.617291e-01   \n",
       "50%       1275.431632      340.595978        0.001310     -1.565672e-01   \n",
       "75%       1832.466989      538.647444        0.307695      6.361991e-01   \n",
       "max       7738.411583     2224.002526        6.463783      5.187815e+00   \n",
       "std       1002.122141      261.346133        0.505268      9.993027e-01   \n",
       "\n",
       "       Sales_robust_scaled  \n",
       "count         35150.000000  \n",
       "mean              0.113955  \n",
       "min              -2.074553  \n",
       "25%              -0.430934  \n",
       "50%               0.000000  \n",
       "75%               0.567331  \n",
       "max               4.410478  \n",
       "std               0.721814  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4933321-b906-4ee3-85a1-811cf8a9d2a4",
   "metadata": {},
   "source": [
    "# Seasonality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b036fba5-8736-4246-b153-aa16b144b77c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing enhanced seasonality analysis for sales and traffic...\n",
      "Using data up to 2021-12-03 00:00:00 for seasonality analysis\n",
      "\n",
      "Analyzing seasonality patterns for Sales...\n",
      "Using data up to 2021-12-03 00:00:00 for analysis\n",
      "Analyzing overall daily patterns (day of week)...\n",
      "Overall day of week analysis for Sales saved\n",
      "Analyzing overall monthly patterns...\n",
      "Overall monthly analysis for Sales saved\n",
      "Performing firm-level seasonality analysis for Sales...\n",
      "Found 50 firms with sufficient data for analysis\n",
      "Analyzing firm 0.0 (1/50)\n",
      "Analyzing firm 1.0 (2/50)\n",
      "Analyzing firm 2.0 (3/50)\n",
      "Analyzing firm 3.0 (4/50)\n",
      "Analyzing firm 4.0 (5/50)\n",
      "Analyzing firm 5.0 (6/50)\n",
      "Analyzing firm 6.0 (7/50)\n",
      "Analyzing firm 7.0 (8/50)\n",
      "Analyzing firm 8.0 (9/50)\n",
      "Analyzing firm 9.0 (10/50)\n",
      "Analyzing firm 10.0 (11/50)\n",
      "Analyzing firm 11.0 (12/50)\n",
      "Analyzing firm 12.0 (13/50)\n",
      "Analyzing firm 13.0 (14/50)\n",
      "Analyzing firm 14.0 (15/50)\n",
      "Analyzing firm 15.0 (16/50)\n",
      "Analyzing firm 16.0 (17/50)\n",
      "Analyzing firm 17.0 (18/50)\n",
      "Analyzing firm 18.0 (19/50)\n",
      "Analyzing firm 19.0 (20/50)\n",
      "Analyzing firm 20.0 (21/50)\n",
      "Analyzing firm 21.0 (22/50)\n",
      "Analyzing firm 22.0 (23/50)\n",
      "Analyzing firm 23.0 (24/50)\n",
      "Analyzing firm 24.0 (25/50)\n",
      "Analyzing firm 25.0 (26/50)\n",
      "Analyzing firm 26.0 (27/50)\n",
      "Analyzing firm 27.0 (28/50)\n",
      "Analyzing firm 28.0 (29/50)\n",
      "Analyzing firm 29.0 (30/50)\n",
      "Analyzing firm 30.0 (31/50)\n",
      "Analyzing firm 31.0 (32/50)\n",
      "Analyzing firm 32.0 (33/50)\n",
      "Analyzing firm 33.0 (34/50)\n",
      "Analyzing firm 34.0 (35/50)\n",
      "Analyzing firm 35.0 (36/50)\n",
      "Analyzing firm 36.0 (37/50)\n",
      "Analyzing firm 37.0 (38/50)\n",
      "Analyzing firm 38.0 (39/50)\n",
      "Analyzing firm 39.0 (40/50)\n",
      "Analyzing firm 40.0 (41/50)\n",
      "Analyzing firm 41.0 (42/50)\n",
      "Analyzing firm 42.0 (43/50)\n",
      "Analyzing firm 43.0 (44/50)\n",
      "Analyzing firm 44.0 (45/50)\n",
      "Analyzing firm 45.0 (46/50)\n",
      "Analyzing firm 46.0 (47/50)\n",
      "Analyzing firm 47.0 (48/50)\n",
      "Analyzing firm 48.0 (49/50)\n",
      "Analyzing firm 49.0 (50/50)\n",
      "Seasonal strength comparison by firm for Sales saved\n",
      "Analyzing overall weekend vs. weekday patterns...\n",
      "Overall weekend vs. weekday analysis for Sales saved\n",
      "Creating overall day of week by month heatmap...\n",
      "Overall day of week by month heatmap for Sales saved\n",
      "\n",
      "Analyzing seasonality patterns for FirmDailyTraffic...\n",
      "Using data up to 2021-12-03 00:00:00 for analysis\n",
      "Analyzing overall daily patterns (day of week)...\n",
      "Overall day of week analysis for FirmDailyTraffic saved\n",
      "Analyzing overall monthly patterns...\n",
      "Overall monthly analysis for FirmDailyTraffic saved\n",
      "Performing firm-level seasonality analysis for FirmDailyTraffic...\n",
      "Found 50 firms with sufficient data for analysis\n",
      "Analyzing firm 0.0 (1/50)\n",
      "Analyzing firm 1.0 (2/50)\n",
      "Analyzing firm 2.0 (3/50)\n",
      "Analyzing firm 3.0 (4/50)\n",
      "Analyzing firm 4.0 (5/50)\n",
      "Analyzing firm 5.0 (6/50)\n",
      "Analyzing firm 6.0 (7/50)\n",
      "Analyzing firm 7.0 (8/50)\n",
      "Analyzing firm 8.0 (9/50)\n",
      "Analyzing firm 9.0 (10/50)\n",
      "Analyzing firm 10.0 (11/50)\n",
      "Analyzing firm 11.0 (12/50)\n",
      "Analyzing firm 12.0 (13/50)\n",
      "Analyzing firm 13.0 (14/50)\n",
      "Analyzing firm 14.0 (15/50)\n",
      "Analyzing firm 15.0 (16/50)\n",
      "Analyzing firm 16.0 (17/50)\n",
      "Analyzing firm 17.0 (18/50)\n",
      "Analyzing firm 18.0 (19/50)\n",
      "Analyzing firm 19.0 (20/50)\n",
      "Analyzing firm 20.0 (21/50)\n",
      "Analyzing firm 21.0 (22/50)\n",
      "Analyzing firm 22.0 (23/50)\n",
      "Analyzing firm 23.0 (24/50)\n",
      "Analyzing firm 24.0 (25/50)\n",
      "Analyzing firm 25.0 (26/50)\n",
      "Analyzing firm 26.0 (27/50)\n",
      "Analyzing firm 27.0 (28/50)\n",
      "Analyzing firm 28.0 (29/50)\n",
      "Analyzing firm 29.0 (30/50)\n",
      "Analyzing firm 30.0 (31/50)\n",
      "Analyzing firm 31.0 (32/50)\n",
      "Analyzing firm 32.0 (33/50)\n",
      "Analyzing firm 33.0 (34/50)\n",
      "Analyzing firm 34.0 (35/50)\n",
      "Analyzing firm 35.0 (36/50)\n",
      "Analyzing firm 36.0 (37/50)\n",
      "Analyzing firm 37.0 (38/50)\n",
      "Analyzing firm 38.0 (39/50)\n",
      "Analyzing firm 39.0 (40/50)\n",
      "Analyzing firm 40.0 (41/50)\n",
      "Analyzing firm 41.0 (42/50)\n",
      "Analyzing firm 42.0 (43/50)\n",
      "Analyzing firm 43.0 (44/50)\n",
      "Analyzing firm 44.0 (45/50)\n",
      "Analyzing firm 45.0 (46/50)\n",
      "Analyzing firm 46.0 (47/50)\n",
      "Analyzing firm 47.0 (48/50)\n",
      "Analyzing firm 48.0 (49/50)\n",
      "Analyzing firm 49.0 (50/50)\n",
      "Seasonal strength comparison by firm for FirmDailyTraffic saved\n",
      "Analyzing overall weekend vs. weekday patterns...\n",
      "Overall weekend vs. weekday analysis for FirmDailyTraffic saved\n",
      "Creating overall day of week by month heatmap...\n",
      "Overall day of week by month heatmap for FirmDailyTraffic saved\n",
      "\n",
      "Analyzing relationship between Sales and Traffic seasonality patterns...\n",
      "Day 0 (Monday): Correlation = 0.7137, p-value = 0.0000, n = 50\n",
      "Day 1 (Tuesday): Correlation = 0.7195, p-value = 0.0000, n = 50\n",
      "Day 2 (Wednesday): Correlation = 0.7215, p-value = 0.0000, n = 50\n",
      "Day 3 (Thursday): Correlation = 0.7291, p-value = 0.0000, n = 50\n",
      "Day 4 (Friday): Correlation = 0.7213, p-value = 0.0000, n = 50\n",
      "Day 5 (Saturday): Correlation = 0.6846, p-value = 0.0000, n = 50\n",
      "Day 6 (Sunday): Correlation = 0.6807, p-value = 0.0000, n = 50\n",
      "Sales-Traffic correlation by day of week analysis saved\n",
      "Firm 0.0: Correlation = -0.0141, p-value = 0.7093, n = 703\n",
      "Firm 1.0: Correlation = 0.1390, p-value = 0.0002, n = 703\n",
      "Firm 2.0: Correlation = 0.0320, p-value = 0.3969, n = 703\n",
      "Firm 3.0: Correlation = 0.2591, p-value = 0.0000, n = 703\n",
      "Firm 4.0: Correlation = 0.1865, p-value = 0.0000, n = 703\n",
      "Firm 5.0: Correlation = 0.2577, p-value = 0.0000, n = 703\n",
      "Firm 6.0: Correlation = 0.3521, p-value = 0.0000, n = 703\n",
      "Firm 7.0: Correlation = 0.1486, p-value = 0.0001, n = 703\n",
      "Firm 8.0: Correlation = 0.1541, p-value = 0.0000, n = 703\n",
      "Firm 9.0: Correlation = 0.2578, p-value = 0.0000, n = 703\n",
      "Firm 10.0: Correlation = 0.1650, p-value = 0.0000, n = 703\n",
      "Firm 11.0: Correlation = -0.0097, p-value = 0.7979, n = 703\n",
      "Firm 12.0: Correlation = 0.1353, p-value = 0.0003, n = 703\n",
      "Firm 13.0: Correlation = 0.2138, p-value = 0.0000, n = 703\n",
      "Firm 14.0: Correlation = 0.1423, p-value = 0.0002, n = 703\n",
      "Firm 15.0: Correlation = 0.1203, p-value = 0.0014, n = 703\n",
      "Firm 16.0: Correlation = 0.1124, p-value = 0.0028, n = 703\n",
      "Firm 17.0: Correlation = 0.1014, p-value = 0.0071, n = 703\n",
      "Firm 18.0: Correlation = 0.0046, p-value = 0.9028, n = 703\n",
      "Firm 19.0: Correlation = 0.1471, p-value = 0.0001, n = 703\n",
      "Firm 20.0: Correlation = 0.3312, p-value = 0.0000, n = 703\n",
      "Firm 21.0: Correlation = 0.0491, p-value = 0.1933, n = 703\n",
      "Firm 22.0: Correlation = 0.2535, p-value = 0.0000, n = 703\n",
      "Firm 23.0: Correlation = 0.2462, p-value = 0.0000, n = 703\n",
      "Firm 24.0: Correlation = 0.0530, p-value = 0.1605, n = 703\n",
      "Firm 25.0: Correlation = 0.0876, p-value = 0.0202, n = 703\n",
      "Firm 26.0: Correlation = -0.0311, p-value = 0.4103, n = 703\n",
      "Firm 27.0: Correlation = 0.1098, p-value = 0.0035, n = 703\n",
      "Firm 28.0: Correlation = 0.1398, p-value = 0.0002, n = 703\n",
      "Firm 29.0: Correlation = 0.1659, p-value = 0.0000, n = 703\n",
      "Firm 30.0: Correlation = 0.2029, p-value = 0.0000, n = 703\n",
      "Firm 31.0: Correlation = 0.2351, p-value = 0.0000, n = 703\n",
      "Firm 32.0: Correlation = 0.0561, p-value = 0.1371, n = 703\n",
      "Firm 33.0: Correlation = 0.2555, p-value = 0.0000, n = 703\n",
      "Firm 34.0: Correlation = 0.2674, p-value = 0.0000, n = 703\n",
      "Firm 35.0: Correlation = 0.2896, p-value = 0.0000, n = 703\n",
      "Firm 36.0: Correlation = 0.1362, p-value = 0.0003, n = 703\n",
      "Firm 37.0: Correlation = 0.1388, p-value = 0.0002, n = 703\n",
      "Firm 38.0: Correlation = 0.3233, p-value = 0.0000, n = 703\n",
      "Firm 39.0: Correlation = 0.1377, p-value = 0.0002, n = 703\n",
      "Firm 40.0: Correlation = 0.2315, p-value = 0.0000, n = 703\n",
      "Firm 41.0: Correlation = 0.3194, p-value = 0.0000, n = 703\n",
      "Firm 42.0: Correlation = 0.0841, p-value = 0.0257, n = 703\n",
      "Firm 43.0: Correlation = 0.1113, p-value = 0.0031, n = 703\n",
      "Firm 44.0: Correlation = 0.1709, p-value = 0.0000, n = 703\n",
      "Firm 45.0: Correlation = 0.1292, p-value = 0.0006, n = 703\n",
      "Firm 46.0: Correlation = 0.2169, p-value = 0.0000, n = 703\n",
      "Firm 47.0: Correlation = 0.1154, p-value = 0.0022, n = 703\n",
      "Firm 48.0: Correlation = 0.2181, p-value = 0.0000, n = 703\n",
      "Firm 49.0: Correlation = -0.0041, p-value = 0.9144, n = 703\n",
      "Sales-Traffic correlation by firm analysis saved\n",
      "\n",
      "Summary of Sales-Traffic correlations by firm:\n",
      "Number of firms analyzed: 50\n",
      "Firms with positive correlation: 46 (92.0%)\n",
      "Firms with negative correlation: 4 (8.0%)\n",
      "Firms with statistically significant correlation (p < 0.05): 41 (82.0%)\n",
      "Average correlation: 0.159\n",
      "Median correlation: 0.145\n",
      "Sales-Traffic correlation strength distribution visualization saved\n",
      "Firm-specific day of week pattern comparison saved\n",
      "Sales vs. Traffic by day of week visualization saved\n",
      "Sales-Traffic correlation by month analysis saved\n",
      "Sales and Traffic pattern comparison saved\n",
      "\n",
      "Performing cluster analysis of seasonality patterns...\n",
      "Sales seasonality cluster analysis saved with 4 clusters\n",
      "Sales seasonality cluster distribution visualization saved\n",
      "\n",
      "Sales seasonality pattern clustering summary:\n",
      "Number of firms analyzed: 50\n",
      "Optimal number of clusters: 4\n",
      "Cluster distribution:\n",
      "  Cluster 1: 18 firms (36.0%)\n",
      "  Cluster 2: 19 firms (38.0%)\n",
      "  Cluster 3: 3 firms (6.0%)\n",
      "  Cluster 4: 10 firms (20.0%)\n",
      "Sales seasonality cluster examples visualization saved\n",
      "\n",
      "Analyzing daily differences in Traffic-Sales relationship...\n",
      "Traffic-Sales relationship analysis by day of week saved\n",
      "Traffic-to-Sales conversion rate analysis by day of week saved\n",
      "\n",
      "Enhanced seasonality analysis complete!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('firm_seasonality_visualizations'):\n",
    "    os.makedirs('firm_seasonality_visualizations')\n",
    "\n",
    "print(\"\\nPerforming enhanced seasonality analysis for sales and traffic...\")\n",
    "\n",
    "def analyze_seasonality(df_model, target_column, cutoff_date=None, by_firm=True):\n",
    "    \"\"\"\n",
    "    Perform comprehensive seasonality analysis on a time series\n",
    "    \n",
    "    Args:\n",
    "        df_model: DataFrame with time series data\n",
    "        target_column: Column to analyze (e.g., 'Sales' or 'FirmDailyTraffic')\n",
    "        cutoff_date: Optional date to split training/forecast periods\n",
    "        by_firm: Whether to perform analysis for each firm separately\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    print(f\"\\nAnalyzing seasonality patterns for {target_column}...\")\n",
    "    results = {}\n",
    "    \n",
    "    if cutoff_date:\n",
    "        print(f\"Using data up to {cutoff_date} for analysis\")\n",
    "        analysis_data = df_model[df_model['Date'] <= cutoff_date].copy()\n",
    "    else:\n",
    "        analysis_data = df_model.copy()\n",
    "    \n",
    "    required_cols = ['FirmID', 'Date', 'DayOfWeek', 'Month', 'IsWeekend', target_column]\n",
    "    missing_cols = [col for col in required_cols if col not in analysis_data.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing required columns for analysis: {missing_cols}\")\n",
    "        if 'Date' in analysis_data.columns:\n",
    "            if 'DayOfWeek' not in analysis_data.columns:\n",
    "                analysis_data['DayOfWeek'] = analysis_data['Date'].dt.dayofweek\n",
    "                print(\"Created 'DayOfWeek' column from Date\")\n",
    "            \n",
    "            if 'Month' not in analysis_data.columns:\n",
    "                analysis_data['Month'] = analysis_data['Date'].dt.month\n",
    "                print(\"Created 'Month' column from Date\")\n",
    "            \n",
    "            if 'IsWeekend' not in analysis_data.columns:\n",
    "                analysis_data['IsWeekend'] = analysis_data['DayOfWeek'].isin([5, 6]).astype(int)\n",
    "                print(\"Created 'IsWeekend' column from DayOfWeek\")\n",
    "    \n",
    "    print(\"Analyzing overall daily patterns (day of week)...\")\n",
    "    dow_data = analysis_data.groupby('DayOfWeek')[target_column].agg(['mean', 'median', 'std', 'count'])\n",
    "    dow_data['day_name'] = [calendar.day_name[day] for day in dow_data.index]\n",
    "    dow_data = dow_data.sort_index() \n",
    "    \n",
    "    fig_dow = go.Figure()\n",
    "    \n",
    "    fig_dow.add_trace(go.Bar(\n",
    "        x=dow_data['day_name'], \n",
    "        y=dow_data['mean'],\n",
    "        name='Mean',\n",
    "        error_y=dict(\n",
    "            type='data', \n",
    "            array=dow_data['std'] / np.sqrt(dow_data['count']),  \n",
    "            visible=True\n",
    "        )\n",
    "    ))\n",
    "    \n",
    "    fig_dow.add_trace(go.Scatter(\n",
    "        x=dow_data['day_name'],\n",
    "        y=dow_data['median'],\n",
    "        mode='lines+markers',\n",
    "        name='Median',\n",
    "        line=dict(color='red', width=2)\n",
    "    ))\n",
    "    \n",
    "    fig_dow.update_layout(\n",
    "        title=f\"Overall {target_column} by Day of Week\",\n",
    "        xaxis_title=\"Day of Week\",\n",
    "        yaxis_title=target_column,\n",
    "        template=\"plotly_white\",\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
    "    )\n",
    "    \n",
    "    fig_dow.write_html(f\"overall_{target_column.lower()}_by_day_of_week.html\")\n",
    "    print(f\"Overall day of week analysis for {target_column} saved\")\n",
    "    \n",
    "    results['overall_day_of_week'] = dow_data\n",
    "    \n",
    "    print(\"Analyzing overall monthly patterns...\")\n",
    "    month_data = analysis_data.groupby('Month')[target_column].agg(['mean', 'median', 'std', 'count'])\n",
    "    month_data['month_name'] = [calendar.month_name[month] for month in month_data.index]\n",
    "    month_data = month_data.sort_index()\n",
    "\n",
    "    fig_month = go.Figure()\n",
    "    \n",
    "    fig_month.add_trace(go.Bar(\n",
    "        x=month_data['month_name'], \n",
    "        y=month_data['mean'],\n",
    "        name='Mean',\n",
    "        error_y=dict(\n",
    "            type='data', \n",
    "            array=month_data['std'] / np.sqrt(month_data['count']), \n",
    "            visible=True\n",
    "        )\n",
    "    ))\n",
    "    \n",
    "    fig_month.add_trace(go.Scatter(\n",
    "        x=month_data['month_name'],\n",
    "        y=month_data['median'],\n",
    "        mode='lines+markers',\n",
    "        name='Median',\n",
    "        line=dict(color='red', width=2)\n",
    "    ))\n",
    "    \n",
    "    fig_month.update_layout(\n",
    "        title=f\"Overall {target_column} by Month\",\n",
    "        xaxis_title=\"Month\",\n",
    "        yaxis_title=target_column,\n",
    "        template=\"plotly_white\",\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
    "    )\n",
    "    \n",
    "    fig_month.write_html(f\"overall_{target_column.lower()}_by_month.html\")\n",
    "    print(f\"Overall monthly analysis for {target_column} saved\")\n",
    "    \n",
    "    results['overall_month'] = month_data\n",
    "    \n",
    "    if by_firm:\n",
    "        print(f\"Performing firm-level seasonality analysis for {target_column}...\")\n",
    "        \n",
    "        firm_counts = analysis_data.groupby('FirmID').size()\n",
    "        firms_to_analyze = firm_counts[firm_counts >= 30].index\n",
    "        \n",
    "        print(f\"Found {len(firms_to_analyze)} firms with sufficient data for analysis\")\n",
    "        \n",
    "        firm_results = {}\n",
    "        \n",
    "        seasonal_strengths = []\n",
    "        \n",
    "        for i, firm_id in enumerate(firms_to_analyze):\n",
    "            print(f\"Analyzing firm {firm_id} ({i+1}/{len(firms_to_analyze)})\")\n",
    "            \n",
    "            firm_data = analysis_data[analysis_data['FirmID'] == firm_id].sort_values('Date')\n",
    "            \n",
    "            try:\n",
    "                firm_dow = firm_data.groupby('DayOfWeek')[target_column].agg(['mean', 'median', 'std', 'count'])\n",
    "                firm_dow['day_name'] = [calendar.day_name[day] for day in firm_dow.index]\n",
    "                firm_dow = firm_dow.sort_index()\n",
    "                \n",
    "                fig_firm_dow = go.Figure()\n",
    "                \n",
    "                fig_firm_dow.add_trace(go.Bar(\n",
    "                    x=firm_dow['day_name'], \n",
    "                    y=firm_dow['mean'],\n",
    "                    name='Mean',\n",
    "                    error_y=dict(\n",
    "                        type='data', \n",
    "                        array=firm_dow['std'] / np.sqrt(firm_dow['count']),\n",
    "                        visible=True\n",
    "                    )\n",
    "                ))\n",
    "                \n",
    "                fig_firm_dow.add_trace(go.Scatter(\n",
    "                    x=firm_dow['day_name'],\n",
    "                    y=firm_dow['median'],\n",
    "                    mode='lines+markers',\n",
    "                    name='Median',\n",
    "                    line=dict(color='red', width=2)\n",
    "                ))\n",
    "                \n",
    "                fig_firm_dow.update_layout(\n",
    "                    title=f\"Firm {firm_id}: {target_column} by Day of Week\",\n",
    "                    xaxis_title=\"Day of Week\",\n",
    "                    yaxis_title=target_column,\n",
    "                    template=\"plotly_white\",\n",
    "                    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
    "                )\n",
    "                \n",
    "                fig_firm_dow.write_html(f\"firm_seasonality_visualizations/firm_{firm_id}_{target_column.lower()}_by_day_of_week.html\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in day of week analysis for firm {firm_id}: {e}\")\n",
    "            \n",
    "            try:\n",
    "                num_months = firm_data['Month'].nunique()\n",
    "                if num_months >= 2:\n",
    "                    firm_month = firm_data.groupby('Month')[target_column].agg(['mean', 'median', 'std', 'count'])\n",
    "                    firm_month['month_name'] = [calendar.month_name[month] for month in firm_month.index]\n",
    "                    firm_month = firm_month.sort_index()\n",
    "                    \n",
    "                    fig_firm_month = go.Figure()\n",
    "                    \n",
    "                    fig_firm_month.add_trace(go.Bar(\n",
    "                        x=firm_month['month_name'], \n",
    "                        y=firm_month['mean'],\n",
    "                        name='Mean',\n",
    "                        error_y=dict(\n",
    "                            type='data', \n",
    "                            array=firm_month['std'] / np.sqrt(firm_month['count']),\n",
    "                            visible=True\n",
    "                        )\n",
    "                    ))\n",
    "                    \n",
    "                    fig_firm_month.add_trace(go.Scatter(\n",
    "                        x=firm_month['month_name'],\n",
    "                        y=firm_month['median'],\n",
    "                        mode='lines+markers',\n",
    "                        name='Median',\n",
    "                        line=dict(color='red', width=2)\n",
    "                    ))\n",
    "                    fig_firm_month.update_layout(\n",
    "                        title=f\"Firm {firm_id}: {target_column} by Month\",\n",
    "                        xaxis_title=\"Month\",\n",
    "                        yaxis_title=target_column,\n",
    "                        template=\"plotly_white\",\n",
    "                        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
    "                    )\n",
    "                    fig_firm_month.write_html(f\"firm_seasonality_visualizations/firm_{firm_id}_{target_column.lower()}_by_month.html\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in monthly analysis for firm {firm_id}: {e}\")\n",
    "                \n",
    "            try:\n",
    "                ts_data = firm_data.set_index('Date')[target_column].resample('D').mean()\n",
    "                ts_data = ts_data.interpolate(method='linear')\n",
    "                \n",
    "                if len(ts_data) >= 14:\n",
    "                    decomposition = seasonal_decompose(ts_data, model='additive', period=7)\n",
    "                    \n",
    "                    fig_decomp = make_subplots(\n",
    "                        rows=4, cols=1,\n",
    "                        subplot_titles=[\"Original\", \"Trend\", \"Seasonality\", \"Residual\"],\n",
    "                        vertical_spacing=0.1,\n",
    "                        shared_xaxes=True\n",
    "                    )\n",
    "\n",
    "                    fig_decomp.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=decomposition.observed.index, \n",
    "                            y=decomposition.observed.values,\n",
    "                            mode='lines',\n",
    "                            name='Original',\n",
    "                            line=dict(color='blue')\n",
    "                        ),\n",
    "                        row=1, col=1\n",
    "                    )\n",
    "\n",
    "                    fig_decomp.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=decomposition.trend.index, \n",
    "                            y=decomposition.trend.values,\n",
    "                            mode='lines',\n",
    "                            name='Trend',\n",
    "                            line=dict(color='red')\n",
    "                        ),\n",
    "                        row=2, col=1\n",
    "                    )\n",
    "\n",
    "                    fig_decomp.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=decomposition.seasonal.index, \n",
    "                            y=decomposition.seasonal.values,\n",
    "                            mode='lines',\n",
    "                            name='Seasonality',\n",
    "                            line=dict(color='green')\n",
    "                        ),\n",
    "                        row=3, col=1\n",
    "                    )\n",
    "\n",
    "                    fig_decomp.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=decomposition.resid.index, \n",
    "                            y=decomposition.resid.values,\n",
    "                            mode='lines',\n",
    "                            name='Residual',\n",
    "                            line=dict(color='purple')\n",
    "                        ),\n",
    "                        row=4, col=1\n",
    "                    )\n",
    "\n",
    "                    fig_decomp.update_layout(\n",
    "                        height=800,\n",
    "                        width=1000,\n",
    "                        title_text=f\"Firm {firm_id}: Time Series Decomposition for {target_column}\",\n",
    "                        template=\"plotly_white\",\n",
    "                        showlegend=True,\n",
    "                        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
    "                    )\n",
    "\n",
    "                    fig_decomp.write_html(f\"firm_seasonality_visualizations/firm_{firm_id}_{target_column.lower()}_decomposition.html\")\n",
    "\n",
    "                    try:\n",
    "                        seasonal_strength = 1 - (np.var(decomposition.resid.dropna()) / \n",
    "                                              np.var((decomposition.seasonal + decomposition.resid).dropna()))\n",
    "                        seasonal_strength = max(0, seasonal_strength)\n",
    "                        \n",
    "                        seasonal_strengths.append({\n",
    "                            'FirmID': firm_id,\n",
    "                            'SeasonalStrength': seasonal_strength,\n",
    "                            'DataPoints': len(firm_data),\n",
    "                            'DateRange': f\"{firm_data['Date'].min().date()} to {firm_data['Date'].max().date()}\"\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error calculating seasonal strength for firm {firm_id}: {e}\")\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"Not enough data for time series decomposition for firm {firm_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in time series decomposition for firm {firm_id}: {e}\")\n",
    "\n",
    "            try:\n",
    "                firm_weekend = firm_data.groupby('IsWeekend')[target_column].agg(['mean', 'median', 'std', 'count'])\n",
    "                if len(firm_weekend) >= 2:  # Need both weekend and weekday data\n",
    "                    firm_weekend['day_type'] = ['Weekday', 'Weekend']\n",
    "\n",
    "                    fig_weekend = go.Figure()\n",
    "\n",
    "                    fig_weekend.add_trace(go.Bar(\n",
    "                        x=firm_weekend['day_type'], \n",
    "                        y=firm_weekend['mean'],\n",
    "                        name='Mean',\n",
    "                        error_y=dict(\n",
    "                            type='data', \n",
    "                            array=firm_weekend['std'] / np.sqrt(firm_weekend['count']),  # Standard error\n",
    "                            visible=True\n",
    "                        )\n",
    "                    ))\n",
    "\n",
    "                    fig_weekend.add_trace(go.Scatter(\n",
    "                        x=firm_weekend['day_type'],\n",
    "                        y=firm_weekend['median'],\n",
    "                        mode='markers',\n",
    "                        name='Median',\n",
    "                        marker=dict(color='red', size=10, symbol='diamond')\n",
    "                    ))\n",
    "\n",
    "                    fig_weekend.update_layout(\n",
    "                        title=f\"Firm {firm_id}: {target_column} Weekend vs. Weekday Comparison\",\n",
    "                        xaxis_title=\"Day Type\",\n",
    "                        yaxis_title=target_column,\n",
    "                        template=\"plotly_white\",\n",
    "                        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
    "                    )\n",
    "\n",
    "                    fig_weekend.write_html(f\"firm_seasonality_visualizations/firm_{firm_id}_{target_column.lower()}_weekend_comparison.html\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error in weekend vs. weekday analysis for firm {firm_id}: {e}\")\n",
    "\n",
    "        if seasonal_strengths:\n",
    "            seasonal_df = pd.DataFrame(seasonal_strengths)\n",
    "\n",
    "            seasonal_df = seasonal_df.sort_values('SeasonalStrength', ascending=False)\n",
    "\n",
    "            fig_seasonal = go.Figure()\n",
    "\n",
    "            fig_seasonal.add_trace(go.Bar(\n",
    "                x=seasonal_df['FirmID'],\n",
    "                y=seasonal_df['SeasonalStrength'],\n",
    "                text=[f\"{s:.3f}\" for s in seasonal_df['SeasonalStrength']],\n",
    "                textposition='outside',\n",
    "                marker_color='blue'\n",
    "            ))\n",
    "\n",
    "            fig_seasonal.add_shape(\n",
    "                type='line',\n",
    "                x0=-0.5,\n",
    "                x1=len(seasonal_df)-0.5,\n",
    "                y0=0.5,\n",
    "                y1=0.5,\n",
    "                line=dict(color='green', dash='dash', width=2)\n",
    "            )\n",
    "            \n",
    "            fig_seasonal.add_shape(\n",
    "                type='line',\n",
    "                x0=-0.5,\n",
    "                x1=len(seasonal_df)-0.5,\n",
    "                y0=0.3,\n",
    "                y1=0.3,\n",
    "                line=dict(color='orange', dash='dash', width=2)\n",
    "            )\n",
    "\n",
    "            fig_seasonal.add_annotation(\n",
    "                x=len(seasonal_df)-1,\n",
    "                y=0.5,\n",
    "                text=\"Strong Seasonality\",\n",
    "                showarrow=False,\n",
    "                yshift=10,\n",
    "                font=dict(color='green')\n",
    "            )\n",
    "            \n",
    "            fig_seasonal.add_annotation(\n",
    "                x=len(seasonal_df)-1,\n",
    "                y=0.3,\n",
    "                text=\"Moderate Seasonality\",\n",
    "                showarrow=False,\n",
    "                yshift=10,\n",
    "                font=dict(color='orange')\n",
    "            )\n",
    "\n",
    "            fig_seasonal.update_layout(\n",
    "                title=f\"{target_column} Seasonal Strength by Firm\",\n",
    "                xaxis_title=\"Firm ID\",\n",
    "                yaxis_title=\"Seasonal Strength (0-1)\",\n",
    "                template=\"plotly_white\",\n",
    "                yaxis=dict(range=[0, 1]),\n",
    "                height=600,\n",
    "                width=max(800, len(seasonal_df) * 50)\n",
    "            )\n",
    "\n",
    "            fig_seasonal.write_html(f\"{target_column.lower()}_seasonal_strength_by_firm.html\")\n",
    "            print(f\"Seasonal strength comparison by firm for {target_column} saved\")\n",
    "\n",
    "            results['firm_seasonal_strength'] = seasonal_df\n",
    "\n",
    "        results['firm_results'] = firm_results\n",
    "\n",
    "    print(\"Analyzing overall weekend vs. weekday patterns...\")\n",
    "    weekend_data = analysis_data.groupby('IsWeekend')[target_column].agg(['mean', 'median', 'std', 'count'])\n",
    "    weekend_data['day_type'] = ['Weekday', 'Weekend']\n",
    "\n",
    "    fig_weekend = go.Figure()\n",
    "\n",
    "    fig_weekend.add_trace(go.Bar(\n",
    "        x=weekend_data['day_type'], \n",
    "        y=weekend_data['mean'],\n",
    "        name='Mean',\n",
    "        error_y=dict(\n",
    "            type='data', \n",
    "            array=weekend_data['std'] / np.sqrt(weekend_data['count']), \n",
    "            visible=True\n",
    "        )\n",
    "    ))\n",
    "\n",
    "    fig_weekend.add_trace(go.Scatter(\n",
    "        x=weekend_data['day_type'],\n",
    "        y=weekend_data['median'],\n",
    "        mode='markers',\n",
    "        name='Median',\n",
    "        marker=dict(color='red', size=10, symbol='diamond')\n",
    "    ))\n",
    "\n",
    "    fig_weekend.update_layout(\n",
    "        title=f\"Overall {target_column}: Weekend vs. Weekday Comparison\",\n",
    "        xaxis_title=\"Day Type\",\n",
    "        yaxis_title=target_column,\n",
    "        template=\"plotly_white\",\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
    "    )\n",
    "\n",
    "    fig_weekend.write_html(f\"overall_{target_column.lower()}_weekend_comparison.html\")\n",
    "    print(f\"Overall weekend vs. weekday analysis for {target_column} saved\")\n",
    "    \n",
    "    results['overall_weekend'] = weekend_data\n",
    "\n",
    "    print(\"Creating overall day of week by month heatmap...\")\n",
    "\n",
    "    heatmap_data = analysis_data.groupby(['Month', 'DayOfWeek'])[target_column].mean().reset_index()\n",
    "\n",
    "    if len(heatmap_data) > 0:\n",
    "        heatmap_pivot = heatmap_data.pivot(index='Month', columns='DayOfWeek', values=target_column)\n",
    "\n",
    "        day_names = [calendar.day_name[i] for i in range(7)]\n",
    "        month_names = [calendar.month_name[i] for i in range(1, 13)]\n",
    "\n",
    "        fig_heatmap = go.Figure(data=go.Heatmap(\n",
    "            z=heatmap_pivot.values,\n",
    "            x=[day_names[i] if i in heatmap_pivot.columns else \"\" for i in range(7)],\n",
    "            y=[month_names[i-1] if i in heatmap_pivot.index else \"\" for i in range(1, 13)],\n",
    "            colorscale='Viridis',\n",
    "            colorbar=dict(title=target_column)\n",
    "        ))\n",
    "\n",
    "        fig_heatmap.update_layout(\n",
    "            title=f\"Overall {target_column} by Day of Week and Month\",\n",
    "            xaxis_title=\"Day of Week\",\n",
    "            yaxis_title=\"Month\",\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "\n",
    "        fig_heatmap.write_html(f\"overall_{target_column.lower()}_dow_month_heatmap.html\")\n",
    "        print(f\"Overall day of week by month heatmap for {target_column} saved\")\n",
    "        \n",
    "        results['overall_heatmap'] = heatmap_pivot\n",
    "    else:\n",
    "        print(f\"Not enough data for day of week by month heatmap for {target_column}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "try:\n",
    "    last_date = df_train['Date'].max()\n",
    "\n",
    "    cutoff_date = last_date\n",
    "    \n",
    "    print(f\"Using data up to {cutoff_date} for seasonality analysis\")\n",
    "\n",
    "    sales_seasonality = analyze_seasonality(df_train, 'Sales', cutoff_date)\n",
    "\n",
    "    traffic_seasonality = analyze_seasonality(df_train, 'FirmDailyTraffic', cutoff_date)\n",
    "except Exception as e:\n",
    "    print(f\"Error in seasonality analysis: {e}\")\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Enhanced Analysis of Relationship between Sales and Traffic \n",
    "#-----------------------------------------------------------\n",
    "try:\n",
    "    print(\"\\nAnalyzing relationship between Sales and Traffic seasonality patterns...\")\n",
    "\n",
    "    dow_sales = df_train.groupby(['FirmID', 'DayOfWeek'])['Sales'].mean().reset_index()\n",
    "    dow_traffic = df_train.groupby(['FirmID', 'DayOfWeek'])['FirmDailyTraffic'].mean().reset_index()\n",
    "\n",
    "    dow_sales['DayOfWeek'] = dow_sales['DayOfWeek'].astype(int)\n",
    "    dow_traffic['DayOfWeek'] = dow_traffic['DayOfWeek'].astype(int)\n",
    "\n",
    "    dow_merged = dow_sales.merge(dow_traffic, on=['FirmID', 'DayOfWeek'], how='inner')\n",
    "\n",
    "    correlations = []\n",
    "    for day in range(7):\n",
    "        day_data = dow_merged[dow_merged['DayOfWeek'] == day]\n",
    "\n",
    "        day_data = day_data.dropna(subset=['Sales', 'FirmDailyTraffic'])\n",
    "        \n",
    "        if len(day_data) > 5:\n",
    "\n",
    "            if day_data['Sales'].std() > 0 and day_data['FirmDailyTraffic'].std() > 0:\n",
    "                try:\n",
    "                    corr, p_value = stats.pearsonr(day_data['Sales'], day_data['FirmDailyTraffic'])\n",
    "                    correlations.append({\n",
    "                        'DayOfWeek': day,\n",
    "                        'DayName': calendar.day_name[day],\n",
    "                        'Correlation': corr,\n",
    "                        'P_Value': p_value,\n",
    "                        'Significant': p_value < 0.05\n",
    "                    })\n",
    "                    print(f\"Day {day} ({calendar.day_name[day]}): Correlation = {corr:.4f}, p-value = {p_value:.4f}, n = {len(day_data)}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error for day {day}: {e}\")\n",
    "\n",
    "    if correlations:\n",
    "        corr_df = pd.DataFrame(correlations)\n",
    "        \n",
    "        fig_corr = go.Figure()\n",
    "\n",
    "        fig_corr.add_trace(go.Bar(\n",
    "            x=corr_df['DayName'],\n",
    "            y=corr_df['Correlation'],\n",
    "            marker_color=['red' if sig else 'blue' for sig in corr_df['Significant']],\n",
    "            text=[f\"p={p:.3f}\" for p in corr_df['P_Value']],\n",
    "            textposition='outside'\n",
    "        ))\n",
    "\n",
    "        fig_corr.add_shape(\n",
    "            type='line',\n",
    "            x0=-0.5,\n",
    "            x1=len(corr_df)-0.5,\n",
    "            y0=0,\n",
    "            y1=0,\n",
    "            line=dict(color='black', dash='dash')\n",
    "        )\n",
    "\n",
    "        fig_corr.update_layout(\n",
    "            title=\"Sales-Traffic Correlation by Day of Week\",\n",
    "            xaxis_title=\"Day of Week\",\n",
    "            yaxis_title=\"Correlation Coefficient\",\n",
    "            template=\"plotly_white\",\n",
    "            yaxis=dict(range=[-1, 1]) \n",
    "        )\n",
    "\n",
    "        fig_corr.write_html(\"sales_traffic_correlation_by_dow.html\")\n",
    "        print(\"Sales-Traffic correlation by day of week analysis saved\")\n",
    "    else:\n",
    "        print(\"Insufficient data for correlation analysis by day of week\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in correlation analysis by day of week: {str(e)}\")\n",
    "    print(\"Attempting to proceed with analysis...\")\n",
    "\n",
    "try:\n",
    "\n",
    "    firm_correlations = []\n",
    "\n",
    "    for firm_id in df_train['FirmID'].unique():\n",
    "        firm_data = df_train[df_train['FirmID'] == firm_id]\n",
    "\n",
    "        valid_data = firm_data.dropna(subset=['Sales', 'FirmDailyTraffic'])\n",
    "\n",
    "        if len(valid_data) >= 10:\n",
    "\n",
    "            if valid_data['Sales'].std() > 0 and valid_data['FirmDailyTraffic'].std() > 0:\n",
    "                try:\n",
    "                    corr, p_value = stats.pearsonr(valid_data['Sales'], valid_data['FirmDailyTraffic'])\n",
    "                    \n",
    "                    firm_correlations.append({\n",
    "                        'FirmID': firm_id,\n",
    "                        'Correlation': corr,\n",
    "                        'P_Value': p_value,\n",
    "                        'Significant': p_value < 0.05,\n",
    "                        'DataPoints': len(valid_data)\n",
    "                    })\n",
    "                    print(f\"Firm {firm_id}: Correlation = {corr:.4f}, p-value = {p_value:.4f}, n = {len(valid_data)}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating correlation for firm {firm_id}: {e}\")\n",
    "    \n",
    "    if firm_correlations:\n",
    "        firm_corr_df = pd.DataFrame(firm_correlations)\n",
    "\n",
    "        firm_corr_df = firm_corr_df.sort_values('Correlation', ascending=False)\n",
    "\n",
    "        fig_firm_corr = go.Figure()\n",
    "\n",
    "        fig_firm_corr.add_trace(go.Bar(\n",
    "            x=firm_corr_df['FirmID'],\n",
    "            y=firm_corr_df['Correlation'],\n",
    "            marker_color=['green' if sig else 'gray' for sig in firm_corr_df['Significant']],\n",
    "            text=[f\"p={p:.3f}\" for p in firm_corr_df['P_Value']],\n",
    "            textposition='outside'\n",
    "        ))\n",
    "\n",
    "        fig_firm_corr.add_shape(\n",
    "            type='line',\n",
    "            x0=-0.5,\n",
    "            x1=len(firm_corr_df)-0.5,\n",
    "            y0=0,\n",
    "            y1=0,\n",
    "            line=dict(color='black', dash='dash')\n",
    "        )\n",
    "\n",
    "        fig_firm_corr.update_layout(\n",
    "            title=\"Sales-Traffic Correlation by Firm\",\n",
    "            xaxis_title=\"Firm ID\",\n",
    "            yaxis_title=\"Correlation Coefficient\",\n",
    "            template=\"plotly_white\",\n",
    "            yaxis=dict(range=[-1, 1]),  \n",
    "            height=600,\n",
    "            width=max(800, len(firm_corr_df) * 50)  \n",
    "        )\n",
    "\n",
    "        fig_firm_corr.write_html(\"sales_traffic_correlation_by_firm.html\")\n",
    "        print(\"Sales-Traffic correlation by firm analysis saved\")\n",
    "\n",
    "        print(\"\\nSummary of Sales-Traffic correlations by firm:\")\n",
    "        print(f\"Number of firms analyzed: {len(firm_corr_df)}\")\n",
    "        print(f\"Firms with positive correlation: {(firm_corr_df['Correlation'] > 0).sum()} ({(firm_corr_df['Correlation'] > 0).sum() / len(firm_corr_df) * 100:.1f}%)\")\n",
    "        print(f\"Firms with negative correlation: {(firm_corr_df['Correlation'] < 0).sum()} ({(firm_corr_df['Correlation'] < 0).sum() / len(firm_corr_df) * 100:.1f}%)\")\n",
    "        print(f\"Firms with statistically significant correlation (p < 0.05): {firm_corr_df['Significant'].sum()} ({firm_corr_df['Significant'].sum() / len(firm_corr_df) * 100:.1f}%)\")\n",
    "        print(f\"Average correlation: {firm_corr_df['Correlation'].mean():.3f}\")\n",
    "        print(f\"Median correlation: {firm_corr_df['Correlation'].median():.3f}\")\n",
    "\n",
    "        firm_corr_df['CorrelationStrength'] = pd.cut(\n",
    "            firm_corr_df['Correlation'].abs(), \n",
    "            bins=[0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "            labels=['Very Weak (0-0.2)', 'Weak (0.2-0.4)', 'Moderate (0.4-0.6)', 'Strong (0.6-0.8)', 'Very Strong (0.8-1.0)']\n",
    "        )\n",
    "\n",
    "        strength_counts = firm_corr_df['CorrelationStrength'].value_counts().sort_index()\n",
    "\n",
    "        fig_corr_pie = go.Figure(data=[go.Pie(\n",
    "            labels=strength_counts.index,\n",
    "            values=strength_counts.values,\n",
    "            hole=.3,\n",
    "            marker_colors=['#e6f2ff', '#99ccff', '#4da6ff', '#0066cc', '#003d7a']\n",
    "        )])\n",
    "        \n",
    "        fig_corr_pie.update_layout(\n",
    "            title=\"Distribution of Sales-Traffic Correlation Strengths Across Firms\",\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "\n",
    "        fig_corr_pie.write_html(\"sales_traffic_correlation_strength_distribution.html\")\n",
    "        print(\"Sales-Traffic correlation strength distribution visualization saved\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in correlation analysis by firm: {e}\")\n",
    "\n",
    "try:\n",
    "    firm_counts = df_train.groupby('FirmID').size()\n",
    "    top_firms = firm_counts.nlargest(5).index\n",
    "\n",
    "    fig_firm_dow_compare = make_subplots(\n",
    "        rows=2, cols=3,\n",
    "        subplot_titles=[f\"Firm {firm_id}\" for firm_id in top_firms] + [\"Overall Average\"],\n",
    "        specs=[[{\"type\": \"scatter\"}, {\"type\": \"scatter\"}, {\"type\": \"scatter\"}], \n",
    "               [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}, {\"type\": \"scatter\"}]],\n",
    "        vertical_spacing=0.15,\n",
    "        horizontal_spacing=0.08\n",
    "    )\n",
    "\n",
    "    for i, firm_id in enumerate(top_firms):\n",
    "        firm_data = df_train[df_train['FirmID'] == firm_id]\n",
    "\n",
    "        if 'DayOfWeek' not in firm_data.columns:\n",
    "            if 'Date' in firm_data.columns:\n",
    "                firm_data['DayOfWeek'] = firm_data['Date'].dt.dayofweek\n",
    "            else:\n",
    "                continue \n",
    "\n",
    "        valid_days = sorted(firm_data['DayOfWeek'].unique())\n",
    "        if len(valid_days) == 0:\n",
    "            continue  # Skip if no valid days\n",
    "            \n",
    "        dow_means = firm_data.groupby('DayOfWeek')['Sales'].mean()\n",
    "        dow_names = [calendar.day_name[d] for d in range(7) if d in dow_means.index]\n",
    "        dow_values = [dow_means[d] if d in dow_means.index else 0 for d in range(7)]\n",
    "\n",
    "        traffic_means = firm_data.groupby('DayOfWeek')['FirmDailyTraffic'].mean()\n",
    "        traffic_values = [traffic_means[d] if d in traffic_means.index else 0 for d in range(7)]\n",
    "\n",
    "        sales_max = max(dow_values)\n",
    "        if sales_max > 0:\n",
    "            sales_normalized = [v/sales_max for v in dow_values]\n",
    "        else:\n",
    "            sales_normalized = dow_values\n",
    "            \n",
    "        traffic_max = max(traffic_values)\n",
    "        if traffic_max > 0:\n",
    "            traffic_normalized = [v/traffic_max for v in traffic_values]\n",
    "        else:\n",
    "            traffic_normalized = traffic_values\n",
    "\n",
    "        row = i // 3 + 1\n",
    "        col = i % 3 + 1\n",
    "        \n",
    "        fig_firm_dow_compare.add_trace(\n",
    "            go.Scatter(\n",
    "                x=dow_names,\n",
    "                y=sales_normalized,\n",
    "                mode='lines+markers',\n",
    "                name=f'Firm {firm_id} - Sales',\n",
    "                line=dict(color='blue'),\n",
    "                showlegend=i==0\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "\n",
    "        fig_firm_dow_compare.add_trace(\n",
    "            go.Scatter(\n",
    "                x=dow_names,\n",
    "                y=traffic_normalized,\n",
    "                mode='lines+markers',\n",
    "                name=f'Firm {firm_id} - Traffic',\n",
    "                line=dict(color='red'),\n",
    "                showlegend=i==0\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "\n",
    "        if len(sales_normalized) == len(traffic_normalized) and len(sales_normalized) >= 2:\n",
    "\n",
    "            if np.std(sales_normalized) > 0 and np.std(traffic_normalized) > 0:\n",
    "                try:\n",
    "                    corr, pval = stats.pearsonr(sales_normalized, traffic_normalized)\n",
    "\n",
    "                    fig_firm_dow_compare.add_annotation(\n",
    "                        x=0.5, y=0.9,\n",
    "                        text=f\"Pattern Corr: {corr:.2f}\",\n",
    "                        showarrow=False,\n",
    "                        xref=f\"x{i+1}\" if i > 0 else \"x\",\n",
    "                        yref=f\"y{i+1}\" if i > 0 else \"y\"\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating correlation for firm {firm_id}: {str(e)}\")\n",
    "\n",
    "    if 'DayOfWeek' not in df_train.columns:\n",
    "        if 'Date' in df_train.columns:\n",
    "            df_train['DayOfWeek'] = df_train['Date'].dt.dayofweek\n",
    "\n",
    "    if 'DayOfWeek' in df_train.columns:\n",
    "        overall_sales = df_train.groupby('DayOfWeek')['Sales'].mean()\n",
    "        overall_traffic = df_train.groupby('DayOfWeek')['FirmDailyTraffic'].mean()\n",
    "        \n",
    "        dow_names = [calendar.day_name[d] for d in range(7) if d in overall_sales.index]\n",
    "\n",
    "        all_dow_names = [calendar.day_name[d] for d in range(7)]\n",
    "\n",
    "        sales_max = overall_sales.max()\n",
    "        if sales_max > 0:\n",
    "            sales_norm = overall_sales / sales_max\n",
    "        else:\n",
    "            sales_norm = overall_sales\n",
    "            \n",
    "        traffic_max = overall_traffic.max()\n",
    "        if traffic_max > 0:\n",
    "            traffic_norm = overall_traffic / traffic_max\n",
    "        else:\n",
    "            traffic_norm = overall_traffic\n",
    "\n",
    "        sales_values = [sales_norm[d] if d in sales_norm.index else 0 for d in range(7)]\n",
    "        traffic_values = [traffic_norm[d] if d in traffic_norm.index else 0 for d in range(7)]\n",
    "        \n",
    "        fig_firm_dow_compare.add_trace(\n",
    "            go.Scatter(\n",
    "                x=all_dow_names,\n",
    "                y=sales_values,\n",
    "                mode='lines+markers',\n",
    "                name='Overall - Sales',\n",
    "                line=dict(color='blue', width=3),\n",
    "                showlegend=True\n",
    "            ),\n",
    "            row=2, col=3\n",
    "        )\n",
    "        \n",
    "        fig_firm_dow_compare.add_trace(\n",
    "            go.Scatter(\n",
    "                x=all_dow_names,\n",
    "                y=traffic_values,\n",
    "                mode='lines+markers',\n",
    "                name='Overall - Traffic',\n",
    "                line=dict(color='red', width=3),\n",
    "                showlegend=True\n",
    "            ),\n",
    "            row=2, col=3\n",
    "        )\n",
    "\n",
    "        if len(sales_values) == len(traffic_values) and len(sales_values) >= 2:\n",
    "\n",
    "            if np.std(sales_values) > 0 and np.std(traffic_values) > 0:\n",
    "                try:\n",
    "                    corr, pval = stats.pearsonr(sales_values, traffic_values)\n",
    "\n",
    "                    fig_firm_dow_compare.add_annotation(\n",
    "                        x=0.5, y=0.9,\n",
    "                        text=f\"Pattern Corr: {corr:.2f}\",\n",
    "                        showarrow=False,\n",
    "                        xref=\"x6\",\n",
    "                        yref=\"y6\"\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating overall correlation: {str(e)}\")\n",
    "\n",
    "    fig_firm_dow_compare.update_layout(\n",
    "        height=700,\n",
    "        width=1200,\n",
    "        title_text=\"Sales vs. Traffic Day-of-Week Patterns (Normalized) by Firm\",\n",
    "        template=\"plotly_white\",\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=-0.2,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for i in range(1, 7):\n",
    "        fig_firm_dow_compare.update_yaxes(title_text=\"Normalized Value\", range=[0, 1.1], row=(i-1)//3+1, col=(i-1)%3+1)\n",
    "        fig_firm_dow_compare.update_xaxes(title_text=\"Day of Week\", row=(i-1)//3+1, col=(i-1)%3+1)\n",
    "\n",
    "    fig_firm_dow_compare.write_html(\"firm_day_of_week_pattern_comparison.html\")\n",
    "    print(\"Firm-specific day of week pattern comparison saved\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in firm-specific day of week pattern comparison: {str(e)}\")\n",
    "    print(\"Continuing with analysis...\")\n",
    "\n",
    "try:\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=3,\n",
    "        subplot_titles=[calendar.day_name[i] for i in range(7)] + [\"All Days\", \"\"],\n",
    "        specs=[[{\"type\": \"scatter\"}, {\"type\": \"scatter\"}, {\"type\": \"scatter\"}], \n",
    "               [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}, {\"type\": \"scatter\"}], \n",
    "               [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}, {\"type\": \"scatter\"}]],\n",
    "        vertical_spacing=0.1,\n",
    "        horizontal_spacing=0.05\n",
    "    )\n",
    "\n",
    "    for i, day in enumerate(range(7)):\n",
    "        day_data = df_train[df_train['DayOfWeek'] == day]\n",
    "\n",
    "        if len(day_data) > 1000:\n",
    "            day_data = day_data.sample(1000, random_state=42)\n",
    "\n",
    "        if day_data['Sales'].std() > 0 and day_data['FirmDailyTraffic'].std() > 0 and len(day_data) > 1:\n",
    "            corr, _ = stats.pearsonr(day_data['Sales'], day_data['FirmDailyTraffic'])\n",
    "        else:\n",
    "            corr = float('nan')\n",
    "\n",
    "        row = i // 3 + 1\n",
    "        col = i % 3 + 1\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=day_data['FirmDailyTraffic'],\n",
    "                y=day_data['Sales'],\n",
    "                mode='markers',\n",
    "                marker=dict(opacity=0.5, size=5),\n",
    "                name=calendar.day_name[day]\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "\n",
    "        fig.update_xaxes(title_text=\"Traffic\", row=row, col=col)\n",
    "        fig.update_yaxes(title_text=\"Sales\", row=row, col=col)\n",
    "\n",
    "        if not np.isnan(corr):\n",
    "            fig.add_annotation(\n",
    "                x=0.5, y=0.9,\n",
    "                text=f\"r = {corr:.3f}\",\n",
    "                showarrow=False,\n",
    "                xref=f\"x{i+1}\" if i > 0 else \"x\",\n",
    "                yref=f\"y{i+1}\" if i > 0 else \"y\"\n",
    "            )\n",
    "\n",
    "    all_data = df_train.sample(min(1000, len(df_train)), random_state=42)\n",
    "\n",
    "    if all_data['Sales'].std() > 0 and all_data['FirmDailyTraffic'].std() > 0:\n",
    "        corr, _ = stats.pearsonr(all_data['Sales'], all_data['FirmDailyTraffic'])\n",
    "    else:\n",
    "        corr = float('nan')\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=all_data['FirmDailyTraffic'],\n",
    "            y=all_data['Sales'],\n",
    "            mode='markers',\n",
    "            marker=dict(opacity=0.5, size=5, color='red'),\n",
    "            name=\"All Days\"\n",
    "        ),\n",
    "        row=3, col=1\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(title_text=\"Traffic\", row=3, col=1)\n",
    "    fig.update_yaxes(title_text=\"Sales\", row=3, col=1)\n",
    "\n",
    "    if not np.isnan(corr):\n",
    "        fig.add_annotation(\n",
    "            x=0.5, y=0.9,\n",
    "            text=f\"r = {corr:.3f}\",\n",
    "            showarrow=False,\n",
    "            xref=\"x7\",\n",
    "            yref=\"y7\"\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=900,\n",
    "        width=1000,\n",
    "        title_text=\"Sales vs. Traffic by Day of Week\",\n",
    "        template=\"plotly_white\",\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    fig.write_html(\"sales_vs_traffic_by_dow.html\")\n",
    "    print(\"Sales vs. Traffic by day of week visualization saved\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in Sales vs. Traffic by day of week analysis: {e}\")\n",
    "\n",
    "try:\n",
    "    if 'Month' not in df_train.columns:\n",
    "        if 'Date' in df_train.columns:\n",
    "            df_train['Month'] = df_train['Date'].dt.month\n",
    "        else:\n",
    "            raise ValueError(\"Cannot create Month column - Date column not found\")\n",
    "\n",
    "    sales_month = df_train.groupby(['FirmID', 'Month'])['Sales'].mean().reset_index()\n",
    "    traffic_month = df_train.groupby(['FirmID', 'Month'])['FirmDailyTraffic'].mean().reset_index()\n",
    "\n",
    "    sales_month['Month'] = sales_month['Month'].astype(int)\n",
    "    traffic_month['Month'] = traffic_month['Month'].astype(int)\n",
    "    \n",
    "    month_merged = sales_month.merge(traffic_month, on=['FirmID', 'Month'], how='inner')\n",
    "\n",
    "    month_correlations = []\n",
    "    for month in range(1, 13):\n",
    "        month_data = month_merged[month_merged['Month'] == month]\n",
    "        if len(month_data) > 5:\n",
    "            if month_data['Sales'].std() > 0 and month_data['FirmDailyTraffic'].std() > 0:\n",
    "                try:\n",
    "                    corr, p_value = stats.pearsonr(month_data['Sales'], month_data['FirmDailyTraffic'])\n",
    "                    month_correlations.append({\n",
    "                        'Month': month,\n",
    "                        'MonthName': calendar.month_name[month],\n",
    "                        'Correlation': corr,\n",
    "                        'P_Value': p_value,\n",
    "                        'Significant': p_value < 0.05\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error calculating correlation for month {month}: {str(e)}\")\n",
    "\n",
    "    if month_correlations:\n",
    "        month_corr_df = pd.DataFrame(month_correlations)\n",
    "        \n",
    "        fig_month_corr = go.Figure()\n",
    "\n",
    "        fig_month_corr.add_trace(go.Bar(\n",
    "            x=month_corr_df['MonthName'],\n",
    "            y=month_corr_df['Correlation'],\n",
    "            marker_color=['green' if sig else 'orange' for sig in month_corr_df['Significant']],\n",
    "            text=[f\"p={p:.3f}\" for p in month_corr_df['P_Value']],\n",
    "            textposition='outside'\n",
    "        ))\n",
    "\n",
    "        fig_month_corr.add_shape(\n",
    "            type='line',\n",
    "            x0=-0.5,\n",
    "            x1=len(month_corr_df)-0.5,\n",
    "            y0=0,\n",
    "            y1=0,\n",
    "            line=dict(color='black', dash='dash')\n",
    "        )\n",
    "\n",
    "        fig_month_corr.update_layout(\n",
    "            title=\"Sales-Traffic Correlation by Month\",\n",
    "            xaxis_title=\"Month\",\n",
    "            yaxis_title=\"Correlation Coefficient\",\n",
    "            template=\"plotly_white\",\n",
    "            yaxis=dict(range=[-1, 1]) \n",
    "        )\n",
    "\n",
    "        fig_month_corr.write_html(\"sales_traffic_correlation_by_month.html\")\n",
    "        print(\"Sales-Traffic correlation by month analysis saved\")\n",
    "    else:\n",
    "        print(\"Insufficient data for correlation analysis by month\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in correlation analysis by month: {str(e)}\")\n",
    "    print(\"Continuing with analysis...\")\n",
    "\n",
    "try:\n",
    "\n",
    "    if 'DayOfWeek' not in df_train.columns:\n",
    "        if 'Date' in df_train.columns:\n",
    "            df_train['DayOfWeek'] = df_train['Date'].dt.dayofweek\n",
    "        else:\n",
    "            raise ValueError(\"Cannot create DayOfWeek column - Date column not found\")\n",
    "\n",
    "    sales_dow = df_train.groupby('DayOfWeek')['Sales'].mean()\n",
    "\n",
    "    sales_max = sales_dow.max()\n",
    "    if sales_max > 0:\n",
    "        sales_dow_norm = sales_dow / sales_max\n",
    "    else:\n",
    "        sales_dow_norm = sales_dow\n",
    "    \n",
    "    traffic_dow = df_train.groupby('DayOfWeek')['FirmDailyTraffic'].mean()\n",
    "\n",
    "    traffic_max = traffic_dow.max()\n",
    "    if traffic_max > 0:\n",
    "        traffic_dow_norm = traffic_dow / traffic_max\n",
    "    else:\n",
    "        traffic_dow_norm = traffic_dow\n",
    "\n",
    "    compare_dow = pd.DataFrame({\n",
    "        'Sales': sales_dow_norm,\n",
    "        'Traffic': traffic_dow_norm\n",
    "    })\n",
    "    compare_dow['DayName'] = [calendar.day_name[i] if i in compare_dow.index else \"Unknown\" for i in compare_dow.index]\n",
    "\n",
    "    if not compare_dow.empty:\n",
    "\n",
    "        fig = make_subplots(\n",
    "            rows=1, cols=3,\n",
    "            subplot_titles=[\"Sales by Day\", \"Traffic by Day\", \"Comparison\"],\n",
    "            specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}, {\"type\": \"scatter\"}]],\n",
    "            horizontal_spacing=0.1\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=compare_dow['DayName'],\n",
    "                y=compare_dow['Sales'],\n",
    "                name='Sales',\n",
    "                marker_color='blue'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=compare_dow['DayName'],\n",
    "                y=compare_dow['Traffic'],\n",
    "                name='Traffic',\n",
    "                marker_color='red'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=compare_dow['DayName'],\n",
    "                y=compare_dow['Sales'],\n",
    "                mode='lines+markers',\n",
    "                name='Sales',\n",
    "                line=dict(color='blue')\n",
    "            ),\n",
    "            row=1, col=3\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=compare_dow['DayName'],\n",
    "                y=compare_dow['Traffic'],\n",
    "                mode='lines+markers',\n",
    "                name='Traffic',\n",
    "                line=dict(color='red')\n",
    "            ),\n",
    "            row=1, col=3\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            height=500,\n",
    "            width=1200,\n",
    "            title_text=\"Sales and Traffic Patterns Comparison\",\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "\n",
    "        fig.write_html(\"sales_traffic_pattern_comparison.html\")\n",
    "        print(\"Sales and Traffic pattern comparison saved\")\n",
    "    else:\n",
    "        print(\"Insufficient data for Sales and Traffic pattern comparison\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in Sales and Traffic pattern comparison: {str(e)}\")\n",
    "    print(\"Continuing with analysis...\")\n",
    "\n",
    "try:\n",
    "    print(\"\\nPerforming cluster analysis of seasonality patterns...\")\n",
    "\n",
    "    if 'DayOfWeek' not in df_train.columns:\n",
    "        if 'Date' in df_train.columns:\n",
    "            df_train['DayOfWeek'] = df_train['Date'].dt.dayofweek\n",
    "            print(\"Created 'DayOfWeek' column from Date for clustering analysis\")\n",
    "        else:\n",
    "            raise ValueError(\"Cannot create DayOfWeek column - Date column not found\")\n",
    "\n",
    "    firm_counts = df_train.groupby('FirmID').size()\n",
    "    firms_with_data = firm_counts[firm_counts >= 30].index\n",
    "    \n",
    "    if len(firms_with_data) >= 5:\n",
    "        pattern_data = []\n",
    "        \n",
    "        for firm_id in firms_with_data:\n",
    "            firm_data = df_train[df_train['FirmID'] == firm_id]\n",
    "\n",
    "            if 'DayOfWeek' not in firm_data.columns or 'Sales' not in firm_data.columns:\n",
    "                continue\n",
    "\n",
    "            days_present = sorted(firm_data['DayOfWeek'].unique())\n",
    "\n",
    "            if len(days_present) >= 5:\n",
    "                try:\n",
    "                    sales_dow = firm_data.groupby('DayOfWeek')['Sales'].mean()\n",
    "\n",
    "                    full_pattern = np.zeros(7)\n",
    "                    for day, value in sales_dow.items():\n",
    "                        if 0 <= day < 7: \n",
    "                            full_pattern[day] = value\n",
    "\n",
    "                    pattern_max = np.max(full_pattern)\n",
    "                    if pattern_max > 0:\n",
    "                        normalized_pattern = full_pattern / pattern_max\n",
    "                    else:\n",
    "                        normalized_pattern = full_pattern\n",
    "                    \n",
    "                    pattern_data.append({\n",
    "                        'FirmID': firm_id,\n",
    "                        'Pattern': normalized_pattern,\n",
    "                        'DataPoints': len(firm_data)\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing pattern for firm {firm_id}: {str(e)}\")\n",
    "\n",
    "        pattern_df = pd.DataFrame(pattern_data)\n",
    "        \n",
    "        if len(pattern_df) >= 5:\n",
    "            pattern_matrix = np.vstack(pattern_df['Pattern'].values)\n",
    "\n",
    "            from sklearn.cluster import KMeans\n",
    "            from sklearn.metrics import silhouette_score\n",
    "\n",
    "            max_clusters = min(10, len(pattern_df) // 2) \n",
    "            scores = []\n",
    "            \n",
    "            for n_clusters in range(2, max_clusters + 1):\n",
    "                try:\n",
    "                    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "                    cluster_labels = kmeans.fit_predict(pattern_matrix)\n",
    "                    \n",
    "                    if len(set(cluster_labels)) > 1:  # Need at least 2 clusters for silhouette score\n",
    "                        silhouette_avg = silhouette_score(pattern_matrix, cluster_labels)\n",
    "                        scores.append((n_clusters, silhouette_avg))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error computing silhouette score for {n_clusters} clusters: {str(e)}\")\n",
    "            \n",
    "            if scores:\n",
    "                best_n_clusters, _ = max(scores, key=lambda x: x[1])\n",
    "\n",
    "                kmeans = KMeans(n_clusters=best_n_clusters, random_state=42, n_init=10)\n",
    "                cluster_labels = kmeans.fit_predict(pattern_matrix)\n",
    "\n",
    "                pattern_df['Cluster'] = cluster_labels\n",
    "\n",
    "                cluster_centers = kmeans.cluster_centers_\n",
    "\n",
    "                fig_clusters = go.Figure()\n",
    "\n",
    "                for i, center in enumerate(cluster_centers):\n",
    "                    fig_clusters.add_trace(go.Scatter(\n",
    "                        x=[calendar.day_name[d] for d in range(7)],\n",
    "                        y=center,\n",
    "                        mode='lines+markers',\n",
    "                        name=f'Cluster {i+1}',\n",
    "                        line=dict(width=3)\n",
    "                    ))\n",
    "\n",
    "                fig_clusters.update_layout(\n",
    "                    title=f\"Sales Seasonality Pattern Clusters (n={best_n_clusters})\",\n",
    "                    xaxis_title=\"Day of Week\",\n",
    "                    yaxis_title=\"Normalized Sales\",\n",
    "                    template=\"plotly_white\",\n",
    "                    legend_title=\"Cluster\"\n",
    "                )\n",
    "\n",
    "                fig_clusters.write_html(\"sales_seasonality_clusters.html\")\n",
    "                print(f\"Sales seasonality cluster analysis saved with {best_n_clusters} clusters\")\n",
    "\n",
    "                cluster_counts = pattern_df['Cluster'].value_counts().sort_index()\n",
    "                \n",
    "                fig_cluster_counts = go.Figure(data=[go.Pie(\n",
    "                    labels=[f'Cluster {i+1} (n={count})' for i, count in enumerate(cluster_counts)],\n",
    "                    values=cluster_counts.values\n",
    "                )])\n",
    "                \n",
    "                fig_cluster_counts.update_layout(\n",
    "                    title=\"Distribution of Firms Across Seasonality Pattern Clusters\",\n",
    "                    template=\"plotly_white\"\n",
    "                )\n",
    "                \n",
    "                fig_cluster_counts.write_html(\"sales_seasonality_cluster_distribution.html\")\n",
    "                print(\"Sales seasonality cluster distribution visualization saved\")\n",
    "\n",
    "                print(f\"\\nSales seasonality pattern clustering summary:\")\n",
    "                print(f\"Number of firms analyzed: {len(pattern_df)}\")\n",
    "                print(f\"Optimal number of clusters: {best_n_clusters}\")\n",
    "                print(\"Cluster distribution:\")\n",
    "                for i, count in enumerate(cluster_counts):\n",
    "                    print(f\"  Cluster {i+1}: {count} firms ({count/len(pattern_df)*100:.1f}%)\")\n",
    "\n",
    "                try:\n",
    "                    fig_cluster_examples = make_subplots(\n",
    "                        rows=best_n_clusters, cols=1,\n",
    "                        subplot_titles=[f\"Cluster {i+1} Examples\" for i in range(best_n_clusters)],\n",
    "                        vertical_spacing=0.1\n",
    "                    )\n",
    "\n",
    "                    for cluster_idx in range(best_n_clusters):\n",
    "                        cluster_firms = pattern_df[pattern_df['Cluster'] == cluster_idx]['FirmID'].values\n",
    "\n",
    "                        sample_firms = cluster_firms[:min(3, len(cluster_firms))]\n",
    "\n",
    "                        fig_cluster_examples.add_trace(\n",
    "                            go.Scatter(\n",
    "                                x=[calendar.day_name[d] for d in range(7)],\n",
    "                                y=cluster_centers[cluster_idx],\n",
    "                                mode='lines',\n",
    "                                name=f'Cluster {cluster_idx+1} Center',\n",
    "                                line=dict(color='black', width=3, dash='dash')\n",
    "                            ),\n",
    "                            row=cluster_idx+1, col=1\n",
    "                        )\n",
    "\n",
    "                        for i, firm_id in enumerate(sample_firms):\n",
    "                            firm_data = df_train[df_train['FirmID'] == firm_id]\n",
    "\n",
    "                            dow_means = firm_data.groupby('DayOfWeek')['Sales'].mean()\n",
    "                            dow_values = [dow_means[d] if d in dow_means.index else 0 for d in range(7)]\n",
    "\n",
    "                            if max(dow_values) > 0:\n",
    "                                normalized = [v / max(dow_values) for v in dow_values]\n",
    "                            else:\n",
    "                                normalized = dow_values\n",
    "\n",
    "                            fig_cluster_examples.add_trace(\n",
    "                                go.Scatter(\n",
    "                                    x=[calendar.day_name[d] for d in range(7)],\n",
    "                                    y=normalized,\n",
    "                                    mode='lines+markers',\n",
    "                                    name=f'Firm {firm_id}',\n",
    "                                    line=dict(width=2),\n",
    "                                    opacity=0.7\n",
    "                                ),\n",
    "                                row=cluster_idx+1, col=1\n",
    "                            )\n",
    "\n",
    "                    fig_cluster_examples.update_layout(\n",
    "                        height=300 * best_n_clusters,\n",
    "                        width=900,\n",
    "                        title_text=\"Example Firms from Each Seasonality Pattern Cluster\",\n",
    "                        template=\"plotly_white\",\n",
    "                        showlegend=True\n",
    "                    )\n",
    "\n",
    "                    for i in range(1, best_n_clusters+1):\n",
    "                        fig_cluster_examples.update_yaxes(\n",
    "                            title_text=\"Normalized Sales\",\n",
    "                            range=[0, 1.1],\n",
    "                            row=i, col=1\n",
    "                        )\n",
    "                        fig_cluster_examples.update_xaxes(\n",
    "                            title_text=\"Day of Week\" if i == best_n_clusters else \"\",\n",
    "                            row=i, col=1\n",
    "                        )\n",
    "\n",
    "                    fig_cluster_examples.write_html(\"sales_seasonality_cluster_examples.html\")\n",
    "                    print(\"Sales seasonality cluster examples visualization saved\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error creating cluster examples visualization: {e}\")\n",
    "            else:\n",
    "                print(\"Could not determine optimal number of clusters - insufficient variation in patterns\")\n",
    "        else:\n",
    "            print(\"Not enough firms with complete patterns for clustering\")\n",
    "    else:\n",
    "        print(\"Not enough firms with sufficient data for clustering analysis\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in seasonal pattern clustering: {str(e)}\")\n",
    "    print(\"Continuing with analysis...\")\n",
    "\n",
    "try:\n",
    "    print(\"\\nAnalyzing daily differences in Traffic-Sales relationship...\")\n",
    "\n",
    "    if 'DayOfWeek' not in df_train.columns:\n",
    "        if 'Date' in df_train.columns:\n",
    "            df_train['DayOfWeek'] = df_train['Date'].dt.dayofweek\n",
    "            print(\"Created 'DayOfWeek' column from Date for daily differences analysis\")\n",
    "        else:\n",
    "            raise ValueError(\"Cannot create DayOfWeek column - Date column not found\")\n",
    "\n",
    "    dow_models = {}\n",
    "    dow_r2_values = []\n",
    "    \n",
    "    for day in range(7):\n",
    "        day_data = df_train[df_train['DayOfWeek'] == day]\n",
    "\n",
    "        valid_data = day_data.dropna(subset=['Sales', 'FirmDailyTraffic'])\n",
    "        \n",
    "        if len(valid_data) >= 30:\n",
    "            if valid_data['Sales'].std() > 0 and valid_data['FirmDailyTraffic'].std() > 0:\n",
    "                try:\n",
    "                    slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "                        valid_data['FirmDailyTraffic'], \n",
    "                        valid_data['Sales']\n",
    "                    )\n",
    "\n",
    "                    dow_models[day] = {\n",
    "                        'slope': slope,\n",
    "                        'intercept': intercept,\n",
    "                        'r_value': r_value,\n",
    "                        'p_value': p_value,\n",
    "                        'std_err': std_err,\n",
    "                        'r_squared': r_value**2,\n",
    "                        'day_name': calendar.day_name[day]\n",
    "                    }\n",
    "                    \n",
    "                    dow_r2_values.append({\n",
    "                        'day': day,\n",
    "                        'day_name': calendar.day_name[day],\n",
    "                        'r_squared': r_value**2,\n",
    "                        'slope': slope,\n",
    "                        'significant': p_value < 0.05\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error modeling relationship for day {day}: {str(e)}\")\n",
    "    \n",
    "    if dow_r2_values:\n",
    "        dow_r2_df = pd.DataFrame(dow_r2_values)\n",
    "\n",
    "        fig_r2 = go.Figure()\n",
    "\n",
    "        fig_r2.add_trace(go.Bar(\n",
    "            x=dow_r2_df['day_name'],\n",
    "            y=dow_r2_df['r_squared'],\n",
    "            marker_color=['green' if sig else 'lightblue' for sig in dow_r2_df['significant']],\n",
    "            text=[f\"Slope: {slope:.4f}\" for slope in dow_r2_df['slope']],\n",
    "            textposition='outside'\n",
    "        ))\n",
    "\n",
    "        fig_r2.update_layout(\n",
    "            title=\"Traffic-Sales Relationship Strength by Day of Week\",\n",
    "            xaxis_title=\"Day of Week\",\n",
    "            yaxis_title=\"R Value\",\n",
    "            template=\"plotly_white\",\n",
    "            yaxis=dict(range=[0, max(dow_r2_df['r_squared']) * 1.2]) \n",
    "        )\n",
    "\n",
    "        fig_r2.write_html(\"traffic_sales_relationship_by_dow.html\")\n",
    "        print(\"Traffic-Sales relationship analysis by day of week saved\")\n",
    "\n",
    "        fig_slope = go.Figure()\n",
    "\n",
    "        fig_slope.add_trace(go.Bar(\n",
    "            x=dow_r2_df['day_name'],\n",
    "            y=dow_r2_df['slope'],\n",
    "            marker_color=['green' if sig else 'lightblue' for sig in dow_r2_df['significant']],\n",
    "            text=[f\"R: {r2:.4f}\" for r2 in dow_r2_df['r_squared']],\n",
    "            textposition='outside'\n",
    "        ))\n",
    "\n",
    "        fig_slope.update_layout(\n",
    "            title=\"Traffic-to-Sales Conversion Rate by Day of Week\",\n",
    "            xaxis_title=\"Day of Week\",\n",
    "            yaxis_title=\"Slope (Sales per Traffic Unit)\",\n",
    "            template=\"plotly_white\"\n",
    "        )\n",
    "\n",
    "        fig_slope.write_html(\"traffic_sales_conversion_by_dow.html\")\n",
    "        print(\"Traffic-to-Sales conversion rate analysis by day of week saved\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Not enough data to model Traffic-Sales relationship by day of week\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in daily differences analysis: {str(e)}\")\n",
    "    print(\"Continuing with analysis...\")\n",
    "\n",
    "print(\"\\nEnhanced seasonality analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70cb199b78bfd54",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Benchmark Model: Univariate OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e3ca11a6c029b71",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing time-based train-validation split...\n",
      "Training subset: 28120 rows (2020-01-01 00:00:00 to 2021-07-16 00:00:00)\n",
      "Validation set: 7030 rows (2021-07-16 00:00:00 to 2021-12-03 00:00:00)\n",
      "\n",
      "Training benchmark global OLS model...\n",
      "       FirmID       Date        Sales  Sales_pred_global\n",
      "28120    16.0 2021-07-16  3384.403700        5412.022243\n",
      "28121     2.0 2021-07-16  1109.660432        1265.946645\n",
      "28122     0.0 2021-07-16  1023.047051        2092.900826\n",
      "28123    31.0 2021-07-16   169.120030         228.358348\n",
      "28124    23.0 2021-07-16  1478.048269        1868.809268\n",
      "28125    39.0 2021-07-16   760.310225        1975.129103\n",
      "28126    34.0 2021-07-16   170.549860         290.827770\n",
      "28127    24.0 2021-07-16   968.838594        1672.318213\n",
      "28128     9.0 2021-07-16  1270.735680        1143.006672\n",
      "28129    37.0 2021-07-16   434.464832         833.195094\n",
      "Global OLS model summary (showing select coefficients):\n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "Intercept          2105.7984     24.864     84.691      0.000    2057.063    2154.534\n",
      "C(FirmID)[T.1.0]   -212.6637     35.147     -6.051      0.000    -281.553    -143.774\n",
      "C(FirmID)[T.2.0]   -834.3826     35.163    -23.729      0.000    -903.303    -765.462\n",
      "C(FirmID)[T.3.0]   -362.2687     35.147    -10.307      0.000    -431.159    -293.378\n",
      "C(FirmID)[T.4.0]   -501.0887     35.163    -14.251      0.000    -570.009    -432.168\n",
      "C(FirmID)[T.5.0]  -1037.7270     35.164    -29.511      0.000   -1106.649    -968.805\n",
      "C(FirmID)[T.6.0]  -1057.4960     35.147    -30.088      0.000   -1126.386    -988.606\n",
      "C(FirmID)[T.7.0]  -1111.8764     35.147    -31.635      0.000   -1180.766   -1042.986\n",
      "C(FirmID)[T.8.0]  -1186.1976     35.147    -33.749      0.000   -1255.088   -1117.307\n",
      "C(FirmID)[T.9.0]   -962.7918     35.163    -27.381      0.000   -1031.712    -893.871\n",
      "C(FirmID)[T.10.0]  1367.9301     35.147     38.920      0.000    1299.040    1436.820\n",
      "C(FirmID)[T.11.0] -1399.4591     35.147    -39.817      0.000   -1468.349   -1330.569\n",
      "C(FirmID)[T.12.0]  -911.2746     35.163    -25.916      0.000    -980.195    -842.354\n",
      "C(FirmID)[T.13.0]  -950.9299     35.147    -27.056      0.000   -1019.820    -882.040\n",
      "C(FirmID)[T.14.0] -1331.3869     35.147    -37.880      0.000   -1400.277   -1262.497\n",
      "C(FirmID)[T.15.0] -1106.6492     35.163    -31.472      0.000   -1175.570   -1037.729\n",
      "C(FirmID)[T.16.0]  3346.9425     35.163     95.185      0.000    3278.022    3415.863\n",
      "C(FirmID)[T.17.0]  -981.6590     35.163    -27.918      0.000   -1050.579    -912.739\n",
      "C(FirmID)[T.18.0]  1670.3407     35.163     47.503      0.000    1601.420    1739.261\n",
      "C(FirmID)[T.19.0] -1865.3870     35.163    -53.050      0.000   -1934.308   -1796.466\n",
      "C(FirmID)[T.20.0] -1565.7367     35.164    -44.527      0.000   -1634.659   -1496.814\n",
      "C(FirmID)[T.21.0]   281.6693     35.163      8.010      0.000     212.749     350.590\n",
      "C(FirmID)[T.22.0]   -61.5734     35.163     -1.751      0.080    -130.494       7.347\n",
      "C(FirmID)[T.23.0]  -213.6340     35.163     -6.076      0.000    -282.555    -144.713\n",
      "C(FirmID)[T.24.0]  -573.0030     35.163    -16.296      0.000    -641.923    -504.083\n",
      "C(FirmID)[T.25.0]  -153.6146     35.163     -4.369      0.000    -222.535     -84.694\n",
      "C(FirmID)[T.26.0]  -259.3203     35.147     -7.378      0.000    -328.210    -190.431\n",
      "C(FirmID)[T.27.0]  -141.2646     35.147     -4.019      0.000    -210.154     -72.375\n",
      "C(FirmID)[T.28.0]  -408.1288     35.147    -11.612      0.000    -477.019    -339.239\n",
      "C(FirmID)[T.29.0]  -908.5485     35.147    -25.850      0.000    -977.438    -839.658\n",
      "C(FirmID)[T.30.0] -1621.9452     35.147    -46.147      0.000   -1690.835   -1553.055\n",
      "C(FirmID)[T.31.0] -1843.6846     35.163    -52.433      0.000   -1912.605   -1774.764\n",
      "C(FirmID)[T.32.0] -1517.4551     35.163    -43.155      0.000   -1586.376   -1448.535\n",
      "C(FirmID)[T.33.0] -1840.3560     35.163    -52.338      0.000   -1909.277   -1771.435\n",
      "C(FirmID)[T.34.0] -1901.3847     35.164    -54.072      0.000   -1970.308   -1832.462\n",
      "C(FirmID)[T.35.0] -1309.4735     35.147    -37.257      0.000   -1378.363   -1240.584\n",
      "C(FirmID)[T.36.0]  -343.8415     35.163     -9.779      0.000    -412.762    -274.921\n",
      "C(FirmID)[T.37.0] -1231.9379     35.163    -35.035      0.000   -1300.858   -1163.017\n",
      "C(FirmID)[T.38.0]   561.7347     35.147     15.982      0.000     492.845     630.625\n",
      "C(FirmID)[T.39.0]  -256.3625     35.163     -7.291      0.000    -325.283    -187.442\n",
      "C(FirmID)[T.40.0] -1499.6904     35.163    -42.649      0.000   -1568.612   -1430.769\n",
      "C(FirmID)[T.41.0]  -552.1650     35.163    -15.703      0.000    -621.085    -483.245\n",
      "C(FirmID)[T.42.0]  -690.7800     35.163    -19.645      0.000    -759.701    -621.859\n",
      "C(FirmID)[T.43.0]  -878.5527     35.147    -24.996      0.000    -947.443    -809.662\n",
      "C(FirmID)[T.44.0]  -597.1691     35.147    -16.990      0.000    -666.059    -528.279\n",
      "C(FirmID)[T.45.0]   169.7706     35.147      4.830      0.000     100.881     238.660\n",
      "C(FirmID)[T.46.0]  -597.9526     35.163    -17.005      0.000    -666.873    -529.032\n",
      "C(FirmID)[T.47.0]   849.7917     35.147     24.178      0.000     780.902     918.681\n",
      "C(FirmID)[T.48.0]  -731.4281     35.163    -20.801      0.000    -800.349    -662.507\n",
      "C(FirmID)[T.49.0]   375.8904     35.163     10.690      0.000     306.970     444.811\n",
      "TrafficPctChange    432.0704     21.335     20.252      0.000     390.253     473.887\n",
      "=====================================================================================\n",
      "\n",
      "Training firm-specific OLS models...\n",
      "Firm 0.0 model: R = -0.5833, Traffic coef = 19.6185, Traffic-Sales corr = 0.0150\n",
      "Firm 30.0 model: R = -4.4121, Traffic coef = 4.3588, Traffic-Sales corr = 0.2384\n",
      "Firm 4.0 model: R = 0.3157, Traffic coef = 21.6402, Traffic-Sales corr = 0.1945\n",
      "Firm 31.0 model: R = -0.3249, Traffic coef = 3.2681, Traffic-Sales corr = 0.2683\n",
      "Firm 32.0 model: R = 0.4926, Traffic coef = 9.2214, Traffic-Sales corr = 0.0794\n",
      "Firm 33.0 model: R = -1.0565, Traffic coef = 5.0112, Traffic-Sales corr = 0.2676\n",
      "Firm 34.0 model: R = 0.2665, Traffic coef = 5.1916, Traffic-Sales corr = 0.2869\n",
      "Firm 35.0 model: R = -0.5437, Traffic coef = 10.2719, Traffic-Sales corr = 0.3202\n",
      "Firm 3.0 model: R = -1.2098, Traffic coef = 40.9517, Traffic-Sales corr = 0.2935\n",
      "Firm 36.0 model: R = -3.3083, Traffic coef = 18.0209, Traffic-Sales corr = 0.1366\n",
      "Firm 29.0 model: R = -2.2074, Traffic coef = 17.2829, Traffic-Sales corr = 0.1840\n",
      "Firm 37.0 model: R = -0.8973, Traffic coef = 11.4816, Traffic-Sales corr = 0.1606\n",
      "Firm 2.0 model: R = -0.5415, Traffic coef = 15.5541, Traffic-Sales corr = 0.0807\n",
      "Firm 47.0 model: R = -0.2991, Traffic coef = 33.2135, Traffic-Sales corr = 0.1396\n",
      "Firm 39.0 model: R = -1.5086, Traffic coef = 21.5159, Traffic-Sales corr = 0.1648\n",
      "Firm 40.0 model: R = 0.0257, Traffic coef = 9.7803, Traffic-Sales corr = 0.2117\n",
      "Firm 41.0 model: R = -0.4220, Traffic coef = 10.9100, Traffic-Sales corr = 0.2940\n",
      "Firm 42.0 model: R = 0.3080, Traffic coef = 18.8986, Traffic-Sales corr = 0.0784\n",
      "Firm 1.0 model: R = 0.0146, Traffic coef = 19.3622, Traffic-Sales corr = 0.1663\n",
      "Firm 43.0 model: R = -1.9851, Traffic coef = 18.5860, Traffic-Sales corr = 0.1353\n",
      "Firm 44.0 model: R = -0.7437, Traffic coef = 26.7021, Traffic-Sales corr = 0.2153\n",
      "Firm 38.0 model: R = -0.0170, Traffic coef = 33.5586, Traffic-Sales corr = 0.3192\n",
      "Firm 27.0 model: R = -3.8259, Traffic coef = 10.6380, Traffic-Sales corr = 0.0990\n",
      "Firm 5.0 model: R = -3.0310, Traffic coef = 21.1762, Traffic-Sales corr = 0.2761\n",
      "Firm 26.0 model: R = 0.2513, Traffic coef = 25.7319, Traffic-Sales corr = -0.0036\n",
      "Firm 11.0 model: R = 0.1122, Traffic coef = 11.4811, Traffic-Sales corr = 0.0161\n",
      "Firm 9.0 model: R = -0.8388, Traffic coef = 12.1906, Traffic-Sales corr = 0.2637\n",
      "Firm 12.0 model: R = -2.7252, Traffic coef = 9.0747, Traffic-Sales corr = 0.1479\n",
      "Firm 49.0 model: R = -0.5255, Traffic coef = 17.0628, Traffic-Sales corr = -0.0319\n",
      "Firm 13.0 model: R = -0.0356, Traffic coef = 16.2036, Traffic-Sales corr = 0.2192\n",
      "Firm 14.0 model: R = -0.5782, Traffic coef = 10.6696, Traffic-Sales corr = 0.1527\n",
      "Firm 8.0 model: R = -1.2433, Traffic coef = 11.7020, Traffic-Sales corr = 0.1820\n",
      "Firm 15.0 model: R = 0.1928, Traffic coef = 10.8447, Traffic-Sales corr = 0.1274\n",
      "Firm 16.0 model: R = -2.7354, Traffic coef = 26.2295, Traffic-Sales corr = 0.1241\n",
      "Firm 17.0 model: R = -0.9366, Traffic coef = 11.3298, Traffic-Sales corr = 0.1110\n",
      "Firm 18.0 model: R = -0.5178, Traffic coef = 26.7913, Traffic-Sales corr = 0.0241\n",
      "Firm 7.0 model: R = -2.8072, Traffic coef = 14.5947, Traffic-Sales corr = 0.1554\n",
      "Firm 19.0 model: R = -0.4286, Traffic coef = 3.1454, Traffic-Sales corr = 0.1711\n",
      "Firm 20.0 model: R = -1.9092, Traffic coef = 13.5357, Traffic-Sales corr = 0.3493\n",
      "Firm 21.0 model: R = 0.1043, Traffic coef = 17.6060, Traffic-Sales corr = 0.0663\n",
      "Firm 22.0 model: R = -3.8995, Traffic coef = 24.4582, Traffic-Sales corr = 0.3049\n",
      "Firm 23.0 model: R = -1.8525, Traffic coef = 24.7042, Traffic-Sales corr = 0.2499\n",
      "Firm 6.0 model: R = -1.2183, Traffic coef = 13.5535, Traffic-Sales corr = 0.3498\n",
      "Firm 24.0 model: R = -1.0647, Traffic coef = 15.8243, Traffic-Sales corr = 0.0982\n",
      "Firm 25.0 model: R = -0.2150, Traffic coef = 25.7008, Traffic-Sales corr = 0.1160\n",
      "Firm 48.0 model: R = -0.8075, Traffic coef = 20.4022, Traffic-Sales corr = 0.2349\n",
      "Firm 45.0 model: R = -0.0584, Traffic coef = 31.7459, Traffic-Sales corr = 0.1512\n",
      "Firm 46.0 model: R = -10.5723, Traffic coef = 15.9800, Traffic-Sales corr = 0.2851\n",
      "Firm 28.0 model: R = 0.1757, Traffic coef = 20.5641, Traffic-Sales corr = 0.1683\n",
      "Firm 10.0 model: R = -0.1425, Traffic coef = 28.2225, Traffic-Sales corr = 0.1681\n",
      "\n",
      "Firm-specific model performance (top 10 by R):\n",
      "    FirmID        R2        RMSE         MAE  DataPoints  \\\n",
      "4     32.0  0.492602  173.984986  140.947417         141   \n",
      "2      4.0  0.315733  382.875026  318.660756         141   \n",
      "17    42.0  0.308038  381.256087  303.775880         141   \n",
      "6     34.0  0.266523   71.557979   58.452539         141   \n",
      "24    26.0  0.251276  581.154675  499.095255         140   \n",
      "32    15.0  0.192761  252.083561  207.262970         141   \n",
      "48    28.0  0.175747  429.453148  341.078765         140   \n",
      "25    11.0  0.112248  254.320543  207.185321         140   \n",
      "39    21.0  0.104308  636.842527  540.106246         141   \n",
      "15    40.0  0.025729  201.990610  172.627039         141   \n",
      "\n",
      "    Traffic_Sales_Correlation  \n",
      "4                    0.079407  \n",
      "2                    0.194466  \n",
      "17                   0.078427  \n",
      "6                    0.286927  \n",
      "24                  -0.003640  \n",
      "32                   0.127357  \n",
      "48                   0.168284  \n",
      "25                   0.016118  \n",
      "39                   0.066277  \n",
      "15                   0.211673  \n",
      "\n",
      "Firms with significant traffic relationship: 50 out of 50 (100.0%)\n",
      "\n",
      "Traffic-Sales correlation analysis:\n",
      "Firms with positive correlation: 48 (96.0%)\n",
      "Firms with negative correlation: 2 (4.0%)\n",
      "Firms with significant positive correlation: 48 (96.0%)\n",
      "Firms with significant negative correlation: 2 (4.0%)\n",
      "Average correlation: 0.1765\n",
      "Median correlation: 0.1672\n",
      "\n",
      "Comparison of models:\n",
      "Individual firm-specific models - Weighted Avg R: -1.1958, RMSE: 498.87, MAE: 415.88\n",
      "Combined model - R: 0.5729, RMSE: 567.92, MAE: 415.88\n",
      "Global model - R: 0.4522, RMSE: 643.14, MAE: 502.09\n",
      "\n",
      "Improvement of combined model over global model:\n",
      "  R improvement: 26.7%\n",
      "  RMSE reduction: 11.7%\n",
      "  MAE reduction: 17.2%\n",
      "\n",
      "Firm-specific models visualization saved\n",
      "Traffic-Sales correlation distribution visualization saved\n",
      "\n",
      "Combined model performance (firm-specific where available, global otherwise):\n",
      "R: 0.4522, RMSE: 643.14, MAE: 502.09\n",
      "Global vs. firm-specific models comparison visualization saved\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPerforming time-based train-validation split...\")\n",
    "\n",
    "df_train = df_train.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "split_idx = int(len(df_train) * 0.8)\n",
    "df_train_subset = df_train.iloc[:split_idx].copy()\n",
    "df_validate = df_train.iloc[split_idx:].copy()\n",
    "\n",
    "print(f\"Training subset: {len(df_train_subset)} rows ({df_train_subset['Date'].min()} to {df_train_subset['Date'].max()})\")\n",
    "print(f\"Validation set: {len(df_validate)} rows ({df_validate['Date'].min()} to {df_validate['Date'].max()})\")\n",
    "\n",
    "print(\"\\nTraining benchmark global OLS model...\")\n",
    "\n",
    "ols_model = smf.ols(\n",
    "    \"Sales ~ C(FirmID) + TrafficPctChange\", \n",
    "    data=df_train_subset\n",
    ").fit()\n",
    "\n",
    "df_validate['Sales_pred_global'] = ols_model.predict(df_validate)\n",
    "print(df_validate[['FirmID', 'Date', 'Sales', 'Sales_pred_global']].head(10))\n",
    "print(f\"Global OLS model summary (showing select coefficients):\\n{ols_model.summary().tables[1]}\")\n",
    "\n",
    "\n",
    "print(\"\\nTraining firm-specific OLS models...\")\n",
    "firm_models = {}\n",
    "firm_metrics = []\n",
    "\n",
    "for firm_id in df_train_subset['FirmID'].unique():\n",
    "    firm_data = df_train_subset[df_train_subset['FirmID'] == firm_id].copy()\n",
    "\n",
    "    if len(firm_data) < 30: \n",
    "        print(f\"Skipping Firm {firm_id}: insufficient data ({len(firm_data)} rows)\")\n",
    "        continue\n",
    "\n",
    "    if firm_data['Sales'].std() == 0 or firm_data['FirmDailyTraffic'].std() == 0:\n",
    "        print(f\"Skipping Firm {firm_id}: no variation in data (constant values)\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        formula = \"Sales ~ TrafficPctChange + FirmDailyTraffic\"\n",
    "\n",
    "        if 'DayOfWeek' in firm_data.columns:\n",
    "            formula += \" + C(DayOfWeek)\"\n",
    "        if 'Month' in firm_data.columns:\n",
    "            formula += \" + C(Month)\"\n",
    "        if 'IsWeekend' in firm_data.columns:\n",
    "            formula += \" + IsWeekend\"\n",
    "\n",
    "        firm_model = smf.ols(formula, data=firm_data).fit()\n",
    "\n",
    "        firm_models[firm_id] = firm_model\n",
    "\n",
    "        firm_validate = df_validate[df_validate['FirmID'] == firm_id].copy()\n",
    "        if len(firm_validate) > 0:\n",
    "            try:\n",
    "                firm_validate['Sales_pred_firm'] = firm_model.predict(firm_validate)\n",
    "\n",
    "                valid_indices = firm_validate.index[firm_validate['Sales'].notna() & firm_validate['Sales_pred_firm'].notna()]\n",
    "                \n",
    "                if len(valid_indices) > 0:\n",
    "                    valid_actual = firm_validate.loc[valid_indices, 'Sales']\n",
    "                    valid_pred = firm_validate.loc[valid_indices, 'Sales_pred_firm']\n",
    "                    \n",
    "                    rmse = np.sqrt(mean_squared_error(valid_actual, valid_pred))\n",
    "                    mae = mean_absolute_error(valid_actual, valid_pred)\n",
    "                    r2 = r2_score(valid_actual, valid_pred)\n",
    "\n",
    "                    traffic_sales_corr = firm_data['FirmDailyTraffic'].corr(firm_data['Sales'])\n",
    "\n",
    "                    firm_metrics.append({\n",
    "                        'FirmID': firm_id,\n",
    "                        'RMSE': rmse,\n",
    "                        'MAE': mae,\n",
    "                        'R2': r2,\n",
    "                        'DataPoints': len(valid_indices),\n",
    "                        'Coefficient_Traffic': firm_model.params.get('FirmDailyTraffic', np.nan),\n",
    "                        'P_Value_Traffic': firm_model.pvalues.get('FirmDailyTraffic', np.nan),\n",
    "                        'Coefficient_TrafficPct': firm_model.params.get('TrafficPctChange', np.nan),\n",
    "                        'P_Value_TrafficPct': firm_model.pvalues.get('TrafficPctChange', np.nan),\n",
    "                        'Traffic_Sales_Correlation': traffic_sales_corr\n",
    "                    })\n",
    "\n",
    "                    column_name = f'Sales_pred_firm_{int(firm_id)}'\n",
    "                    if column_name not in df_validate.columns:\n",
    "                        df_validate[column_name] = np.nan\n",
    "\n",
    "                    for idx in valid_indices:\n",
    "                        df_validate.at[idx, column_name] = firm_validate.at[idx, 'Sales_pred_firm']\n",
    "                    \n",
    "                    print(f\"Firm {firm_id} model: R = {r2:.4f}, Traffic coef = {firm_model.params.get('FirmDailyTraffic', np.nan):.4f}, Traffic-Sales corr = {traffic_sales_corr:.4f}\")\n",
    "                else:\n",
    "                    print(f\"No valid predictions for Firm {firm_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error type: {type(e)}\")\n",
    "                print(f\"Error predicting for Firm {firm_id}: {str(e)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error modeling Firm {firm_id}: {e}\")\n",
    "\n",
    "if firm_metrics:\n",
    "    firm_metrics_df = pd.DataFrame(firm_metrics)\n",
    "\n",
    "    firm_metrics_df = firm_metrics_df.sort_values('R2', ascending=False)\n",
    "    \n",
    "    print(\"\\nFirm-specific model performance (top 10 by R):\")\n",
    "    print(firm_metrics_df[['FirmID', 'R2', 'RMSE', 'MAE', 'DataPoints', 'Traffic_Sales_Correlation']].head(10))\n",
    "\n",
    "    firm_metrics_df['Traffic_Significant'] = firm_metrics_df['P_Value_Traffic'] < 0.05\n",
    "    \n",
    "    print(f\"\\nFirms with significant traffic relationship: {firm_metrics_df['Traffic_Significant'].sum()} out of {len(firm_metrics_df)} ({firm_metrics_df['Traffic_Significant'].mean()*100:.1f}%)\")\n",
    "\n",
    "    positive_corr = (firm_metrics_df['Traffic_Sales_Correlation'] > 0).sum()\n",
    "    negative_corr = (firm_metrics_df['Traffic_Sales_Correlation'] < 0).sum()\n",
    "    significant_positive = ((firm_metrics_df['Traffic_Sales_Correlation'] > 0) & firm_metrics_df['Traffic_Significant']).sum()\n",
    "    significant_negative = ((firm_metrics_df['Traffic_Sales_Correlation'] < 0) & firm_metrics_df['Traffic_Significant']).sum()\n",
    "    \n",
    "    print(\"\\nTraffic-Sales correlation analysis:\")\n",
    "    print(f\"Firms with positive correlation: {positive_corr} ({positive_corr/len(firm_metrics_df)*100:.1f}%)\")\n",
    "    print(f\"Firms with negative correlation: {negative_corr} ({negative_corr/len(firm_metrics_df)*100:.1f}%)\")\n",
    "    print(f\"Firms with significant positive correlation: {significant_positive} ({significant_positive/len(firm_metrics_df)*100:.1f}%)\")\n",
    "    print(f\"Firms with significant negative correlation: {significant_negative} ({significant_negative/len(firm_metrics_df)*100:.1f}%)\")\n",
    "    print(f\"Average correlation: {firm_metrics_df['Traffic_Sales_Correlation'].mean():.4f}\")\n",
    "    print(f\"Median correlation: {firm_metrics_df['Traffic_Sales_Correlation'].median():.4f}\")\n",
    "\n",
    "    df_validate['Sales_pred_combined'] = df_validate['Sales_pred_global']  \n",
    "\n",
    "    for firm_id in firm_models.keys():\n",
    "        column_name = f'Sales_pred_firm_{int(firm_id)}'\n",
    "        if column_name in df_validate.columns:\n",
    "            mask = df_validate[column_name].notna()\n",
    "            if mask.any():\n",
    "                df_validate.loc[mask, 'Sales_pred_combined'] = df_validate.loc[mask, column_name]\n",
    "\n",
    "    valid_mask_global = df_validate['Sales'].notna() & df_validate['Sales_pred_global'].notna()\n",
    "    valid_mask_combined = df_validate['Sales'].notna() & df_validate['Sales_pred_combined'].notna()\n",
    "    \n",
    "    if valid_mask_global.sum() > 0 and valid_mask_combined.sum() > 0:\n",
    "        valid_actual_global = df_validate.loc[valid_mask_global, 'Sales']\n",
    "        valid_pred_global = df_validate.loc[valid_mask_global, 'Sales_pred_global']\n",
    "        \n",
    "        valid_actual_combined = df_validate.loc[valid_mask_combined, 'Sales']\n",
    "        valid_pred_combined = df_validate.loc[valid_mask_combined, 'Sales_pred_combined']\n",
    "        \n",
    "        global_rmse = np.sqrt(mean_squared_error(valid_actual_global, valid_pred_global))\n",
    "        global_mae = mean_absolute_error(valid_actual_global, valid_pred_global)\n",
    "        global_r2 = r2_score(valid_actual_global, valid_pred_global)\n",
    "        \n",
    "        combined_rmse = np.sqrt(mean_squared_error(valid_actual_combined, valid_pred_combined))\n",
    "        combined_mae = mean_absolute_error(valid_actual_combined, valid_pred_combined)\n",
    "        combined_r2 = r2_score(valid_actual_combined, valid_pred_combined)\n",
    "        \n",
    "        print(\"\\nComparison of models:\")\n",
    "\n",
    "        avg_r2 = np.average(firm_metrics_df['R2'], weights=firm_metrics_df['DataPoints'])\n",
    "        avg_rmse = np.average(firm_metrics_df['RMSE'], weights=firm_metrics_df['DataPoints'])\n",
    "        avg_mae = np.average(firm_metrics_df['MAE'], weights=firm_metrics_df['DataPoints'])\n",
    "        \n",
    "        print(f\"Individual firm-specific models - Weighted Avg R: {avg_r2:.4f}, RMSE: {avg_rmse:.2f}, MAE: {avg_mae:.2f}\")\n",
    "        print(f\"Combined model - R: {combined_r2:.4f}, RMSE: {combined_rmse:.2f}, MAE: {combined_mae:.2f}\")\n",
    "        print(f\"Global model - R: {global_r2:.4f}, RMSE: {global_rmse:.2f}, MAE: {global_mae:.2f}\")\n",
    "\n",
    "        if global_r2 != 0:\n",
    "            r2_improvement = (combined_r2 - global_r2) / abs(global_r2) * 100\n",
    "        else:\n",
    "            r2_improvement = float('inf') if combined_r2 > 0 else float('-inf') if combined_r2 < 0 else 0\n",
    "            \n",
    "        rmse_improvement = (global_rmse - combined_rmse) / global_rmse * 100\n",
    "        mae_improvement = (global_mae - combined_mae) / global_mae * 100\n",
    "        \n",
    "        print(f\"\\nImprovement of combined model over global model:\")\n",
    "        print(f\"  R improvement: {r2_improvement:.1f}%\")\n",
    "        print(f\"  RMSE reduction: {rmse_improvement:.1f}%\")\n",
    "        print(f\"  MAE reduction: {mae_improvement:.1f}%\")\n",
    "    else:\n",
    "        print(\"Not enough valid predictions to compare models\")\n",
    "\n",
    "try:\n",
    "    if 'firm_metrics_df' in locals() and not firm_metrics_df.empty:\n",
    "        plot_df = firm_metrics_df.sort_values('Coefficient_Traffic', ascending=False)\n",
    "\n",
    "        fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=plot_df['FirmID'],\n",
    "                y=plot_df['Coefficient_Traffic'],\n",
    "                name=\"Traffic Coefficient\",\n",
    "                marker_color=['green' if sig else 'lightgreen' for sig in plot_df['Traffic_Significant']],\n",
    "                opacity=0.7\n",
    "            ),\n",
    "            secondary_y=False,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=plot_df['FirmID'],\n",
    "                y=plot_df['R2'],\n",
    "                mode='markers',\n",
    "                name=\"R Value\",\n",
    "                marker=dict(\n",
    "                    size=8,\n",
    "                    color='red',\n",
    "                    symbol='circle'\n",
    "                )\n",
    "            ),\n",
    "            secondary_y=True,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=plot_df['FirmID'],\n",
    "                y=plot_df['Traffic_Sales_Correlation'],\n",
    "                mode='markers',\n",
    "                name=\"Traffic-Sales Correlation\",\n",
    "                marker=dict(\n",
    "                    size=8,\n",
    "                    color='blue',\n",
    "                    symbol='diamond'\n",
    "                )\n",
    "            ),\n",
    "            secondary_y=True,\n",
    "        )\n",
    "\n",
    "        fig.add_shape(\n",
    "            type=\"line\",\n",
    "            x0=plot_df['FirmID'].iloc[0],\n",
    "            y0=0,\n",
    "            x1=plot_df['FirmID'].iloc[-1],\n",
    "            y1=0,\n",
    "            line=dict(color=\"black\", width=1, dash=\"dash\"),\n",
    "            xref=\"x\",\n",
    "            yref=\"y\"\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text=\"Firm-Specific Models: Traffic Coefficient and Model Performance\",\n",
    "            template=\"plotly_white\",\n",
    "            height=600,\n",
    "            width=max(1000, len(plot_df)*20),\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=1.02,\n",
    "                xanchor=\"right\",\n",
    "                x=1\n",
    "            ),\n",
    "            xaxis=dict(\n",
    "                title=\"Firm ID\",\n",
    "                tickangle=45\n",
    "            )\n",
    "        )\n",
    "\n",
    "        fig.update_yaxes(title_text=\"Traffic Coefficient\", secondary_y=False)\n",
    "        fig.update_yaxes(title_text=\"R / Correlation\", secondary_y=True)\n",
    "\n",
    "        fig.write_html(\"firm_specific_models_performance.html\")\n",
    "        print(\"\\nFirm-specific models visualization saved\")\n",
    "\n",
    "        corr_fig = go.Figure()\n",
    "\n",
    "        corr_fig.add_trace(\n",
    "            go.Histogram(\n",
    "                x=firm_metrics_df['Traffic_Sales_Correlation'],\n",
    "                nbinsx=20,\n",
    "                marker_color='blue',\n",
    "                opacity=0.7,\n",
    "                name=\"All Correlations\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        significant_mask = firm_metrics_df['Traffic_Significant']\n",
    "        if significant_mask.sum() > 0:\n",
    "            corr_fig.add_trace(\n",
    "                go.Histogram(\n",
    "                    x=firm_metrics_df.loc[significant_mask, 'Traffic_Sales_Correlation'],\n",
    "                    nbinsx=20,\n",
    "                    marker_color='green',\n",
    "                    opacity=0.7,\n",
    "                    name=\"Significant Correlations\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        corr_fig.add_shape(\n",
    "            type=\"line\",\n",
    "            x0=0, y0=0,\n",
    "            x1=0, y1=1,\n",
    "            yref=\"paper\",\n",
    "            line=dict(color=\"red\", width=2, dash=\"dash\")\n",
    "        )\n",
    "\n",
    "        corr_fig.update_layout(\n",
    "            title_text=\"Distribution of Traffic-Sales Correlations Across Firms\",\n",
    "            template=\"plotly_white\",\n",
    "            height=500,\n",
    "            width=800,\n",
    "            xaxis_title=\"Correlation Coefficient\",\n",
    "            yaxis_title=\"Number of Firms\",\n",
    "            bargap=0.1,\n",
    "            barmode='overlay'\n",
    "        )\n",
    "\n",
    "        corr_fig.write_html(\"traffic_sales_correlation_distribution.html\")\n",
    "        print(\"Traffic-Sales correlation distribution visualization saved\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error creating firm-specific model visualizations: {e}\")\n",
    "\n",
    "try:\n",
    "    df_validate['Sales_pred_combined'] = np.nan\n",
    "    \n",
    "    for firm_id in firm_models.keys():\n",
    "        firm_mask = df_validate['FirmID'] == firm_id\n",
    "        pred_col = f'Sales_pred_firm_{firm_id}'\n",
    "        \n",
    "        if pred_col in df_validate.columns:\n",
    "            df_validate.loc[firm_mask, 'Sales_pred_combined'] = df_validate.loc[firm_mask, pred_col]\n",
    "\n",
    "    mask = df_validate['Sales_pred_combined'].isna()\n",
    "    df_validate.loc[mask, 'Sales_pred_combined'] = df_validate.loc[mask, 'Sales_pred_global']\n",
    "\n",
    "    valid_mask_combined = df_validate['Sales'].notna() & df_validate['Sales_pred_combined'].notna()\n",
    "    \n",
    "    if valid_mask_combined.sum() > 0:\n",
    "        valid_actual_combined = df_validate.loc[valid_mask_combined, 'Sales']\n",
    "        valid_pred_combined = df_validate.loc[valid_mask_combined, 'Sales_pred_combined']\n",
    "        \n",
    "        combined_rmse = np.sqrt(mean_squared_error(valid_actual_combined, valid_pred_combined))\n",
    "        combined_mae = mean_absolute_error(valid_actual_combined, valid_pred_combined)\n",
    "        combined_r2 = r2_score(valid_actual_combined, valid_pred_combined)\n",
    "        \n",
    "        print(\"\\nCombined model performance (firm-specific where available, global otherwise):\")\n",
    "        print(f\"R: {combined_r2:.4f}, RMSE: {combined_rmse:.2f}, MAE: {combined_mae:.2f}\")\n",
    "\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=[\n",
    "                \"Global vs. Firm-Specific Predictions\", \n",
    "                \"Prediction Errors\", \n",
    "                \"Predictions Over Time (Sample Firms)\",\n",
    "                \"R by Firm\"\n",
    "            ],\n",
    "            specs=[\n",
    "                [{\"type\": \"scatter\"}, {\"type\": \"histogram\"}],\n",
    "                [{\"type\": \"scatter\"}, {\"type\": \"bar\"}]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=valid_actual_combined,\n",
    "                y=valid_pred_combined,\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=8,\n",
    "                    color='blue',\n",
    "                    opacity=0.6\n",
    "                ),\n",
    "                name=\"Firm-Specific Models\"\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "        valid_global = df_validate['Sales'].notna() & df_validate['Sales_pred_global'].notna()\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df_validate.loc[valid_global, 'Sales'],\n",
    "                y=df_validate.loc[valid_global, 'Sales_pred_global'],\n",
    "                mode='markers',\n",
    "                marker=dict(\n",
    "                    size=8,\n",
    "                    color='red',\n",
    "                    opacity=0.4\n",
    "                ),\n",
    "                name=\"Global Model\"\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "        max_val = max(max(valid_actual_combined), max(valid_pred_combined))\n",
    "        min_val = min(min(valid_actual_combined), min(valid_pred_combined))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[min_val, max_val],\n",
    "                y=[min_val, max_val],\n",
    "                mode='lines',\n",
    "                line=dict(color='black', dash='dash'),\n",
    "                name=\"Perfect Prediction\"\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "        df_validate['error_global'] = df_validate['Sales'] - df_validate['Sales_pred_global']\n",
    "        df_validate['error_combined'] = df_validate['Sales'] - df_validate['Sales_pred_combined']\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Histogram(\n",
    "                x=df_validate['error_global'],\n",
    "                nbinsx=30,\n",
    "                marker_color='red',\n",
    "                opacity=0.5,\n",
    "                name=\"Global Model Errors\"\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Histogram(\n",
    "                x=df_validate['error_combined'],\n",
    "                nbinsx=30,\n",
    "                marker_color='blue',\n",
    "                opacity=0.5,\n",
    "                name=\"Firm-Specific Model Errors\"\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "\n",
    "        if 'firm_metrics_df' in locals() and not firm_metrics_df.empty:\n",
    "            top_firms = firm_metrics_df.sort_values('R2', ascending=False).head(5)['FirmID'].tolist()\n",
    "            \n",
    "            for i, firm_id in enumerate(top_firms):\n",
    "                firm_data = df_validate[df_validate['FirmID'] == firm_id].sort_values('Date')\n",
    "                \n",
    "                if len(firm_data) > 0:\n",
    "                    pred_col = f'Sales_pred_firm_{firm_id}'\n",
    "                    if pred_col in firm_data.columns:\n",
    "                        fig.add_trace(\n",
    "                            go.Scatter(\n",
    "                                x=firm_data['Date'],\n",
    "                                y=firm_data['Sales'],\n",
    "                                mode='lines',\n",
    "                                name=f\"Firm {firm_id} - Actual\",\n",
    "                                line=dict(width=2)\n",
    "                            ),\n",
    "                            row=2, col=1\n",
    "                        )\n",
    "\n",
    "                        fig.add_trace(\n",
    "                            go.Scatter(\n",
    "                                x=firm_data['Date'],\n",
    "                                y=firm_data[pred_col],\n",
    "                                mode='lines',\n",
    "                                name=f\"Firm {firm_id} - Predicted\",\n",
    "                                line=dict(width=2, dash='dash')\n",
    "                            ),\n",
    "                            row=2, col=1\n",
    "                        )\n",
    "\n",
    "        if 'firm_metrics_df' in locals() and not firm_metrics_df.empty:\n",
    "            plot_firms = firm_metrics_df.sort_values('R2', ascending=False).head(20)\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=plot_firms['FirmID'],\n",
    "                    y=plot_firms['R2'],\n",
    "                    marker_color='blue',\n",
    "                    name=\"R by Firm\"\n",
    "                ),\n",
    "                row=2, col=2\n",
    "            )\n",
    "\n",
    "            fig.add_shape(\n",
    "                type=\"line\",\n",
    "                x0=plot_firms['FirmID'].iloc[0],\n",
    "                y0=global_r2,\n",
    "                x1=plot_firms['FirmID'].iloc[-1],\n",
    "                y1=global_r2,\n",
    "                line=dict(color=\"red\", width=2, dash=\"dash\"),\n",
    "                row=2, col=2\n",
    "            )\n",
    "\n",
    "            fig.add_annotation(\n",
    "                text=f\"Global Model R = {global_r2:.4f}\",\n",
    "                x=plot_firms['FirmID'].iloc[-1],\n",
    "                y=global_r2,\n",
    "                showarrow=False,\n",
    "                font=dict(color=\"red\"),\n",
    "                xref=\"x3\",\n",
    "                yref=\"y3\"\n",
    "            )\n",
    "\n",
    "        fig.update_layout(\n",
    "            title_text=\"Global vs. Firm-Specific Models Performance\",\n",
    "            template=\"plotly_white\",\n",
    "            showlegend=True,\n",
    "            height=900,\n",
    "            width=1200,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=-0.2,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            )\n",
    "        )\n",
    "\n",
    "        fig.update_xaxes(title_text=\"Actual Sales\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Predicted Sales\", row=1, col=1)\n",
    "        fig.update_xaxes(title_text=\"Prediction Error\", row=1, col=2)\n",
    "        fig.update_yaxes(title_text=\"Frequency\", row=1, col=2)\n",
    "        fig.update_xaxes(title_text=\"Date\", row=2, col=1)\n",
    "        fig.update_yaxes(title_text=\"Sales\", row=2, col=1)\n",
    "        fig.update_xaxes(title_text=\"Firm ID\", row=2, col=2, tickangle=45)\n",
    "        fig.update_yaxes(title_text=\"R\", row=2, col=2)\n",
    "\n",
    "        fig.add_annotation(\n",
    "            xref=\"paper\", yref=\"paper\",\n",
    "            x=0.25, y=0.05, \n",
    "            text=f\"Global - R: {global_r2:.4f}, RMSE: {global_rmse:.2f}, MAE: {global_mae:.2f}<br>Firm-Specific - R: {combined_r2:.4f}, RMSE: {combined_rmse:.2f}, MAE: {combined_mae:.2f}\",\n",
    "            showarrow=False,\n",
    "            font=dict(size=12),\n",
    "            bgcolor=\"rgba(255, 255, 255, 0.8)\",\n",
    "            bordercolor=\"black\",\n",
    "            borderwidth=1,\n",
    "            borderpad=4\n",
    "        )\n",
    "\n",
    "        fig.write_html(\"global_vs_firm_specific_models.html\")\n",
    "        print(\"Global vs. firm-specific models comparison visualization saved\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error creating model comparison visualizations: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18decc0-7985-4d87-826c-8f3caeec629d",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41e3676a-d64a-4f3f-9299-2e234e5a0e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FOCUSED MULTI-FIRM HYPERPARAMETER TUNING\n",
      "==================================================\n",
      "Using existing training data from feature engineering step\n",
      "Creating validation split while preserving time series structure...\n",
      "Training set: 28100 rows, Validation set: 7050 rows\n",
      "Number of firms in training: 50\n",
      "Number of firms in validation: 50\n",
      "\n",
      "Beginning focused multi-firm model tuning process...\n",
      "Current memory usage: 307.74 MB\n",
      "Creating stratified firm sample for LightGBM tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 11:48:00,590] A new study created in memory with name: no-name-5acf8f84-e9ab-4111-aa3d-297064476526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified sample for LightGBM tuning: 10000 rows\n",
      "Number of firms in sample: 50 (out of 50 total)\n",
      "\n",
      "Tuning LightGBM hyperparameters using Optuna...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287903ad3ca448c7b9493ac87c4915c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-03-02 11:48:01,615] Trial 0 finished with value: 791.4113391899431 and parameters: {'lambda_l1': 0.13292918943162169, 'lambda_l2': 7.114476009343421, 'num_leaves': 79, 'feature_fraction': 0.779597545259111, 'bagging_fraction': 0.6468055921327309, 'bagging_freq': 1, 'min_child_samples': 12, 'learning_rate': 0.08795585311974417, 'max_depth': 6, 'n_estimators': 383}. Best is trial 0 with value: 791.4113391899431.\n",
      "[I 2025-03-02 11:48:02,304] Trial 1 finished with value: 785.9678842926689 and parameters: {'lambda_l1': 0.011527987128232402, 'lambda_l2': 8.123245085588687, 'num_leaves': 87, 'feature_fraction': 0.6637017332034828, 'bagging_fraction': 0.6545474901621302, 'bagging_freq': 1, 'min_child_samples': 22, 'learning_rate': 0.05722807884690141, 'max_depth': 5, 'n_estimators': 216}. Best is trial 1 with value: 785.9678842926689.\n",
      "[I 2025-03-02 11:48:02,987] Trial 2 finished with value: 791.5256689270853 and parameters: {'lambda_l1': 0.6847920095574779, 'lambda_l2': 0.02621087878265441, 'num_leaves': 43, 'feature_fraction': 0.7099085529881075, 'bagging_fraction': 0.7368209952651108, 'bagging_freq': 4, 'min_child_samples': 18, 'learning_rate': 0.05628109945722505, 'max_depth': 6, 'n_estimators': 118}. Best is trial 1 with value: 785.9678842926689.\n",
      "[I 2025-03-02 11:48:04,142] Trial 3 finished with value: 828.6241249378229 and parameters: {'lambda_l1': 0.6647135865318028, 'lambda_l2': 0.03247673570627449, 'num_leaves': 25, 'feature_fraction': 0.884665661176, 'bagging_fraction': 0.8896896099223679, 'bagging_freq': 5, 'min_child_samples': 22, 'learning_rate': 0.018790490260574548, 'max_depth': 7, 'n_estimators': 276}. Best is trial 1 with value: 785.9678842926689.\n",
      "[I 2025-03-02 11:48:04,840] Trial 4 finished with value: 792.8000726992823 and parameters: {'lambda_l1': 0.023233503515390115, 'lambda_l2': 0.3058656666978526, 'num_leaves': 22, 'feature_fraction': 0.8727961206236347, 'bagging_fraction': 0.677633994480005, 'bagging_freq': 4, 'min_child_samples': 22, 'learning_rate': 0.05680612190600298, 'max_depth': 6, 'n_estimators': 174}. Best is trial 1 with value: 785.9678842926689.\n",
      "[I 2025-03-02 11:48:05,370] Trial 5 finished with value: 795.0043920460462 and parameters: {'lambda_l1': 8.10501612641158, 'lambda_l2': 2.1154290797261215, 'num_leaves': 96, 'feature_fraction': 0.8684482051282947, 'bagging_fraction': 0.7793699936433256, 'bagging_freq': 5, 'min_child_samples': 13, 'learning_rate': 0.027638457617723072, 'max_depth': 3, 'n_estimators': 230}. Best is trial 1 with value: 785.9678842926689.\n",
      "[I 2025-03-02 11:48:05,795] Trial 6 finished with value: 740.5323079728871 and parameters: {'lambda_l1': 0.14656553886225335, 'lambda_l2': 0.06516990611177176, 'num_leaves': 87, 'feature_fraction': 0.7070259980080768, 'bagging_fraction': 0.6842803529062143, 'bagging_freq': 3, 'min_child_samples': 15, 'learning_rate': 0.08219772826786358, 'max_depth': 3, 'n_estimators': 495}. Best is trial 6 with value: 740.5323079728871.\n",
      "[I 2025-03-02 11:48:06,471] Trial 7 finished with value: 846.1479141069897 and parameters: {'lambda_l1': 2.0736445177905036, 'lambda_l2': 0.039459088111000004, 'num_leaves': 20, 'feature_fraction': 0.8446384285364503, 'bagging_fraction': 0.8120572031542852, 'bagging_freq': 4, 'min_child_samples': 41, 'learning_rate': 0.016664018656068133, 'max_depth': 5, 'n_estimators': 146}. Best is trial 6 with value: 740.5323079728871.\n",
      "[I 2025-03-02 11:48:07,531] Trial 8 finished with value: 787.6199751057045 and parameters: {'lambda_l1': 3.8842777547031417, 'lambda_l2': 0.7411299781083243, 'num_leaves': 46, 'feature_fraction': 0.6190675050858071, 'bagging_fraction': 0.6932946965146987, 'bagging_freq': 2, 'min_child_samples': 39, 'learning_rate': 0.06738017242196918, 'max_depth': 8, 'n_estimators': 289}. Best is trial 6 with value: 740.5323079728871.\n",
      "[I 2025-03-02 11:48:08,201] Trial 9 finished with value: 773.4162012419735 and parameters: {'lambda_l1': 0.022844556850020535, 'lambda_l2': 1.3795402040204179, 'num_leaves': 81, 'feature_fraction': 0.7683831592708489, 'bagging_fraction': 0.8312901539863683, 'bagging_freq': 3, 'min_child_samples': 31, 'learning_rate': 0.04847869165226947, 'max_depth': 3, 'n_estimators': 143}. Best is trial 6 with value: 740.5323079728871.\n",
      "[I 2025-03-02 11:48:08,773] Trial 10 finished with value: 765.8970049874285 and parameters: {'lambda_l1': 0.14130910630957608, 'lambda_l2': 0.12336245991553763, 'num_leaves': 67, 'feature_fraction': 0.7040552674219865, 'bagging_fraction': 0.6066392625569426, 'bagging_freq': 2, 'min_child_samples': 49, 'learning_rate': 0.09756662996201976, 'max_depth': 4, 'n_estimators': 497}. Best is trial 6 with value: 740.5323079728871.\n",
      "[I 2025-03-02 11:48:09,205] Trial 11 finished with value: 768.2405387492562 and parameters: {'lambda_l1': 0.11803885045838007, 'lambda_l2': 0.1170236581521654, 'num_leaves': 66, 'feature_fraction': 0.7111460167977234, 'bagging_fraction': 0.6053937642080603, 'bagging_freq': 2, 'min_child_samples': 50, 'learning_rate': 0.09990373323110084, 'max_depth': 4, 'n_estimators': 489}. Best is trial 6 with value: 740.5323079728871.\n",
      "[I 2025-03-02 11:48:09,734] Trial 12 finished with value: 775.9072515744573 and parameters: {'lambda_l1': 0.1225971166631815, 'lambda_l2': 0.13806324702643175, 'num_leaves': 65, 'feature_fraction': 0.7080731362217992, 'bagging_fraction': 0.6047616174682944, 'bagging_freq': 2, 'min_child_samples': 31, 'learning_rate': 0.08505414716037386, 'max_depth': 4, 'n_estimators': 497}. Best is trial 6 with value: 740.5323079728871.\n",
      "[I 2025-03-02 11:48:10,276] Trial 13 finished with value: 759.4867124685587 and parameters: {'lambda_l1': 0.32285757624548944, 'lambda_l2': 0.010875731897814758, 'num_leaves': 74, 'feature_fraction': 0.6535081269520909, 'bagging_fraction': 0.7202867909046132, 'bagging_freq': 3, 'min_child_samples': 50, 'learning_rate': 0.07852233074870997, 'max_depth': 4, 'n_estimators': 413}. Best is trial 6 with value: 740.5323079728871.\n",
      "[I 2025-03-02 11:48:10,802] Trial 14 finished with value: 739.517801856273 and parameters: {'lambda_l1': 0.4602247488957906, 'lambda_l2': 0.01157052742471586, 'num_leaves': 95, 'feature_fraction': 0.6328840644450545, 'bagging_fraction': 0.7318537080542177, 'bagging_freq': 3, 'min_child_samples': 38, 'learning_rate': 0.07482259279097855, 'max_depth': 3, 'n_estimators': 405}. Best is trial 14 with value: 739.517801856273.\n",
      "[I 2025-03-02 11:48:11,329] Trial 15 finished with value: 756.7192067598829 and parameters: {'lambda_l1': 0.7673489374204938, 'lambda_l2': 0.013349713013981301, 'num_leaves': 100, 'feature_fraction': 0.6163575199379949, 'bagging_fraction': 0.7658293083691561, 'bagging_freq': 3, 'min_child_samples': 41, 'learning_rate': 0.07344345755533134, 'max_depth': 3, 'n_estimators': 401}. Best is trial 14 with value: 739.517801856273.\n",
      "[I 2025-03-02 11:48:11,781] Trial 16 finished with value: 750.0804667777531 and parameters: {'lambda_l1': 0.05111210744513728, 'lambda_l2': 0.0631937209884012, 'num_leaves': 90, 'feature_fraction': 0.6625583041527509, 'bagging_fraction': 0.7308618513111106, 'bagging_freq': 3, 'min_child_samples': 35, 'learning_rate': 0.06707275377034784, 'max_depth': 3, 'n_estimators': 351}. Best is trial 14 with value: 739.517801856273.\n",
      "[I 2025-03-02 11:48:12,429] Trial 17 finished with value: 804.0130569265002 and parameters: {'lambda_l1': 1.7374995311787065, 'lambda_l2': 0.024222306685753946, 'num_leaves': 53, 'feature_fraction': 0.8003377348752279, 'bagging_fraction': 0.7003126062483137, 'bagging_freq': 4, 'min_child_samples': 26, 'learning_rate': 0.04141935705963702, 'max_depth': 5, 'n_estimators': 449}. Best is trial 14 with value: 739.517801856273.\n",
      "[I 2025-03-02 11:48:12,815] Trial 18 finished with value: 768.3206012557808 and parameters: {'lambda_l1': 0.3046026103435051, 'lambda_l2': 0.2786302437595202, 'num_leaves': 92, 'feature_fraction': 0.7391515925906486, 'bagging_fraction': 0.8028329926341196, 'bagging_freq': 2, 'min_child_samples': 35, 'learning_rate': 0.0868642295230877, 'max_depth': 3, 'n_estimators': 351}. Best is trial 14 with value: 739.517801856273.\n",
      "[I 2025-03-02 11:48:13,206] Trial 19 finished with value: 753.578114238395 and parameters: {'lambda_l1': 0.33143302288549775, 'lambda_l2': 0.06295156541948319, 'num_leaves': 84, 'feature_fraction': 0.600893120176249, 'bagging_fraction': 0.6484011608836823, 'bagging_freq': 3, 'min_child_samples': 45, 'learning_rate': 0.06817318538465607, 'max_depth': 4, 'n_estimators': 448}. Best is trial 14 with value: 739.517801856273.\n",
      "[I 2025-03-02 11:48:13,836] Trial 20 finished with value: 762.2099937564835 and parameters: {'lambda_l1': 0.06982268582973876, 'lambda_l2': 0.01600637835001834, 'num_leaves': 75, 'feature_fraction': 0.6484508902615143, 'bagging_fraction': 0.8622096119390471, 'bagging_freq': 4, 'min_child_samples': 16, 'learning_rate': 0.07850784888873835, 'max_depth': 8, 'n_estimators': 442}. Best is trial 14 with value: 739.517801856273.\n",
      "[I 2025-03-02 11:48:14,222] Trial 21 finished with value: 745.2262670008967 and parameters: {'lambda_l1': 0.03168909220824998, 'lambda_l2': 0.05487573843312577, 'num_leaves': 92, 'feature_fraction': 0.6748301430484173, 'bagging_fraction': 0.7396273498276867, 'bagging_freq': 3, 'min_child_samples': 36, 'learning_rate': 0.06198323198848261, 'max_depth': 3, 'n_estimators': 336}. Best is trial 14 with value: 739.517801856273.\n",
      "[I 2025-03-02 11:48:14,652] Trial 22 finished with value: 758.2033626228405 and parameters: {'lambda_l1': 0.04959363165917246, 'lambda_l2': 0.06156653029474261, 'num_leaves': 100, 'feature_fraction': 0.6859853170760355, 'bagging_fraction': 0.75807164374138, 'bagging_freq': 3, 'min_child_samples': 36, 'learning_rate': 0.04301447867794914, 'max_depth': 3, 'n_estimators': 364}. Best is trial 14 with value: 739.517801856273.\n",
      "[I 2025-03-02 11:48:15,092] Trial 23 finished with value: 763.0858140401493 and parameters: {'lambda_l1': 0.01958020138143412, 'lambda_l2': 0.18200643892250776, 'num_leaves': 92, 'feature_fraction': 0.7368363266476146, 'bagging_fraction': 0.7116118573912997, 'bagging_freq': 3, 'min_child_samples': 26, 'learning_rate': 0.06344392867193148, 'max_depth': 4, 'n_estimators': 326}. Best is trial 14 with value: 739.517801856273.\n",
      "[I 2025-03-02 11:48:15,457] Trial 24 finished with value: 729.318587251191 and parameters: {'lambda_l1': 0.21053800286241015, 'lambda_l2': 0.048077431498039065, 'num_leaves': 74, 'feature_fraction': 0.6323926559209622, 'bagging_fraction': 0.6707848213231955, 'bagging_freq': 3, 'min_child_samples': 28, 'learning_rate': 0.07683863062248529, 'max_depth': 3, 'n_estimators': 316}. Best is trial 24 with value: 729.318587251191.\n",
      "[I 2025-03-02 11:48:15,789] Trial 25 finished with value: 743.8370741508886 and parameters: {'lambda_l1': 0.2564268520715144, 'lambda_l2': 0.018947233350203933, 'num_leaves': 73, 'feature_fraction': 0.6367541606189938, 'bagging_fraction': 0.6706255455664099, 'bagging_freq': 4, 'min_child_samples': 28, 'learning_rate': 0.09103409164877482, 'max_depth': 4, 'n_estimators': 426}. Best is trial 24 with value: 729.318587251191.\n",
      "[I 2025-03-02 11:48:16,119] Trial 26 finished with value: 724.1639565808207 and parameters: {'lambda_l1': 1.1831840825199607, 'lambda_l2': 0.6737215057623974, 'num_leaves': 59, 'feature_fraction': 0.6350702860071836, 'bagging_fraction': 0.627216130866271, 'bagging_freq': 2, 'min_child_samples': 18, 'learning_rate': 0.07819371721475231, 'max_depth': 3, 'n_estimators': 260}. Best is trial 26 with value: 724.1639565808207.\n",
      "[I 2025-03-02 11:48:16,520] Trial 27 finished with value: 755.6892576704688 and parameters: {'lambda_l1': 1.3687929438378819, 'lambda_l2': 0.5781001028132936, 'num_leaves': 55, 'feature_fraction': 0.6039601282512199, 'bagging_fraction': 0.6287750000042439, 'bagging_freq': 1, 'min_child_samples': 10, 'learning_rate': 0.07414451875211947, 'max_depth': 5, 'n_estimators': 263}. Best is trial 26 with value: 724.1639565808207.\n",
      "[I 2025-03-02 11:48:16,971] Trial 28 finished with value: 774.4643723213355 and parameters: {'lambda_l1': 1.0565181942071888, 'lambda_l2': 3.0158689907730394, 'num_leaves': 32, 'feature_fraction': 0.6335269274816778, 'bagging_fraction': 0.6347281711116831, 'bagging_freq': 2, 'min_child_samples': 32, 'learning_rate': 0.09352614648328503, 'max_depth': 7, 'n_estimators': 311}. Best is trial 26 with value: 724.1639565808207.\n",
      "[I 2025-03-02 11:48:17,336] Trial 29 finished with value: 758.6807796506722 and parameters: {'lambda_l1': 3.464600433671384, 'lambda_l2': 0.8556038729211388, 'num_leaves': 59, 'feature_fraction': 0.8045625415034034, 'bagging_fraction': 0.662880959314237, 'bagging_freq': 1, 'min_child_samples': 19, 'learning_rate': 0.07418654591000172, 'max_depth': 3, 'n_estimators': 244}. Best is trial 26 with value: 724.1639565808207.\n",
      "[I 2025-03-02 11:48:17,739] Trial 30 finished with value: 769.2752892880843 and parameters: {'lambda_l1': 0.5050187422498977, 'lambda_l2': 0.4482898853192922, 'num_leaves': 47, 'feature_fraction': 0.63220602790502, 'bagging_fraction': 0.6367788170581131, 'bagging_freq': 2, 'min_child_samples': 27, 'learning_rate': 0.04887100316349187, 'max_depth': 4, 'n_estimators': 206}. Best is trial 26 with value: 724.1639565808207.\n",
      "[I 2025-03-02 11:48:18,074] Trial 31 finished with value: 740.2220598133435 and parameters: {'lambda_l1': 0.2190187391533826, 'lambda_l2': 0.09952023753925088, 'num_leaves': 78, 'feature_fraction': 0.6884356957028, 'bagging_fraction': 0.6910184186995422, 'bagging_freq': 3, 'min_child_samples': 14, 'learning_rate': 0.08271208429064542, 'max_depth': 3, 'n_estimators': 465}. Best is trial 26 with value: 724.1639565808207.\n",
      "[I 2025-03-02 11:48:18,478] Trial 32 finished with value: 759.9858247987617 and parameters: {'lambda_l1': 0.24285782964393593, 'lambda_l2': 4.612867582805325, 'num_leaves': 78, 'feature_fraction': 0.6814023402177551, 'bagging_fraction': 0.7097641683117224, 'bagging_freq': 1, 'min_child_samples': 11, 'learning_rate': 0.08213115016911787, 'max_depth': 3, 'n_estimators': 379}. Best is trial 26 with value: 724.1639565808207.\n",
      "[I 2025-03-02 11:48:18,798] Trial 33 finished with value: 755.7857395313367 and parameters: {'lambda_l1': 0.4708474285803342, 'lambda_l2': 0.21864679368150816, 'num_leaves': 69, 'feature_fraction': 0.6432223050679868, 'bagging_fraction': 0.6572463279453631, 'bagging_freq': 3, 'min_child_samples': 21, 'learning_rate': 0.09098572899292173, 'max_depth': 3, 'n_estimators': 393}. Best is trial 26 with value: 724.1639565808207.\n",
      "[I 2025-03-02 11:48:19,171] Trial 34 finished with value: 759.1237229993992 and parameters: {'lambda_l1': 0.1908654078054056, 'lambda_l2': 0.035548810404493154, 'num_leaves': 62, 'feature_fraction': 0.6695280378381226, 'bagging_fraction': 0.6229038787255778, 'bagging_freq': 2, 'min_child_samples': 14, 'learning_rate': 0.07649030161256774, 'max_depth': 4, 'n_estimators': 306}. Best is trial 26 with value: 724.1639565808207.\n",
      "[I 2025-03-02 11:48:19,606] Trial 35 finished with value: 757.5166294913803 and parameters: {'lambda_l1': 0.08643281592125487, 'lambda_l2': 0.11796005137614246, 'num_leaves': 82, 'feature_fraction': 0.6281545483265762, 'bagging_fraction': 0.6899138588964825, 'bagging_freq': 4, 'min_child_samples': 18, 'learning_rate': 0.07087070274757215, 'max_depth': 5, 'n_estimators': 199}. Best is trial 26 with value: 724.1639565808207.\n",
      "[I 2025-03-02 11:48:19,943] Trial 36 finished with value: 745.7476628356451 and parameters: {'lambda_l1': 0.7463509694311818, 'lambda_l2': 1.420359244518571, 'num_leaves': 58, 'feature_fraction': 0.6878222519196154, 'bagging_fraction': 0.6743445302642933, 'bagging_freq': 2, 'min_child_samples': 24, 'learning_rate': 0.06119359298813315, 'max_depth': 3, 'n_estimators': 258}. Best is trial 26 with value: 724.1639565808207.\n",
      "[I 2025-03-02 11:48:20,367] Trial 37 finished with value: 767.1407814301813 and parameters: {'lambda_l1': 0.4131118789172893, 'lambda_l2': 0.09224726823323995, 'num_leaves': 38, 'feature_fraction': 0.6571039815532957, 'bagging_fraction': 0.7284136352009485, 'bagging_freq': 5, 'min_child_samples': 16, 'learning_rate': 0.08242298024893902, 'max_depth': 7, 'n_estimators': 466}. Best is trial 26 with value: 724.1639565808207.\n",
      "[I 2025-03-02 11:48:20,864] Trial 38 finished with value: 762.7936223845609 and parameters: {'lambda_l1': 0.9248964253724521, 'lambda_l2': 0.36830418345984056, 'num_leaves': 70, 'feature_fraction': 0.614144314867561, 'bagging_fraction': 0.7464735805122054, 'bagging_freq': 4, 'min_child_samples': 20, 'learning_rate': 0.027791718005272102, 'max_depth': 3, 'n_estimators': 380}. Best is trial 26 with value: 724.1639565808207.\n",
      "[I 2025-03-02 11:48:21,209] Trial 39 finished with value: 775.9160094967663 and parameters: {'lambda_l1': 2.6881328993209506, 'lambda_l2': 0.04078540121139985, 'num_leaves': 86, 'feature_fraction': 0.7321089963919458, 'bagging_fraction': 0.6508048467303689, 'bagging_freq': 1, 'min_child_samples': 24, 'learning_rate': 0.08708224800104619, 'max_depth': 4, 'n_estimators': 473}. Best is trial 26 with value: 724.1639565808207.\n",
      "[I 2025-03-02 11:48:21,705] Trial 40 finished with value: 744.906385936887 and parameters: {'lambda_l1': 7.4189282096398665, 'lambda_l2': 0.024618225705503396, 'num_leaves': 51, 'feature_fraction': 0.6505788453014613, 'bagging_fraction': 0.7900648957708704, 'bagging_freq': 3, 'min_child_samples': 13, 'learning_rate': 0.09379537122452172, 'max_depth': 6, 'n_estimators': 292}. Best is trial 26 with value: 724.1639565808207.\n",
      "[I 2025-03-02 11:48:22,059] Trial 41 finished with value: 738.9897950871884 and parameters: {'lambda_l1': 0.16738689482525176, 'lambda_l2': 0.08434486528839849, 'num_leaves': 78, 'feature_fraction': 0.6915304674060415, 'bagging_fraction': 0.6838619664897031, 'bagging_freq': 3, 'min_child_samples': 16, 'learning_rate': 0.08161936031050812, 'max_depth': 3, 'n_estimators': 468}. Best is trial 26 with value: 724.1639565808207.\n",
      "[I 2025-03-02 11:48:22,400] Trial 42 finished with value: 747.6793493040899 and parameters: {'lambda_l1': 0.18309724940729952, 'lambda_l2': 0.0904018314871201, 'num_leaves': 78, 'feature_fraction': 0.6946008554022777, 'bagging_fraction': 0.6853080228822012, 'bagging_freq': 3, 'min_child_samples': 17, 'learning_rate': 0.08242965682666936, 'max_depth': 3, 'n_estimators': 423}. Best is trial 26 with value: 724.1639565808207.\n",
      "[I 2025-03-02 11:48:22,816] Trial 43 finished with value: 729.8669261525079 and parameters: {'lambda_l1': 0.19061087412907401, 'lambda_l2': 0.2055471390320699, 'num_leaves': 72, 'feature_fraction': 0.7245865839525159, 'bagging_fraction': 0.7078214278236672, 'bagging_freq': 4, 'min_child_samples': 13, 'learning_rate': 0.07881797510531703, 'max_depth': 3, 'n_estimators': 458}. Best is trial 26 with value: 724.1639565808207.\n",
      "[I 2025-03-02 11:48:23,219] Trial 44 finished with value: 757.1038728991007 and parameters: {'lambda_l1': 0.5505362187439381, 'lambda_l2': 0.19709091938693407, 'num_leaves': 65, 'feature_fraction': 0.7133090104761339, 'bagging_fraction': 0.7099218489714504, 'bagging_freq': 5, 'min_child_samples': 23, 'learning_rate': 0.07036737880923266, 'max_depth': 3, 'n_estimators': 431}. Best is trial 26 with value: 724.1639565808207.\n",
      "[I 2025-03-02 11:48:23,755] Trial 45 finished with value: 740.8892775038285 and parameters: {'lambda_l1': 1.2168153362011516, 'lambda_l2': 0.9287218429150907, 'num_leaves': 71, 'feature_fraction': 0.7771854608305809, 'bagging_fraction': 0.6657822456000259, 'bagging_freq': 4, 'min_child_samples': 12, 'learning_rate': 0.07683056929054316, 'max_depth': 4, 'n_estimators': 476}. Best is trial 26 with value: 724.1639565808207.\n",
      "[I 2025-03-02 11:48:24,150] Trial 46 finished with value: 756.0971474571031 and parameters: {'lambda_l1': 0.09526514152841205, 'lambda_l2': 0.49851628728316244, 'num_leaves': 62, 'feature_fraction': 0.7579271533536095, 'bagging_fraction': 0.6209293770355101, 'bagging_freq': 4, 'min_child_samples': 29, 'learning_rate': 0.058056442505415896, 'max_depth': 3, 'n_estimators': 231}. Best is trial 26 with value: 724.1639565808207.\n",
      "[I 2025-03-02 11:48:24,494] Trial 47 finished with value: 754.9435344520132 and parameters: {'lambda_l1': 0.15151149411115353, 'lambda_l2': 0.010567984175461955, 'num_leaves': 96, 'feature_fraction': 0.7170606156686711, 'bagging_fraction': 0.7004704380945744, 'bagging_freq': 3, 'min_child_samples': 45, 'learning_rate': 0.06601947884499869, 'max_depth': 3, 'n_estimators': 176}. Best is trial 26 with value: 724.1639565808207.\n",
      "[I 2025-03-02 11:48:24,952] Trial 48 finished with value: 750.8008760664694 and parameters: {'lambda_l1': 0.6361548844209799, 'lambda_l2': 0.04584786471606354, 'num_leaves': 88, 'feature_fraction': 0.6207893424844597, 'bagging_fraction': 0.7696973294415477, 'bagging_freq': 5, 'min_child_samples': 38, 'learning_rate': 0.07957294116138411, 'max_depth': 4, 'n_estimators': 452}. Best is trial 26 with value: 724.1639565808207.\n",
      "[I 2025-03-02 11:48:25,693] Trial 49 finished with value: 764.0910382911177 and parameters: {'lambda_l1': 0.010299044764324479, 'lambda_l2': 0.2780382508957555, 'num_leaves': 83, 'feature_fraction': 0.6732042224249826, 'bagging_fraction': 0.6420724165512418, 'bagging_freq': 2, 'min_child_samples': 10, 'learning_rate': 0.05385872013555407, 'max_depth': 6, 'n_estimators': 403}. Best is trial 26 with value: 724.1639565808207.\n",
      "Number of finished trials: 50\n",
      "Best LightGBM parameters: {'lambda_l1': 1.1831840825199607, 'lambda_l2': 0.6737215057623974, 'num_leaves': 59, 'feature_fraction': 0.6350702860071836, 'bagging_fraction': 0.627216130866271, 'bagging_freq': 2, 'min_child_samples': 18, 'learning_rate': 0.07819371721475231, 'max_depth': 3, 'n_estimators': 260}\n",
      "Best RMSE: 724.1640\n",
      "Validation RMSE: 407.9871\n",
      "Validation R: 0.7795\n",
      "Optimization history saved to lightgbm_optimization_history.html\n",
      "\n",
      "Parameter importance:\n",
      "  feature_fraction: 0.383\n",
      "  learning_rate: 0.223\n",
      "  num_leaves: 0.175\n",
      "  max_depth: 0.077\n",
      "  bagging_fraction: 0.046\n",
      "  min_child_samples: 0.038\n",
      "  bagging_freq: 0.021\n",
      "  n_estimators: 0.019\n",
      "  lambda_l1: 0.012\n",
      "  lambda_l2: 0.007\n",
      "Parameter importances saved to lightgbm_param_importances.html\n",
      "LightGBM actual vs predicted plot saved\n",
      "LightGBM firm-specific performance visualization saved\n",
      "Current memory usage: 337.18 MB\n",
      "\n",
      "Tuning Prophet for up to 10 different firms with 15 parameter combinations each...\n",
      "Total firms with non-missing data: 50\n",
      "Selected 10 firms for Prophet tuning: [0.0, 30.0, 4.0, 31.0, 32.0, 33.0, 34.0, 35.0, 3.0, 36.0]\n",
      "Firm data points: Firm 0.0: 703, Firm 30.0: 703, Firm 4.0: 703, Firm 31.0: 703, Firm 32.0: 703, Firm 33.0: 703, Firm 34.0: 703, Firm 35.0: 703, Firm 3.0: 703, Firm 36.0: 703\n",
      "Testing 15 parameter combinations for each firm\n",
      "\n",
      "Tuning Prophet for Firm 0.0\n",
      "  Data points for Firm 0.0: 703\n",
      "  Training data: 562 rows\n",
      "  Validation data: 141 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:27 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:28 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 688.6649, R: -0.4194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:29 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 652.5441, R: -0.2744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:29 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 929.2574, R: -1.5843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:29 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.9} - RMSE: 948.5243, R: -1.6926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:29 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 830.1020, R: -1.0622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:30 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.9} - RMSE: 950.5926, R: -1.7044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:30 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.5, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 1090.0323, R: -2.5560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:30 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 930.4121, R: -1.5908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:31 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 652.6111, R: -0.2746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:31 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 945.5456, R: -1.6757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:31 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 697.2004, R: -0.4548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:32 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.5, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 1100.8113, R: -2.6266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:32 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 944.1181, R: -1.6677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:32 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 954.5637, R: -1.7270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:32 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 945.1998, R: -1.6738\n",
      "  Best Prophet parameters for Firm 0.0: {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n",
      "  Best RMSE: 652.5441\n",
      "  Best validation R: -0.2744\n",
      "  Parameter importance visualization saved for Firm 0.0\n",
      "\n",
      "Tuning Prophet for Firm 30.0\n",
      "  Data points for Firm 30.0: 703\n",
      "  Training data: 562 rows\n",
      "  Validation data: 141 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:33 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 151.5860, R: -1.2310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:33 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 136.4205, R: -0.8069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:34 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 175.9076, R: -2.0043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:34 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.9} - RMSE: 171.3552, R: -1.8508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:34 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 160.6186, R: -1.5047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:35 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.9} - RMSE: 176.1259, R: -2.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:35 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.5, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 161.4313, R: -1.5302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:35 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 168.9634, R: -1.7718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:36 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 135.9022, R: -0.7932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:36 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 174.7971, R: -1.9665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:36 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 150.3616, R: -1.1951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:36 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.5, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 158.2727, R: -1.4321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:37 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 174.8770, R: -1.9692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:37 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 171.2568, R: -1.8475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:37 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 174.2973, R: -1.9495\n",
      "  Best Prophet parameters for Firm 30.0: {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n",
      "  Best RMSE: 135.9022\n",
      "  Best validation R: -0.7932\n",
      "  Parameter importance visualization saved for Firm 30.0\n",
      "\n",
      "Tuning Prophet for Firm 4.0\n",
      "  Data points for Firm 4.0: 703\n",
      "  Training data: 562 rows\n",
      "  Validation data: 141 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:38 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 380.9688, R: 0.3225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:38 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 386.6452, R: 0.3022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:38 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 407.9387, R: 0.2232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:39 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.9} - RMSE: 413.2861, R: 0.2027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:39 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 404.3234, R: 0.2369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:39 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.9} - RMSE: 417.3226, R: 0.1871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:39 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.5, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 516.3817, R: -0.2447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:40 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:40 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 397.5474, R: 0.2623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:40 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 386.4759, R: 0.3028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:40 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 414.7674, R: 0.1970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:41 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 383.7593, R: 0.3126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:41 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.5, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 483.2714, R: -0.0902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:41 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 415.0328, R: 0.1960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:41 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:41 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 415.6920, R: 0.1934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:41 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 416.2468, R: 0.1913\n",
      "  Best Prophet parameters for Firm 4.0: {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n",
      "  Best RMSE: 380.9688\n",
      "  Best validation R: 0.3225\n",
      "  Parameter importance visualization saved for Firm 4.0\n",
      "\n",
      "Tuning Prophet for Firm 31.0\n",
      "  Data points for Firm 31.0: 703\n",
      "  Training data: 562 rows\n",
      "  Validation data: 141 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:42 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 75.2575, R: -0.0820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:42 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 74.6891, R: -0.0657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:43 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 86.2519, R: -0.4212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:43 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.9} - RMSE: 88.7338, R: -0.5042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:43 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 85.4118, R: -0.3937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:44 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.9} - RMSE: 90.4637, R: -0.5634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:44 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.5, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 94.4429, R: -0.7040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:44 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 87.3382, R: -0.4572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:44 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 75.0732, R: -0.0767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:44 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:45 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 88.7148, R: -0.5035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:45 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 73.9327, R: -0.0442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:45 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.5, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 94.9111, R: -0.7209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:46 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 88.5944, R: -0.4995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:46 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 90.6655, R: -0.5704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:46 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 88.6368, R: -0.5009\n",
      "  Best Prophet parameters for Firm 31.0: {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n",
      "  Best RMSE: 73.9327\n",
      "  Best validation R: -0.0442\n",
      "  Parameter importance visualization saved for Firm 31.0\n",
      "\n",
      "Tuning Prophet for Firm 32.0\n",
      "  Data points for Firm 32.0: 703\n",
      "  Training data: 562 rows\n",
      "  Validation data: 141 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:46 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 228.7933, R: 0.1226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:47 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 223.6484, R: 0.1616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:47 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 191.5940, R: 0.3847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:47 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.9} - RMSE: 190.9603, R: 0.3888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:48 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 190.1666, R: 0.3938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:48 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.9} - RMSE: 188.8304, R: 0.4023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:48 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.5, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 199.7734, R: 0.3310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:48 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 194.2617, R: 0.3674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:49 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 223.5951, R: 0.1620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:49 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 189.1930, R: 0.4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:49 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 220.8229, R: 0.1826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:49 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.5, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 211.0768, R: 0.2532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:50 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 188.9928, R: 0.4013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:50 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 191.2999, R: 0.3866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:50 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 190.0199, R: 0.3948\n",
      "  Best Prophet parameters for Firm 32.0: {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.9}\n",
      "  Best RMSE: 188.8304\n",
      "  Best validation R: 0.4023\n",
      "  Parameter importance visualization saved for Firm 32.0\n",
      "\n",
      "Tuning Prophet for Firm 33.0\n",
      "  Data points for Firm 33.0: 703\n",
      "  Training data: 562 rows\n",
      "  Validation data: 141 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:50 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:51 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 130.2612, R: -1.9820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:51 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 123.7045, R: -1.6894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:51 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 119.0908, R: -1.4925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:52 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.9} - RMSE: 119.8570, R: -1.5247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:52 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 98.1235, R: -0.6921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:52 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.9} - RMSE: 116.7777, R: -1.3966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:52 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.5, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 125.2564, R: -1.7573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:53 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 122.2568, R: -1.6268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:53 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 123.7242, R: -1.6903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:53 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 118.9355, R: -1.4860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:53 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:54 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 131.8365, R: -2.0546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:54 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.5, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 129.3947, R: -1.9425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:54 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 118.5894, R: -1.4716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:54 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:55 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 118.1106, R: -1.4517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:55 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 118.7180, R: -1.4769\n",
      "  Best Prophet parameters for Firm 33.0: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.95}\n",
      "  Best RMSE: 98.1235\n",
      "  Best validation R: -0.6921\n",
      "  Parameter importance visualization saved for Firm 33.0\n",
      "\n",
      "Tuning Prophet for Firm 34.0\n",
      "  Data points for Firm 34.0: 703\n",
      "  Training data: 562 rows\n",
      "  Validation data: 141 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:55 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:48:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:55 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 66.2021, R: 0.3722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:55 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:56 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 68.8689, R: 0.3206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:56 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 63.1781, R: 0.4283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:56 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.9} - RMSE: 61.0212, R: 0.4666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:56 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 63.9636, R: 0.4139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:57 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.9} - RMSE: 61.5676, R: 0.4570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:57 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.5, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 83.7291, R: -0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:57 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 62.6096, R: 0.4385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:57 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:58 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 68.8698, R: 0.3206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:58 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 61.3144, R: 0.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:58 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 66.8717, R: 0.3594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:59 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.5, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 84.1414, R: -0.0141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:59 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 61.3036, R: 0.4617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:59 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:48:59 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 61.9342, R: 0.4505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48:59 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 61.3162, R: 0.4615\n",
      "  Best Prophet parameters for Firm 34.0: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.9}\n",
      "  Best RMSE: 61.0212\n",
      "  Best validation R: 0.4666\n",
      "  Parameter importance visualization saved for Firm 34.0\n",
      "\n",
      "Tuning Prophet for Firm 35.0\n",
      "  Data points for Firm 35.0: 703\n",
      "  Training data: 562 rows\n",
      "  Validation data: 141 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:00 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "11:49:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:00 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:00 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 196.3632, R: -0.0861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:01 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "11:49:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:01 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 185.4817, R: 0.0309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:02 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 198.8054, R: -0.1133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:02 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.9} - RMSE: 197.4587, R: -0.0983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:02 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 187.1737, R: 0.0132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:03 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.9} - RMSE: 196.8062, R: -0.0910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:03 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.5, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 211.2740, R: -0.2573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:03 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 197.2498, R: -0.0959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:03 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "11:49:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:04 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 185.5406, R: 0.0303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:05 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 197.9516, R: -0.1038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:05 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "11:49:05 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:06 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 196.6410, R: -0.0892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:06 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.5, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 210.4883, R: -0.2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:06 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 196.7887, R: -0.0908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:07 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 196.3917, R: -0.0864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:07 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 197.6801, R: -0.1007\n",
      "  Best Prophet parameters for Firm 35.0: {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n",
      "  Best RMSE: 185.4817\n",
      "  Best validation R: 0.0309\n",
      "  Parameter importance visualization saved for Firm 35.0\n",
      "\n",
      "Tuning Prophet for Firm 3.0\n",
      "  Data points for Firm 3.0: 703\n",
      "  Training data: 562 rows\n",
      "  Validation data: 141 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:07 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "11:49:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:08 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 624.1654, R: -0.6406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:08 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "11:49:08 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:09 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 592.2967, R: -0.4774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:09 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 642.7324, R: -0.7397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:10 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.9} - RMSE: 621.2017, R: -0.6251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:10 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 590.9738, R: -0.4708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:10 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.9} - RMSE: 630.4234, R: -0.6737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:11 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.5, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 671.4745, R: -0.8987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:11 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 624.2094, R: -0.6408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:11 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "11:49:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:12 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 592.3337, R: -0.4775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:12 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 631.9490, R: -0.6818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:12 - cmdstanpy - ERROR - Chain [1] error: error during processing Operation not permitted\n",
      "Optimization terminated abnormally. Falling back to Newton.\n",
      "11:49:12 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:13 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 632.2411, R: -0.6833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:13 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.5, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 646.0726, R: -0.7578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:14 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 631.0594, R: -0.6771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:14 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 621.2348, R: -0.6252\n",
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 633.5407, R: -0.6903\n",
      "  Best Prophet parameters for Firm 3.0: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.95}\n",
      "  Best RMSE: 590.9738\n",
      "  Best validation R: -0.4708\n",
      "  Parameter importance visualization saved for Firm 3.0\n",
      "\n",
      "Tuning Prophet for Firm 36.0\n",
      "  Data points for Firm 36.0: 703\n",
      "  Training data: 562 rows\n",
      "  Validation data: 141 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:14 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:15 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 735.8379, R: -3.5554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:15 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 613.6411, R: -2.1681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:15 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 827.1755, R: -4.7565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:15 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.9} - RMSE: 827.9943, R: -4.7679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:16 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:16 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 688.5562, R: -2.9888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:16 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.9} - RMSE: 833.5774, R: -4.8460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:16 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.5, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 839.9468, R: -4.9356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:17 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.01, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 815.6498, R: -4.5972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:17 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 611.9192, R: -2.1503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:49:17 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 833.6017, R: -4.8463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:17 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 728.6984, R: -3.4675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:18 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.5, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 838.7011, R: -4.9181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:18 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 835.0006, R: -4.8659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:49:18 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.8} - RMSE: 829.3860, R: -4.7873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:49:18 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Parameters {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.95} - RMSE: 831.4445, R: -4.8161\n",
      "  Best Prophet parameters for Firm 36.0: {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n",
      "  Best RMSE: 611.9192\n",
      "  Best validation R: -2.1503\n",
      "  Parameter importance visualization saved for Firm 36.0\n",
      "\n",
      "Prophet predictions by firm visualization saved\n",
      "Prophet parameters by firm summary saved\n",
      "Cross-firm parameter impact visualization saved\n",
      "Current memory usage: 343.06 MB\n",
      "\n",
      "Tuning ensemble weights for each firm...\n",
      "Validation set contains 50 unique firms\n",
      "\n",
      "Tuning ensemble for Firm 0.0\n",
      "  Error tuning ensemble for Firm 0.0: Regressor 'TrafficPctChange' missing from dataframe\n",
      "\n",
      "Tuning ensemble for Firm 30.0\n",
      "  Error tuning ensemble for Firm 30.0: Regressor 'TrafficPctChange' missing from dataframe\n",
      "\n",
      "Tuning ensemble for Firm 4.0\n",
      "  Error tuning ensemble for Firm 4.0: Regressor 'TrafficPctChange' missing from dataframe\n",
      "\n",
      "Tuning ensemble for Firm 31.0\n",
      "  Error tuning ensemble for Firm 31.0: Regressor 'TrafficPctChange' missing from dataframe\n",
      "\n",
      "Tuning ensemble for Firm 32.0\n",
      "  Error tuning ensemble for Firm 32.0: Regressor 'TrafficPctChange' missing from dataframe\n",
      "\n",
      "Tuning ensemble for Firm 33.0\n",
      "  Error tuning ensemble for Firm 33.0: Regressor 'TrafficPctChange' missing from dataframe\n",
      "\n",
      "Tuning ensemble for Firm 34.0\n",
      "  Error tuning ensemble for Firm 34.0: Regressor 'TrafficPctChange' missing from dataframe\n",
      "\n",
      "Tuning ensemble for Firm 35.0\n",
      "  Error tuning ensemble for Firm 35.0: Regressor 'TrafficPctChange' missing from dataframe\n",
      "\n",
      "Tuning ensemble for Firm 3.0\n",
      "  Error tuning ensemble for Firm 3.0: Regressor 'TrafficPctChange' missing from dataframe\n",
      "\n",
      "Tuning ensemble for Firm 36.0\n",
      "  Error tuning ensemble for Firm 36.0: Regressor 'TrafficPctChange' missing from dataframe\n",
      "\n",
      "Global ensemble weights (for firms without specific models):\n",
      "  LightGBM: 0.50, Prophet: 0.50\n",
      "Current memory usage: 343.09 MB\n",
      "\n",
      "==================================================\n",
      "SUMMARY OF MULTI-FIRM MODEL TUNING RESULTS\n",
      "==================================================\n",
      "LightGBM global model:\n",
      "  Best parameters: {'lambda_l1': 1.1831840825199607, 'lambda_l2': 0.6737215057623974, 'num_leaves': 59, 'feature_fraction': 0.6350702860071836, 'bagging_fraction': 0.627216130866271, 'bagging_freq': 2, 'min_child_samples': 18, 'learning_rate': 0.07819371721475231, 'max_depth': 3, 'n_estimators': 260}\n",
      "  Validation RMSE: 724.1640\n",
      "  Validation R: 0.7795\n",
      "\n",
      "Prophet models:\n",
      "  Number of firm-specific models: 10\n",
      "  Firm 0.0:\n",
      "    Best parameters: {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n",
      "    Validation RMSE: 652.5441\n",
      "    Validation R: -0.2744\n",
      "  Firm 30.0:\n",
      "    Best parameters: {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n",
      "    Validation RMSE: 135.9022\n",
      "    Validation R: -0.7932\n",
      "  Firm 4.0:\n",
      "    Best parameters: {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n",
      "    Validation RMSE: 380.9688\n",
      "    Validation R: 0.3225\n",
      "  Firm 31.0:\n",
      "    Best parameters: {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n",
      "    Validation RMSE: 73.9327\n",
      "    Validation R: -0.0442\n",
      "  Firm 32.0:\n",
      "    Best parameters: {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.9}\n",
      "    Validation RMSE: 188.8304\n",
      "    Validation R: 0.4023\n",
      "  Firm 33.0:\n",
      "    Best parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.95}\n",
      "    Validation RMSE: 98.1235\n",
      "    Validation R: -0.6921\n",
      "  Firm 34.0:\n",
      "    Best parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.9}\n",
      "    Validation RMSE: 61.0212\n",
      "    Validation R: 0.4666\n",
      "  Firm 35.0:\n",
      "    Best parameters: {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n",
      "    Validation RMSE: 185.4817\n",
      "    Validation R: 0.0309\n",
      "  Firm 3.0:\n",
      "    Best parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.95}\n",
      "    Validation RMSE: 590.9738\n",
      "    Validation R: -0.4708\n",
      "  Firm 36.0:\n",
      "    Best parameters: {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n",
      "    Validation RMSE: 611.9192\n",
      "    Validation R: -2.1503\n",
      "\n",
      "Ensemble weights:\n",
      "  Global weights: LightGBM=0.50, Prophet=0.50\n",
      "\n",
      "Model RMSE comparison visualization saved\n",
      "All tuning results saved to multi_firm_tuning_results.json\n",
      "\n",
      "Multi-firm model tuning process completed successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FOCUSED MULTI-FIRM HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"Using existing training data from feature engineering step\")\n",
    "\n",
    "print(\"Creating validation split while preserving time series structure...\")\n",
    "train_dfs = []\n",
    "valid_dfs = []\n",
    "\n",
    "for firm in df_train['FirmID'].unique():\n",
    "    firm_data = df_train[df_train['FirmID'] == firm].copy().sort_values('Date')\n",
    "\n",
    "    train_size = int(0.8 * len(firm_data))\n",
    "\n",
    "    if train_size >= len(firm_data):\n",
    "        train_size = max(int(0.7 * len(firm_data)), len(firm_data) - 5)\n",
    "\n",
    "    train_dfs.append(firm_data.iloc[:train_size])\n",
    "    valid_dfs.append(firm_data.iloc[train_size:])\n",
    "\n",
    "df_train_model = pd.concat(train_dfs).reset_index(drop=True)\n",
    "df_valid = pd.concat(valid_dfs).reset_index(drop=True)\n",
    "\n",
    "print(f\"Training set: {len(df_train_model)} rows, Validation set: {len(df_valid)} rows\")\n",
    "print(f\"Number of firms in training: {df_train_model['FirmID'].nunique()}\")\n",
    "print(f\"Number of firms in validation: {df_valid['FirmID'].nunique()}\")\n",
    "\n",
    "volatility_features = [\n",
    "        'FirmID', 'FirmDailyTraffic',  # Core features\n",
    "        'DayOfWeek', 'Month', 'IsWeekend',  # Time components\n",
    "        'Sales_lag1', 'Sales_lag7'  # Minimal lag features\n",
    "    ]\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "X_train = df_train_model[volatility_features]\n",
    "y_train = df_train_model['Sales']\n",
    "X_valid = df_valid[volatility_features]\n",
    "y_valid = df_valid['Sales']\n",
    "\n",
    "def calculate_r2(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate R-squared (coefficient of determination) with proper error handling\n",
    "\n",
    "    Args:\n",
    "        y_true: Actual values\n",
    "        y_pred: Predicted values\n",
    "\n",
    "    Returns:\n",
    "        r2 value (raw, not clamped)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return r2_score(y_true, y_pred)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error calculating R2: {e}\")\n",
    "        return float('nan')\n",
    "\n",
    "def plot_optimization_history(study, filename=\"optuna_optimization_history.html\"):\n",
    "    \"\"\"\n",
    "    Plot the optimization history of an Optuna study using Plotly\n",
    "    \"\"\"\n",
    "    trials = study.trials\n",
    "    values = [t.value for t in trials if t.value is not None]\n",
    "    best_values = np.minimum.accumulate(values)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(len(values))),\n",
    "        y=values,\n",
    "        mode='markers',\n",
    "        name='Trial Value',\n",
    "        marker=dict(color='blue', size=8)\n",
    "    ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=list(range(len(best_values))),\n",
    "        y=best_values,\n",
    "        mode='lines',\n",
    "        name='Best Value',\n",
    "        line=dict(color='red', width=2)\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Optimization History\",\n",
    "        xaxis_title=\"Trial\",\n",
    "        yaxis_title=\"Objective Value (RMSE)\",\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "\n",
    "    fig.write_html(filename)\n",
    "    print(f\"Optimization history saved to {filename}\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_param_importances(study, filename=\"optuna_param_importances.html\"):\n",
    "    \"\"\"\n",
    "    Plot the parameter importances of an Optuna study using Plotly\n",
    "    \"\"\"\n",
    "    importances = optuna.importance.get_param_importances(study)\n",
    "    sorted_importances = dict(sorted(importances.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=list(sorted_importances.keys()),\n",
    "        y=list(sorted_importances.values()),\n",
    "        marker_color='royalblue'\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Parameter Importances\",\n",
    "        xaxis_title=\"Parameter\",\n",
    "        yaxis_title=\"Importance\",\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "\n",
    "    fig.write_html(filename)\n",
    "    print(f\"Parameter importances saved to {filename}\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "def stratified_firm_sample(X, y, max_per_firm=2000, min_per_firm=50, max_total=10000):\n",
    "    \"\"\"\n",
    "    Create a stratified sample across firms to ensure all firms are represented\n",
    "    while preserving time dependencies within each firm's data\n",
    "\n",
    "    Args:\n",
    "        X: Feature DataFrame with FirmID column\n",
    "        y: Target variable\n",
    "        max_per_firm: Maximum samples to take from any single firm\n",
    "        min_per_firm: Minimum samples to include from a firm when available\n",
    "        max_total: Maximum total samples to return\n",
    "\n",
    "    Returns:\n",
    "        List of indices for the stratified sample\n",
    "    \"\"\"\n",
    "    firms = X['FirmID'].unique()\n",
    "    sampled_indices = []\n",
    "\n",
    "    for firm in firms:\n",
    "        firm_indices = X[X['FirmID'] == firm].index\n",
    "        if len(firm_indices) <= min_per_firm:\n",
    "            firm_sample = firm_indices\n",
    "        else:\n",
    "            sorted_indices = sorted(firm_indices)\n",
    "            sample_size = min(max_per_firm, len(firm_indices))\n",
    "\n",
    "            step = len(sorted_indices) / sample_size\n",
    "            systematic_indices = [sorted_indices[int(i * step)] for i in range(sample_size)]\n",
    "            firm_sample = systematic_indices\n",
    "\n",
    "        sampled_indices.extend(firm_sample)\n",
    "\n",
    "    if len(sampled_indices) > max_total:\n",
    "        firm_counts = X.loc[sampled_indices, 'FirmID'].value_counts()\n",
    "        prop_sample_indices = []\n",
    "\n",
    "        for firm, count in firm_counts.items():\n",
    "            firm_indices = [idx for idx in sampled_indices if X.loc[idx, 'FirmID'] == firm]\n",
    "\n",
    "            alloc_size = int(max_total * (count / len(sampled_indices)))\n",
    "\n",
    "            alloc_size = max(1, alloc_size)\n",
    "\n",
    "            if len(firm_indices) <= alloc_size:\n",
    "                prop_sample_indices.extend(firm_indices)\n",
    "            else:\n",
    "                step = len(firm_indices) / alloc_size\n",
    "                systematic_indices = [firm_indices[int(i * step)] for i in range(alloc_size)]\n",
    "                prop_sample_indices.extend(systematic_indices)\n",
    "\n",
    "        sampled_indices = prop_sample_indices[:max_total]\n",
    "\n",
    "    return sampled_indices\n",
    "\n",
    "\n",
    "def tune_lightgbm_optuna(X_train, y_train, X_valid, y_valid, tscv, n_trials=100):\n",
    "    \"\"\"\n",
    "    Tune LightGBM hyperparameters using Optuna with Bayesian optimization\n",
    "\n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Target variable\n",
    "        X_valid: Validation features\n",
    "        y_valid: Validation target\n",
    "        tscv: TimeSeriesSplit cross-validation object\n",
    "        n_trials: Number of trials for optimization\n",
    "\n",
    "    Returns:\n",
    "        Best parameters and performance metrics\n",
    "    \"\"\"\n",
    "    print(\"\\nTuning LightGBM hyperparameters using Optuna...\")\n",
    "    \n",
    "    categorical_columns = ['FirmID', 'DayOfWeek', 'Month', 'DayOfMonth', 'IsWeekend']\n",
    "    for col in categorical_columns:\n",
    "        if col in X_train.columns:\n",
    "            X_train[col] = X_train[col].astype(int)\n",
    "            X_valid[col] = X_valid[col].astype(int)\n",
    "    \n",
    "    categorical_features = [X_train.columns.get_loc(col) for col in categorical_columns \n",
    "                          if col in X_train.columns]\n",
    "\n",
    "    # print(X_train.info())\n",
    "\n",
    "    def objective(trial):\n",
    "        param = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'verbosity': -1,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'lambda_l1': trial.suggest_float('lambda_l1', 0.01, 10.0, log=True),\n",
    "            'lambda_l2': trial.suggest_float('lambda_l2', 0.01, 10.0, log=True),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "            'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 0.9),\n",
    "            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 0.9),\n",
    "            'bagging_freq': trial.suggest_int('bagging_freq', 1, 5),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 10, 50),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 500)\n",
    "        }\n",
    "\n",
    "        cv_scores_rmse = []\n",
    "        cv_scores_r2 = []\n",
    "\n",
    "        for train_idx, val_idx in tscv.split(X_train):\n",
    "            X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "            y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "            train_data = lgb.Dataset(X_train_fold, label=y_train_fold, categorical_feature=categorical_features)\n",
    "            val_data = lgb.Dataset(X_val_fold, label=y_val_fold, categorical_feature=categorical_features)\n",
    "\n",
    "            model = lgb.train(\n",
    "                param,\n",
    "                train_data,\n",
    "                num_boost_round=param['n_estimators'],\n",
    "                valid_sets=[val_data],\n",
    "                valid_names=['validation'],\n",
    "                callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n",
    "            )\n",
    "\n",
    "            y_pred = model.predict(X_val_fold)\n",
    "            rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n",
    "            r2 = calculate_r2(y_val_fold, y_pred)\n",
    "\n",
    "            cv_scores_rmse.append(rmse)\n",
    "            cv_scores_r2.append(r2)\n",
    "\n",
    "        return np.mean(cv_scores_rmse)\n",
    "\n",
    "    sampler = optuna.samplers.TPESampler(seed=42)\n",
    "\n",
    "    try:\n",
    "        study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "        best_params = study.best_params\n",
    "        best_score = study.best_value\n",
    "\n",
    "        print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "        print(f\"Best LightGBM parameters: {best_params}\")\n",
    "        print(f\"Best RMSE: {best_score:.4f}\")\n",
    "\n",
    "        final_model = lgb.LGBMRegressor(**best_params, categorical_feature=categorical_features)\n",
    "        final_model.fit(X_train, y_train)\n",
    "\n",
    "        y_valid_pred = final_model.predict(X_valid)\n",
    "\n",
    "        valid_rmse = np.sqrt(mean_squared_error(y_valid, y_valid_pred))\n",
    "        valid_r2 = calculate_r2(y_valid, y_valid_pred)\n",
    "\n",
    "        print(f\"Validation RMSE: {valid_rmse:.4f}\")\n",
    "        print(f\"Validation R: {valid_r2:.4f}\")\n",
    "\n",
    "        if len(study.trials) > 5:\n",
    "            try:\n",
    "                plot_optimization_history(study, \"lightgbm_optimization_history.html\")\n",
    "\n",
    "                importances = optuna.importance.get_param_importances(study)\n",
    "                print(\"\\nParameter importance:\")\n",
    "                for key, value in importances.items():\n",
    "                    print(f\"  {key}: {value:.3f}\")\n",
    "                plot_param_importances(study, \"lightgbm_param_importances.html\")\n",
    "\n",
    "                if len(y_valid) > 1000:\n",
    "                    sample_idx = np.random.choice(len(y_valid), 1000, replace=False)\n",
    "                    y_true_sample = y_valid.iloc[sample_idx]\n",
    "                    y_pred_sample = y_valid_pred[sample_idx]\n",
    "                else:\n",
    "                    y_true_sample = y_valid\n",
    "                    y_pred_sample = y_valid_pred\n",
    "\n",
    "                fig = go.Figure()\n",
    "\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=y_true_sample,\n",
    "                    y=y_pred_sample,\n",
    "                    mode='markers',\n",
    "                    marker=dict(color='blue', size=8, opacity=0.6),\n",
    "                    name='Predictions'\n",
    "                ))\n",
    "\n",
    "                max_val = max(max(y_true_sample), max(y_pred_sample))\n",
    "                min_val = min(min(y_true_sample), min(y_pred_sample))\n",
    "\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=[min_val, max_val],\n",
    "                    y=[min_val, max_val],\n",
    "                    mode='lines',\n",
    "                    line=dict(color='red', dash='dash'),\n",
    "                    name='Perfect Prediction'\n",
    "                ))\n",
    "\n",
    "                fig.update_layout(\n",
    "                    title=f\"LightGBM: Actual vs Predicted Sales (Validation R = {valid_r2:.4f})\",\n",
    "                    xaxis_title=\"Actual Sales\",\n",
    "                    yaxis_title=\"Predicted Sales\",\n",
    "                    template=\"plotly_white\"\n",
    "                )\n",
    "\n",
    "                fig.write_html(\"lightgbm_actual_vs_predicted.html\")\n",
    "                print(\"LightGBM actual vs predicted plot saved\")\n",
    "\n",
    "                firm_metrics = {}\n",
    "                for firm in X_valid['FirmID'].unique():\n",
    "                    firm_mask = X_valid['FirmID'] == firm\n",
    "                    firm_y_true = y_valid[firm_mask]\n",
    "                    firm_y_pred = y_valid_pred[firm_mask]\n",
    "\n",
    "                    if len(firm_y_true) > 0:\n",
    "                        firm_rmse = np.sqrt(mean_squared_error(firm_y_true, firm_y_pred))\n",
    "                        firm_r2 = calculate_r2(firm_y_true, firm_y_pred)\n",
    "                        firm_metrics[firm] = {'rmse': firm_rmse, 'r2': firm_r2, 'count': len(firm_y_true)}\n",
    "\n",
    "                firms = list(firm_metrics.keys())\n",
    "                if len(firms) > 0:\n",
    "                    firms_to_show = sorted(firms, key=lambda x: firm_metrics[x]['count'], reverse=True)[:15]\n",
    "\n",
    "                    fig = make_subplots(rows=2, cols=1,\n",
    "                                        subplot_titles=[\"LightGBM RMSE by Firm\",\n",
    "                                                        \"LightGBM R by Firm\"],\n",
    "                                        vertical_spacing=0.15)\n",
    "\n",
    "                    fig.add_trace(\n",
    "                        go.Bar(\n",
    "                            x=[f\"Firm {f}\" for f in firms_to_show],\n",
    "                            y=[firm_metrics[f]['rmse'] for f in firms_to_show],\n",
    "                            name=\"RMSE\",\n",
    "                            marker_color='red',\n",
    "                            text=[f\"{firm_metrics[f]['rmse']:.4f}\" for f in firms_to_show],\n",
    "                            textposition=\"outside\"\n",
    "                        ),\n",
    "                        row=1, col=1\n",
    "                    )\n",
    "\n",
    "                    fig.add_trace(\n",
    "                        go.Bar(\n",
    "                            x=[f\"Firm {f}\" for f in firms_to_show],\n",
    "                            y=[firm_metrics[f]['r2'] for f in firms_to_show],\n",
    "                            name=\"R\",\n",
    "                            marker_color='blue',\n",
    "                            text=[f\"{firm_metrics[f]['r2']:.4f}\" for f in firms_to_show],\n",
    "                            textposition=\"outside\"\n",
    "                        ),\n",
    "                        row=2, col=1\n",
    "                    )\n",
    "\n",
    "                    fig.update_layout(\n",
    "                        height=800,\n",
    "                        title_text=\"LightGBM Performance by Firm\",\n",
    "                        showlegend=False\n",
    "                    )\n",
    "\n",
    "                    fig.write_html(\"lightgbm_firm_performance.html\")\n",
    "                    print(\"LightGBM firm-specific performance visualization saved\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating visualization: {e}\")\n",
    "\n",
    "        if 'n_estimators' in best_params:\n",
    "            best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "\n",
    "        return best_params, best_score, valid_r2, final_model\n",
    "    except Exception as e:\n",
    "        print(f\"Error during LightGBM Optuna tuning: {e}\")\n",
    "        print(\"Full traceback:\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        fallback_params = {\n",
    "            'lambda_l1': 0.1,\n",
    "            'lambda_l2': 0.1,\n",
    "            'num_leaves': 31,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'min_child_samples': 20,\n",
    "            'learning_rate': 0.05,\n",
    "            'max_depth': 5,\n",
    "            'n_estimators': 100\n",
    "        }\n",
    "        print(f\"Using fallback parameters: {fallback_params}\")\n",
    "        return fallback_params, None, None, None\n",
    "\n",
    "\n",
    "def tune_prophet_for_firms(df, top_firms=None, param_combinations=15, max_firms=10):\n",
    "    \"\"\"\n",
    "    Tune Prophet hyperparameters for multiple firms and return a dictionary of models\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with Date, Sales, FirmDailyTraffic and FirmID\n",
    "        top_firms: List of firm IDs to focus on (if None, uses firms with most data)\n",
    "        param_combinations: Number of random parameter combinations to try\n",
    "        max_firms: Maximum number of firms to tune for\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with firm-specific prophet parameters, models and metrics\n",
    "    \"\"\"\n",
    "    print(\n",
    "        f\"\\nTuning Prophet for up to {max_firms} different firms with {param_combinations} parameter combinations each...\")\n",
    "\n",
    "    firm_counts = df['FirmID'].value_counts()\n",
    "    print(f\"Total firms with non-missing data: {len(firm_counts)}\")\n",
    "\n",
    "    if top_firms is None:\n",
    "        valid_firms = firm_counts[firm_counts >= 30].index.tolist()\n",
    "\n",
    "        top_firms = valid_firms[:max_firms]\n",
    "    else:\n",
    "        top_firms = [firm for firm in top_firms if firm in firm_counts.index and firm_counts[firm] >= 30]\n",
    "        top_firms = top_firms[:max_firms]\n",
    "\n",
    "    print(f\"Selected {len(top_firms)} firms for Prophet tuning: {top_firms}\")\n",
    "    print(f\"Firm data points: {', '.join([f'Firm {f}: {firm_counts[f]}' for f in top_firms])}\")\n",
    "\n",
    "    firm_results = {}\n",
    "\n",
    "    param_grid = {\n",
    "        'changepoint_prior_scale': [0.001, 0.01, 0.05, 0.1, 0.5],\n",
    "        'seasonality_prior_scale': [0.01, 0.1, 1.0, 10.0],\n",
    "        'holidays_prior_scale': [0.01, 0.1, 1.0, 10.0],\n",
    "        'seasonality_mode': ['additive'],\n",
    "        'changepoint_range': [0.8, 0.9, 0.95]\n",
    "    }\n",
    "\n",
    "    all_params = [dict(zip(param_grid.keys(), v)) for v in itertools.product(*param_grid.values())]\n",
    "    np.random.seed(42)\n",
    "    if len(all_params) > param_combinations:\n",
    "        param_samples = np.random.choice(range(len(all_params)), param_combinations, replace=False)\n",
    "        param_list = [all_params[i] for i in param_samples]\n",
    "    else:\n",
    "        param_list = all_params\n",
    "\n",
    "    print(f\"Testing {len(param_list)} parameter combinations for each firm\")\n",
    "\n",
    "    prophet_regressors = [\n",
    "        'FirmDailyTraffic',\n",
    "        'TrafficPctChange'\n",
    "    ]\n",
    "\n",
    "    combined_fig = make_subplots(rows=min(len(top_firms), 6), cols=1,\n",
    "                                 subplot_titles=[f\"Firm {firm}\" for firm in top_firms[:6]],\n",
    "                                 vertical_spacing=0.1)\n",
    "\n",
    "    param_importance_dfs = []\n",
    "\n",
    "    for i, firm_id in enumerate(top_firms):\n",
    "        print(f\"\\nTuning Prophet for Firm {firm_id}\")\n",
    "\n",
    "        firm_data = df[df['FirmID'] == firm_id].copy()\n",
    "        print(f\"  Data points for Firm {firm_id}: {len(firm_data)}\")\n",
    "\n",
    "        train_size = int(0.8 * len(firm_data))\n",
    "        if train_size >= len(firm_data):\n",
    "            train_size = max(int(0.7 * len(firm_data)), len(firm_data) - 5)\n",
    "\n",
    "        train_df = firm_data.iloc[:train_size].copy()\n",
    "        val_df = firm_data.iloc[train_size:].copy()\n",
    "\n",
    "        train_prophet = train_df.rename(columns={'Date': 'ds', 'Sales': 'y'})\n",
    "        val_prophet = val_df.rename(columns={'Date': 'ds', 'Sales': 'y'})\n",
    "\n",
    "        for regressor in prophet_regressors:\n",
    "            if regressor in train_prophet.columns:\n",
    "                if train_prophet[regressor].isna().any():\n",
    "\n",
    "                    train_prophet[regressor] = train_prophet[regressor].fillna(train_prophet[regressor].median())\n",
    "\n",
    "                if val_prophet[regressor].isna().any():\n",
    "\n",
    "                    val_prophet[regressor] = val_prophet[regressor].fillna(train_prophet[regressor].median())\n",
    "\n",
    "        print(f\"  Training data: {len(train_prophet)} rows\")\n",
    "        print(f\"  Validation data: {len(val_prophet)} rows\")\n",
    "\n",
    "        if len(train_prophet) < 10 or len(val_prophet) < 5:\n",
    "            print(f\"  Not enough data for Firm {firm_id}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        param_results = []\n",
    "        rmses = []\n",
    "        r2s = []\n",
    "        models = []\n",
    "\n",
    "        for params in param_list:\n",
    "            try:\n",
    "                model = Prophet(\n",
    "                    changepoint_prior_scale=params['changepoint_prior_scale'],\n",
    "                    seasonality_prior_scale=params['seasonality_prior_scale'],\n",
    "                    holidays_prior_scale=params['holidays_prior_scale'],\n",
    "                    seasonality_mode=params['seasonality_mode'],\n",
    "                    changepoint_range=params['changepoint_range'],\n",
    "                    yearly_seasonality=True,\n",
    "                    weekly_seasonality=True,\n",
    "                    daily_seasonality=False\n",
    "                )\n",
    "\n",
    "                for regressor in prophet_regressors:\n",
    "                    if regressor in train_prophet.columns:\n",
    "                        model.add_regressor(regressor)\n",
    "\n",
    "                try:\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.filterwarnings(\"ignore\")\n",
    "                        model.fit(train_prophet)\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error during Prophet model fit with parameters {params}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                future = pd.DataFrame(val_prophet['ds'])\n",
    "\n",
    "                for regressor in prophet_regressors:\n",
    "                    if regressor in val_prophet.columns:\n",
    "                        future[regressor] = val_prophet[regressor].values\n",
    "\n",
    "                forecast = model.predict(future)\n",
    "\n",
    "                y_true = val_prophet['y'].values\n",
    "                y_pred = forecast['yhat'].values\n",
    "                rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "                r2 = calculate_r2(y_true, y_pred)\n",
    "\n",
    "                rmses.append((params, rmse))\n",
    "                r2s.append((params, r2))\n",
    "                models.append((params, model))\n",
    "\n",
    "                param_results.append({\n",
    "                    'firm_id': firm_id,\n",
    "                    'changepoint_prior_scale': params['changepoint_prior_scale'],\n",
    "                    'seasonality_prior_scale': params['seasonality_prior_scale'],\n",
    "                    'holidays_prior_scale': params['holidays_prior_scale'],\n",
    "                    'changepoint_range': params['changepoint_range'],\n",
    "                    'rmse': rmse,\n",
    "                    'r2': r2\n",
    "                })\n",
    "\n",
    "                print(f\"  Parameters {params} - RMSE: {rmse:.4f}, R: {r2:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error with Prophet parameters {params} for Firm {firm_id}: {e}\")\n",
    "\n",
    "        if param_results:\n",
    "            param_df = pd.DataFrame(param_results)\n",
    "            param_importance_dfs.append(param_df)\n",
    "\n",
    "        if rmses:\n",
    "            best_params, best_rmse = min(rmses, key=lambda x: x[1])\n",
    "\n",
    "            best_r2 = next((r2 for params, r2 in r2s if params == best_params), float('nan'))\n",
    "\n",
    "            best_model = next((model for params, model in models if params == best_params), None)\n",
    "\n",
    "            print(f\"  Best Prophet parameters for Firm {firm_id}: {best_params}\")\n",
    "            print(f\"  Best RMSE: {best_rmse:.4f}\")\n",
    "            print(f\"  Best validation R: {best_r2:.4f}\")\n",
    "\n",
    "            future = pd.DataFrame(val_prophet['ds'])\n",
    "\n",
    "            for regressor in prophet_regressors:\n",
    "                if regressor in val_prophet.columns:\n",
    "                    future[regressor] = val_prophet[regressor].values\n",
    "\n",
    "            forecast = best_model.predict(future)\n",
    "\n",
    "            if i < 6:\n",
    "                combined_fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=val_prophet['ds'],\n",
    "                        y=val_prophet['y'],\n",
    "                        mode='markers',\n",
    "                        name=f'Actual (Firm {firm_id})',\n",
    "                        marker=dict(color='blue')\n",
    "                    ),\n",
    "                    row=i + 1, col=1\n",
    "                )\n",
    "\n",
    "                combined_fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=val_prophet['ds'],\n",
    "                        y=forecast['yhat'],\n",
    "                        mode='lines',\n",
    "                        name=f'Predicted (Firm {firm_id})',\n",
    "                        line=dict(color='red')\n",
    "                    ),\n",
    "                    row=i + 1, col=1\n",
    "                )\n",
    "\n",
    "            firm_results[firm_id] = {\n",
    "                'best_params': best_params,\n",
    "                'best_rmse': best_rmse,\n",
    "                'best_r2': best_r2,\n",
    "                'best_model': best_model,\n",
    "                'training_data': train_prophet,\n",
    "                'validation_data': val_prophet,\n",
    "                'all_results': param_results\n",
    "            }\n",
    "\n",
    "            if len(param_results) > 5:\n",
    "                try:\n",
    "                    param_df = pd.DataFrame(param_results)\n",
    "\n",
    "                    corr_rmse = param_df[['changepoint_prior_scale', 'seasonality_prior_scale',\n",
    "                                          'holidays_prior_scale', 'changepoint_range']].corrwith(param_df['rmse'])\n",
    "\n",
    "                    corr_r2 = param_df[['changepoint_prior_scale', 'seasonality_prior_scale',\n",
    "                                        'holidays_prior_scale', 'changepoint_range']].corrwith(param_df['r2'])\n",
    "\n",
    "                    param_fig = make_subplots(rows=1, cols=2,\n",
    "                                              subplot_titles=[f\"Parameter Correlation with RMSE\",\n",
    "                                                              f\"Parameter Correlation with R\"])\n",
    "\n",
    "                    param_fig.add_trace(\n",
    "                        go.Bar(\n",
    "                            x=corr_rmse.index,\n",
    "                            y=corr_rmse.values,\n",
    "                            name=\"Correlation with RMSE\",\n",
    "                            marker_color='red'\n",
    "                        ),\n",
    "                        row=1, col=1\n",
    "                    )\n",
    "\n",
    "                    param_fig.add_trace(\n",
    "                        go.Bar(\n",
    "                            x=corr_r2.index,\n",
    "                            y=corr_r2.values,\n",
    "                            name=\"Correlation with R\",\n",
    "                            marker_color='blue'\n",
    "                        ),\n",
    "                        row=1, col=2\n",
    "                    )\n",
    "\n",
    "                    param_fig.update_layout(\n",
    "                        title=f\"Firm {firm_id} - Prophet Parameter Importance\",\n",
    "                        height=400,\n",
    "                        width=800\n",
    "                    )\n",
    "\n",
    "                    param_fig.write_html(f\"prophet_firm_{firm_id}_param_importance.html\")\n",
    "                    print(f\"  Parameter importance visualization saved for Firm {firm_id}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error creating parameter importance visualization for Firm {firm_id}: {e}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"  No valid models for Firm {firm_id}\")\n",
    "\n",
    "    combined_fig.update_layout(\n",
    "        height=300 * min(len(top_firms), 6),\n",
    "        title_text=\"Prophet Model Predictions by Firm\",\n",
    "        showlegend=True\n",
    "    )\n",
    "\n",
    "    combined_fig.write_html(\"prophet_predictions_by_firm.html\")\n",
    "    print(\"\\nProphet predictions by firm visualization saved\")\n",
    "\n",
    "    firm_ids = list(firm_results.keys())\n",
    "\n",
    "    if firm_ids:\n",
    "        summary_table = go.Figure(data=[go.Table(\n",
    "            header=dict(\n",
    "                values=[\"Firm ID\", \"changepoint_prior_scale\", \"seasonality_prior_scale\",\n",
    "                        \"holidays_prior_scale\", \"changepoint_range\", \"RMSE\", \"R\"],\n",
    "                fill_color='paleturquoise',\n",
    "                align='left'\n",
    "            ),\n",
    "            cells=dict(\n",
    "                values=[\n",
    "                    firm_ids,\n",
    "                    [firm_results[firm]['best_params']['changepoint_prior_scale'] for firm in firm_ids],\n",
    "                    [firm_results[firm]['best_params']['seasonality_prior_scale'] for firm in firm_ids],\n",
    "                    [firm_results[firm]['best_params']['holidays_prior_scale'] for firm in firm_ids],\n",
    "                    [firm_results[firm]['best_params']['changepoint_range'] for firm in firm_ids],\n",
    "                    [f\"{firm_results[firm]['best_rmse']:.4f}\" for firm in firm_ids],\n",
    "                    [f\"{firm_results[firm]['best_r2']:.4f}\" for firm in firm_ids]\n",
    "                ],\n",
    "                fill_color='lavender',\n",
    "                align='left'\n",
    "            )\n",
    "        )])\n",
    "\n",
    "        summary_table.update_layout(title=\"Prophet Best Parameters by Firm\")\n",
    "        summary_table.write_html(\"prophet_parameters_by_firm.html\")\n",
    "        print(\"Prophet parameters by firm summary saved\")\n",
    "\n",
    "        if len(param_importance_dfs) > 1:\n",
    "            try:\n",
    "                all_params_df = pd.concat(param_importance_dfs)\n",
    "\n",
    "                param_dist_fig = make_subplots(rows=2, cols=2,\n",
    "                                               subplot_titles=[\"changepoint_prior_scale\", \"seasonality_prior_scale\",\n",
    "                                                               \"holidays_prior_scale\", \"changepoint_range\"],\n",
    "                                               vertical_spacing=0.2)\n",
    "\n",
    "                for i, param in enumerate(['changepoint_prior_scale', 'seasonality_prior_scale',\n",
    "                                           'holidays_prior_scale', 'changepoint_range']):\n",
    "                    row, col = i // 2 + 1, i % 2 + 1\n",
    "\n",
    "                    param_groups = all_params_df.groupby(param)['rmse'].agg(['mean', 'median', 'std']).reset_index()\n",
    "                    param_groups = param_groups.sort_values(param)\n",
    "\n",
    "                    param_dist_fig.add_trace(\n",
    "                        go.Bar(\n",
    "                            x=param_groups[param].astype(str),\n",
    "                            y=param_groups['mean'],\n",
    "                            name=\"Mean RMSE\",\n",
    "                            marker_color='red',\n",
    "                            error_y=dict(\n",
    "                                type='data',\n",
    "                                array=param_groups['std'],\n",
    "                                visible=True\n",
    "                            )\n",
    "                        ),\n",
    "                        row=row, col=col\n",
    "                    )\n",
    "\n",
    "                param_dist_fig.update_layout(\n",
    "                    title=\"Parameter Value Impact on RMSE Across All Firms\",\n",
    "                    height=700,\n",
    "                    showlegend=False\n",
    "                )\n",
    "\n",
    "                param_dist_fig.write_html(\"prophet_parameter_impact_all_firms.html\")\n",
    "                print(\"Cross-firm parameter impact visualization saved\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating cross-firm parameter analysis: {e}\")\n",
    "\n",
    "    return firm_results\n",
    "\n",
    "\n",
    "def build_firm_specific_ensemble(X_train, y_train, X_valid, y_valid,\n",
    "                                 lgb_model, prophet_firm_results,\n",
    "                                 weight_steps=21):\n",
    "    \"\"\"\n",
    "    Build firm-specific ensemble models by tuning weights for each firm\n",
    "\n",
    "    Args:\n",
    "        X_train: Training features for LightGBM\n",
    "        y_train: Target variable for training\n",
    "        X_valid: Validation features for LightGBM\n",
    "        y_valid: Validation target\n",
    "        lgb_model: Trained LightGBM model\n",
    "        prophet_firm_results: Dictionary of Prophet models by firm\n",
    "        weight_steps: Number of weight combinations to try (resolution)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of firm-specific ensemble weights and metrics\n",
    "    \"\"\"\n",
    "    print(\"\\nTuning ensemble weights for each firm...\")\n",
    "\n",
    "    validation_firms = X_valid['FirmID'].unique()\n",
    "    print(f\"Validation set contains {len(validation_firms)} unique firms\")\n",
    "\n",
    "    lgb_preds = lgb_model.predict(X_valid)\n",
    "\n",
    "    validation_df = X_valid.copy()\n",
    "    validation_df['true_sales'] = y_valid.values\n",
    "    validation_df['lgb_pred'] = lgb_preds\n",
    "    validation_df['Date'] = df_valid['Date'] \n",
    "\n",
    "    weight_grid = np.linspace(0, 1, weight_steps) \n",
    "\n",
    "    ensemble_results = {}\n",
    "\n",
    "    firm_weights = []\n",
    "    firm_names = []\n",
    "    firm_rmses = []\n",
    "    firm_r2s = []\n",
    "\n",
    "    for firm_id in prophet_firm_results.keys():\n",
    "        print(f\"\\nTuning ensemble for Firm {firm_id}\")\n",
    "\n",
    "        firm_valid = validation_df[validation_df['FirmID'] == firm_id].copy()\n",
    "\n",
    "        if len(firm_valid) == 0:\n",
    "            print(f\"  No validation data for Firm {firm_id}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        prophet_model = prophet_firm_results[firm_id]['best_model']\n",
    "\n",
    "        prophet_valid = firm_valid.rename(columns={'Date': 'ds'})\n",
    "\n",
    "        required_regressors = [col for col in prophet_model.extra_regressors.keys()\n",
    "                               if col in prophet_valid.columns]\n",
    "\n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings(\"ignore\")\n",
    "                prophet_future = prophet_valid[['ds'] + required_regressors]\n",
    "\n",
    "                for col in required_regressors:\n",
    "                    if prophet_future[col].isna().any():\n",
    "                        prophet_future[col] = prophet_future[col].fillna(prophet_future[col].median())\n",
    "\n",
    "                prophet_forecast = prophet_model.predict(prophet_future)\n",
    "                prophet_preds = prophet_forecast['yhat'].values\n",
    "\n",
    "                firm_valid['prophet_pred'] = prophet_preds\n",
    "\n",
    "                weight_results = []\n",
    "\n",
    "                for lgb_weight in weight_grid:\n",
    "                    prophet_weight = 1 - lgb_weight\n",
    "\n",
    "                    ensemble_preds = (\n",
    "                            lgb_weight * firm_valid['lgb_pred'].values +\n",
    "                            prophet_weight * firm_valid['prophet_pred'].values\n",
    "                    )\n",
    "\n",
    "                    rmse = np.sqrt(mean_squared_error(firm_valid['true_sales'].values, ensemble_preds))\n",
    "                    r2 = calculate_r2(firm_valid['true_sales'].values, ensemble_preds)\n",
    "\n",
    "                    weight_results.append({\n",
    "                        'lgb_weight': lgb_weight,\n",
    "                        'prophet_weight': prophet_weight,\n",
    "                        'rmse': rmse,\n",
    "                        'r2': r2\n",
    "                    })\n",
    "\n",
    "                    print(\n",
    "                        f\"  LightGBM weight: {lgb_weight:.2f}, Prophet weight: {prophet_weight:.2f} - RMSE: {rmse:.4f}, R: {r2:.4f}\")\n",
    "\n",
    "                best_result = min(weight_results, key=lambda x: x['rmse'])\n",
    "\n",
    "                print(f\"  Best ensemble weights for Firm {firm_id}:\")\n",
    "                print(f\"    LightGBM: {best_result['lgb_weight']:.2f}, Prophet: {best_result['prophet_weight']:.2f}\")\n",
    "                print(f\"    RMSE: {best_result['rmse']:.4f}, R: {best_result['r2']:.4f}\")\n",
    "\n",
    "                ensemble_results[firm_id] = {\n",
    "                    'weights': {\n",
    "                        'lightgbm': best_result['lgb_weight'],\n",
    "                        'prophet': best_result['prophet_weight']\n",
    "                    },\n",
    "                    'rmse': best_result['rmse'],\n",
    "                    'r2': best_result['r2'],\n",
    "                    'all_weight_results': weight_results\n",
    "                }\n",
    "\n",
    "                firm_weights.append(best_result['lgb_weight'])\n",
    "                firm_names.append(f\"Firm {firm_id}\")\n",
    "                firm_rmses.append(best_result['rmse'])\n",
    "                firm_r2s.append(best_result['r2'])\n",
    "\n",
    "                weight_df = pd.DataFrame(weight_results)\n",
    "\n",
    "                firm_fig = make_subplots(rows=2, cols=1,\n",
    "                                         subplot_titles=[f\"Firm {firm_id} - Ensemble RMSE by Weight\",\n",
    "                                                         f\"Firm {firm_id} - Ensemble R by Weight\"],\n",
    "                                         vertical_spacing=0.1)\n",
    "\n",
    "                firm_fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=weight_df['lgb_weight'],\n",
    "                        y=weight_df['rmse'],\n",
    "                        mode='lines+markers',\n",
    "                        name=\"RMSE\",\n",
    "                        line=dict(color='red', width=2),\n",
    "                        marker=dict(color='red', size=8)\n",
    "                    ),\n",
    "                    row=1, col=1\n",
    "                )\n",
    "\n",
    "                firm_fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=weight_df['lgb_weight'],\n",
    "                        y=weight_df['r2'],\n",
    "                        mode='lines+markers',\n",
    "                        name=\"R\",\n",
    "                        line=dict(color='blue', width=2),\n",
    "                        marker=dict(color='blue', size=8)\n",
    "                    ),\n",
    "                    row=2, col=1\n",
    "                )\n",
    "\n",
    "                firm_fig.update_layout(\n",
    "                    height=600,\n",
    "                    title_text=f\"Firm {firm_id} - Ensemble Weight Tuning\",\n",
    "                    showlegend=True,\n",
    "                    xaxis_title=\"LightGBM Weight\",\n",
    "                    xaxis2_title=\"LightGBM Weight\",\n",
    "                    yaxis_title=\"RMSE\",\n",
    "                    yaxis2_title=\"R\"\n",
    "                )\n",
    "\n",
    "                firm_fig.write_html(f\"ensemble_firm_{firm_id}_tuning.html\")\n",
    "                print(f\"  Ensemble weight tuning visualization saved for Firm {firm_id}\")\n",
    "\n",
    "                comparison_fig = go.Figure()\n",
    "\n",
    "                firm_valid = firm_valid.sort_values('Date')\n",
    "\n",
    "                comparison_fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=firm_valid['Date'],\n",
    "                        y=firm_valid['true_sales'],\n",
    "                        mode='lines+markers',\n",
    "                        name='Actual Sales',\n",
    "                        line=dict(color='black', width=2)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                comparison_fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=firm_valid['Date'],\n",
    "                        y=firm_valid['lgb_pred'],\n",
    "                        mode='lines',\n",
    "                        name='LightGBM',\n",
    "                        line=dict(color='blue', width=1.5, dash='dash')\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                comparison_fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=firm_valid['Date'],\n",
    "                        y=firm_valid['prophet_pred'],\n",
    "                        mode='lines',\n",
    "                        name='Prophet',\n",
    "                        line=dict(color='green', width=1.5, dash='dash')\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                ensemble_preds = (\n",
    "                        best_result['lgb_weight'] * firm_valid['lgb_pred'].values +\n",
    "                        best_result['prophet_weight'] * firm_valid['prophet_pred'].values\n",
    "                )\n",
    "\n",
    "                comparison_fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=firm_valid['Date'],\n",
    "                        y=ensemble_preds,\n",
    "                        mode='lines',\n",
    "                        name='Ensemble',\n",
    "                        line=dict(color='red', width=2)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                comparison_fig.update_layout(\n",
    "                    title=f\"Firm {firm_id} - Model Predictions Comparison\",\n",
    "                    xaxis_title=\"Date\",\n",
    "                    yaxis_title=\"Sales\",\n",
    "                    template=\"plotly_white\",\n",
    "                    legend=dict(\n",
    "                        orientation=\"h\",\n",
    "                        yanchor=\"bottom\",\n",
    "                        y=1.02,\n",
    "                        xanchor=\"right\",\n",
    "                        x=1\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                comparison_fig.write_html(f\"ensemble_firm_{firm_id}_comparison.html\")\n",
    "                print(f\"  Model comparison visualization saved for Firm {firm_id}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error tuning ensemble for Firm {firm_id}: {e}\")\n",
    "\n",
    "    if firm_names:\n",
    "        fig = make_subplots(rows=3, cols=1,\n",
    "                            subplot_titles=[\"Best LightGBM Weight by Firm\",\n",
    "                                            \"Ensemble RMSE by Firm\",\n",
    "                                            \"Ensemble R by Firm\"],\n",
    "                            vertical_spacing=0.1)\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=firm_names,\n",
    "                y=firm_weights,\n",
    "                name=\"LightGBM Weight\",\n",
    "                marker_color='purple',\n",
    "                text=[f\"{w:.2f}\" for w in firm_weights],\n",
    "                textposition=\"outside\"\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=firm_names,\n",
    "                y=firm_rmses,\n",
    "                name=\"RMSE\",\n",
    "                marker_color='red',\n",
    "                text=[f\"{r:.4f}\" for r in firm_rmses],\n",
    "                textposition=\"outside\"\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=firm_names,\n",
    "                y=firm_r2s,\n",
    "                name=\"R\",\n",
    "                marker_color='blue',\n",
    "                text=[f\"{r:.4f}\" for r in firm_r2s],\n",
    "                textposition=\"outside\"\n",
    "            ),\n",
    "            row=3, col=1\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            height=900,\n",
    "            title_text=\"Ensemble Performance by Firm\",\n",
    "            template=\"plotly_white\",\n",
    "            showlegend=False\n",
    "        )\n",
    "\n",
    "        fig.write_html(\"ensemble_all_firms_comparison.html\")\n",
    "        print(\"\\nEnsemble comparison across firms visualization saved\")\n",
    "\n",
    "        summary_table = go.Figure(data=[go.Table(\n",
    "            header=dict(\n",
    "                values=[\"Firm ID\", \"LightGBM Weight\", \"Prophet Weight\", \"RMSE\", \"R\", \"Improvement over LightGBM\"],\n",
    "                fill_color='paleturquoise',\n",
    "                align='left'\n",
    "            ),\n",
    "            cells=dict(\n",
    "                values=[\n",
    "                    [name.replace(\"Firm \", \"\") for name in firm_names],\n",
    "                    [f\"{w:.2f}\" for w in firm_weights],\n",
    "                    [f\"{1 - w:.2f}\" for w in firm_weights],\n",
    "                    [f\"{r:.4f}\" for r in firm_rmses],\n",
    "                    [f\"{r:.4f}\" for r in firm_r2s],\n",
    "                    [\n",
    "                        # Convert to float first\n",
    "                        f\"{((prophet_firm_results[float(name.replace('Firm ', ''))]['best_rmse'] - r) / prophet_firm_results[float(name.replace('Firm ', ''))]['best_rmse'] * 100):.2f}%\"\n",
    "                        if ((prophet_firm_results[float(name.replace('Firm ', ''))]['best_rmse'] - r) > 0) else\n",
    "                        f\"{((prophet_firm_results[float(name.replace('Firm ', ''))]['best_rmse'] - r) / prophet_firm_results[float(name.replace('Firm ', ''))]['best_rmse'] * 100):.2f}%\"\n",
    "                        for name, r in zip(firm_names, firm_rmses)\n",
    "                    ]\n",
    "                ],\n",
    "                fill_color='lavender',\n",
    "                align='left'\n",
    "            )\n",
    "        )])\n",
    "\n",
    "        summary_table.update_layout(title=\"Ensemble Weights and Performance by Firm\")\n",
    "        summary_table.write_html(\"ensemble_weights_by_firm.html\")\n",
    "        print(\"Ensemble weights by firm summary saved\")\n",
    "\n",
    "    all_lgb_weights = [ensemble_results[firm_id]['weights']['lightgbm'] for firm_id in ensemble_results]\n",
    "    all_prophet_weights = [ensemble_results[firm_id]['weights']['prophet'] for firm_id in ensemble_results]\n",
    "\n",
    "    if all_lgb_weights:\n",
    "        global_lgb_weight = np.mean(all_lgb_weights)\n",
    "        global_prophet_weight = np.mean(all_prophet_weights)\n",
    "    else:\n",
    "        global_lgb_weight = 0.5\n",
    "        global_prophet_weight = 0.5\n",
    "\n",
    "    print(f\"\\nGlobal ensemble weights (for firms without specific models):\")\n",
    "    print(f\"  LightGBM: {global_lgb_weight:.2f}, Prophet: {global_prophet_weight:.2f}\")\n",
    "\n",
    "    ensemble_results['global'] = {\n",
    "        'weights': {\n",
    "            'lightgbm': global_lgb_weight,\n",
    "            'prophet': global_prophet_weight\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        if len(ensemble_results) > 3:\n",
    "            print(\"\\nAnalyzing factors that influence optimal ensemble weights...\")\n",
    "\n",
    "            firm_data = []\n",
    "            for firm_id in ensemble_results.keys():\n",
    "                if firm_id != 'global':\n",
    "\n",
    "                    firm_count = len(df_train[df_train['FirmID'] == firm_id])\n",
    "                    firm_traffic_mean = df_train[df_train['FirmID'] == firm_id]['FirmDailyTraffic'].mean()\n",
    "                    firm_traffic_std = df_train[df_train['FirmID'] == firm_id]['FirmDailyTraffic'].std()\n",
    "                    firm_sales_mean = df_train[df_train['FirmID'] == firm_id]['Sales'].mean()\n",
    "                    firm_sales_std = df_train[df_train['FirmID'] == firm_id]['Sales'].std()\n",
    "\n",
    "                    firm_data.append({\n",
    "                        'firm_id': firm_id,\n",
    "                        'lgb_weight': ensemble_results[firm_id]['weights']['lightgbm'],\n",
    "                        'data_points': firm_count,\n",
    "                        'traffic_mean': firm_traffic_mean,\n",
    "                        'traffic_std': firm_traffic_std,\n",
    "                        'sales_mean': firm_sales_mean,\n",
    "                        'sales_std': firm_sales_std,\n",
    "                        'traffic_variability': firm_traffic_std / firm_traffic_mean if firm_traffic_mean > 0 else 0,\n",
    "                        'sales_variability': firm_sales_std / firm_sales_mean if firm_sales_mean > 0 else 0\n",
    "                    })\n",
    "\n",
    "            if firm_data:\n",
    "                firm_df = pd.DataFrame(firm_data)\n",
    "\n",
    "                correlations = firm_df.corr()['lgb_weight'].drop('lgb_weight')\n",
    "\n",
    "                corr_fig = go.Figure()\n",
    "\n",
    "                corr_fig.add_trace(\n",
    "                    go.Bar(\n",
    "                        x=correlations.index,\n",
    "                        y=correlations.values,\n",
    "                        marker_color=['blue' if v >= 0 else 'red' for v in correlations.values]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                corr_fig.update_layout(\n",
    "                    title=\"Factors Correlated with Optimal LightGBM Weight\",\n",
    "                    xaxis_title=\"Factor\",\n",
    "                    yaxis_title=\"Correlation Coefficient\",\n",
    "                    template=\"plotly_white\"\n",
    "                )\n",
    "\n",
    "                corr_fig.write_html(\"weight_correlations.html\")\n",
    "                print(\"Weight correlation analysis saved\")\n",
    "\n",
    "                print(\"\\nInsights about optimal ensemble weights:\")\n",
    "                for factor, corr in correlations.items():\n",
    "                    direction = \"higher\" if corr > 0 else \"lower\"\n",
    "                    strength = \"strong\" if abs(corr) > 0.7 else \"moderate\" if abs(corr) > 0.3 else \"weak\"\n",
    "                    if abs(corr) > 0.2:  # Only report meaningful correlations\n",
    "                        print(f\"  {factor}: {strength} {direction} correlation ({corr:.2f}) with LightGBM weight\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing weight correlations: {e}\")\n",
    "\n",
    "    return ensemble_results\n",
    "\n",
    "print(\"\\nBeginning focused multi-firm model tuning process...\")\n",
    "print_memory_usage()\n",
    "\n",
    "print(f\"Creating stratified firm sample for LightGBM tuning...\")\n",
    "sample_idx = stratified_firm_sample(X_train, y_train, max_total=10000)\n",
    "X_train_sample = X_train.iloc[sample_idx]\n",
    "y_train_sample = y_train.iloc[sample_idx]\n",
    "\n",
    "print(f\"Stratified sample for LightGBM tuning: {len(X_train_sample)} rows\")\n",
    "print(f\"Number of firms in sample: {X_train_sample['FirmID'].nunique()} (out of {X_train['FirmID'].nunique()} total)\")\n",
    "\n",
    "lgb_best_params, lgb_best_rmse, lgb_best_r2, lgb_model = tune_lightgbm_optuna(\n",
    "    X_train_sample, y_train_sample, X_valid, y_valid, tscv, n_trials=50\n",
    ")\n",
    "print_memory_usage()\n",
    "\n",
    "top_firms = df_train['FirmID'].value_counts().iloc[:10].index.tolist()\n",
    "prophet_firm_results = tune_prophet_for_firms(\n",
    "    df_train, top_firms=top_firms, param_combinations=15, max_firms=10\n",
    ")\n",
    "print_memory_usage()\n",
    "\n",
    "ensemble_weights = build_firm_specific_ensemble(\n",
    "    X_train, y_train, X_valid, y_valid, lgb_model, prophet_firm_results\n",
    ")\n",
    "print_memory_usage()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SUMMARY OF MULTI-FIRM MODEL TUNING RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"LightGBM global model:\")\n",
    "print(f\"  Best parameters: {lgb_best_params}\")\n",
    "print(f\"  Validation RMSE: {lgb_best_rmse:.4f}\")\n",
    "print(f\"  Validation R: {lgb_best_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nProphet models:\")\n",
    "print(f\"  Number of firm-specific models: {len(prophet_firm_results)}\")\n",
    "firm_prophet_rmses = []\n",
    "\n",
    "for firm_id, results in prophet_firm_results.items():\n",
    "    print(f\"  Firm {firm_id}:\")\n",
    "    print(f\"    Best parameters: {results['best_params']}\")\n",
    "    print(f\"    Validation RMSE: {results['best_rmse']:.4f}\")\n",
    "    print(f\"    Validation R: {results['best_r2']:.4f}\")\n",
    "    firm_prophet_rmses.append(results['best_rmse'])\n",
    "\n",
    "print(f\"\\nEnsemble weights:\")\n",
    "print(f\"  Global weights: LightGBM={ensemble_weights['global']['weights']['lightgbm']:.2f}, \"\n",
    "      f\"Prophet={ensemble_weights['global']['weights']['prophet']:.2f}\")\n",
    "\n",
    "firm_ensemble_rmses = []\n",
    "for firm_id, results in ensemble_weights.items():\n",
    "    if firm_id != 'global':\n",
    "        print(f\"  Firm {firm_id}: LightGBM={results['weights']['lightgbm']:.2f}, \"\n",
    "              f\"Prophet={results['weights']['prophet']:.2f}, RMSE={results.get('rmse', 'N/A')}\")\n",
    "        if 'rmse' in results:\n",
    "            firm_ensemble_rmses.append(results['rmse'])\n",
    "\n",
    "if firm_prophet_rmses and firm_ensemble_rmses:\n",
    "    avg_prophet_rmse = np.mean(firm_prophet_rmses)\n",
    "    avg_ensemble_rmse = np.mean(firm_ensemble_rmses)\n",
    "    improvement = (avg_prophet_rmse - avg_ensemble_rmse) / avg_prophet_rmse * 100\n",
    "\n",
    "    print(f\"\\nOverall ensemble improvement:\")\n",
    "    print(f\"  Average Prophet-only RMSE: {avg_prophet_rmse:.4f}\")\n",
    "    print(f\"  Average Ensemble RMSE: {avg_ensemble_rmse:.4f}\")\n",
    "    print(f\"  Average improvement: {improvement:.2f}%\")\n",
    "\n",
    "try:\n",
    "    model_comparison = go.Figure()\n",
    "\n",
    "    firm_names = [f\"Firm {firm_id}\" for firm_id in prophet_firm_results.keys() if firm_id in ensemble_weights]\n",
    "    lgb_rmses = [lgb_best_rmse] * len(firm_names)\n",
    "    prophet_rmses = [prophet_firm_results[float(name.replace('Firm ', ''))]['best_rmse'] for name in firm_names]\n",
    "    ensemble_rmses = [ensemble_weights[float(name.replace('Firm ', ''))]['rmse'] for name in firm_names]\n",
    "\n",
    "    model_comparison.add_trace(\n",
    "        go.Bar(\n",
    "            x=firm_names,\n",
    "            y=lgb_rmses,\n",
    "            name='LightGBM',\n",
    "            marker_color='blue',\n",
    "            text=[f\"{r:.4f}\" for r in lgb_rmses],\n",
    "            textposition=\"outside\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model_comparison.add_trace(\n",
    "        go.Bar(\n",
    "            x=firm_names,\n",
    "            y=prophet_rmses,\n",
    "            name='Prophet',\n",
    "            marker_color='green',\n",
    "            text=[f\"{r:.4f}\" for r in prophet_rmses],\n",
    "            textposition=\"outside\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model_comparison.add_trace(\n",
    "        go.Bar(\n",
    "            x=firm_names,\n",
    "            y=ensemble_rmses,\n",
    "            name='Ensemble',\n",
    "            marker_color='red',\n",
    "            text=[f\"{r:.4f}\" for r in ensemble_rmses],\n",
    "            textposition=\"outside\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model_comparison.update_layout(\n",
    "        title=\"RMSE Comparison Across All Models by Firm\",\n",
    "        xaxis_title=\"Firm\",\n",
    "        yaxis_title=\"RMSE (lower is better)\",\n",
    "        template=\"plotly_white\",\n",
    "        barmode='group',\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model_comparison.write_html(\"model_rmse_comparison.html\")\n",
    "    print(\"\\nModel RMSE comparison visualization saved\")\n",
    "\n",
    "    all_tuning_results = {\n",
    "        'lightgbm': {\n",
    "            'best_params': lgb_best_params,\n",
    "            'validation_rmse': lgb_best_rmse,\n",
    "            'validation_r2': lgb_best_r2\n",
    "        },\n",
    "        'prophet': {\n",
    "            firm_id: {\n",
    "                'best_params': results['best_params'],\n",
    "                'validation_rmse': results['best_rmse'],\n",
    "                'validation_r2': results['best_r2']\n",
    "            } for firm_id, results in prophet_firm_results.items()\n",
    "        },\n",
    "        'ensemble': {\n",
    "            firm_id: {\n",
    "                'weights': results['weights'],\n",
    "                'validation_rmse': results.get('rmse', 'N/A'),\n",
    "                'validation_r2': results.get('r2', 'N/A')\n",
    "            } for firm_id, results in ensemble_weights.items()\n",
    "        },\n",
    "        'overall_improvement': {\n",
    "            'avg_prophet_rmse': avg_prophet_rmse if 'avg_prophet_rmse' in locals() else None,\n",
    "            'avg_ensemble_rmse': avg_ensemble_rmse if 'avg_ensemble_rmse' in locals() else None,\n",
    "            'improvement_percentage': improvement if 'improvement' in locals() else None\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open('multi_firm_tuning_results.json', 'w') as f:\n",
    "        json.dump(all_tuning_results, f, indent=4, default=str)\n",
    "\n",
    "    print(\"All tuning results saved to multi_firm_tuning_results.json\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating final visualizations: {e}\")\n",
    "    print(\"Full traceback:\")\n",
    "    traceback.print_exc()\n",
    "    \n",
    "\n",
    "print(\"\\nMulti-firm model tuning process completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c26c778ecdff090",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Advanced Models with Tuned Hyperparameters and Forecast Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e59c179573fd8eb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing forecast dataframe...\n",
      "Cutoff date for forecasting: 2021-12-03 00:00:00\n",
      "\n",
      "Training LGBM model with simplified features...\n",
      "Current memory usage: 308.57 MB\n",
      "\n",
      "Implementing rolling forecast for LightGBM...\n",
      "Processing firm 0.0 with rolling forecast...\n",
      "Processing firm 1.0 with rolling forecast...\n",
      "Processing firm 2.0 with rolling forecast...\n",
      "Processing firm 3.0 with rolling forecast...\n",
      "Processing firm 4.0 with rolling forecast...\n",
      "Processing firm 5.0 with rolling forecast...\n",
      "Processing firm 6.0 with rolling forecast...\n",
      "Processing firm 7.0 with rolling forecast...\n",
      "Processing firm 8.0 with rolling forecast...\n",
      "Processing firm 9.0 with rolling forecast...\n",
      "Processing firm 10.0 with rolling forecast...\n",
      "Processing firm 11.0 with rolling forecast...\n",
      "Processing firm 12.0 with rolling forecast...\n",
      "Processing firm 13.0 with rolling forecast...\n",
      "Processing firm 14.0 with rolling forecast...\n",
      "Processing firm 15.0 with rolling forecast...\n",
      "Processing firm 16.0 with rolling forecast...\n",
      "Processing firm 17.0 with rolling forecast...\n",
      "Processing firm 18.0 with rolling forecast...\n",
      "Processing firm 19.0 with rolling forecast...\n",
      "Processing firm 20.0 with rolling forecast...\n",
      "Processing firm 21.0 with rolling forecast...\n",
      "Processing firm 22.0 with rolling forecast...\n",
      "Processing firm 23.0 with rolling forecast...\n",
      "Processing firm 24.0 with rolling forecast...\n",
      "Processing firm 25.0 with rolling forecast...\n",
      "Processing firm 26.0 with rolling forecast...\n",
      "Processing firm 27.0 with rolling forecast...\n",
      "Processing firm 28.0 with rolling forecast...\n",
      "Processing firm 29.0 with rolling forecast...\n",
      "Processing firm 30.0 with rolling forecast...\n",
      "Processing firm 31.0 with rolling forecast...\n",
      "Processing firm 32.0 with rolling forecast...\n",
      "Processing firm 33.0 with rolling forecast...\n",
      "Processing firm 34.0 with rolling forecast...\n",
      "Processing firm 35.0 with rolling forecast...\n",
      "Processing firm 36.0 with rolling forecast...\n",
      "Processing firm 37.0 with rolling forecast...\n",
      "Processing firm 38.0 with rolling forecast...\n",
      "Processing firm 39.0 with rolling forecast...\n",
      "Processing firm 40.0 with rolling forecast...\n",
      "Processing firm 41.0 with rolling forecast...\n",
      "Processing firm 42.0 with rolling forecast...\n",
      "Processing firm 43.0 with rolling forecast...\n",
      "Processing firm 44.0 with rolling forecast...\n",
      "Processing firm 45.0 with rolling forecast...\n",
      "Processing firm 46.0 with rolling forecast...\n",
      "Processing firm 47.0 with rolling forecast...\n",
      "Processing firm 48.0 with rolling forecast...\n",
      "Processing firm 49.0 with rolling forecast...\n",
      "LightGBM cross-validation - Mean MAE: 490.9029, Mean RMSE: 707.2112, Mean R: 0.5012\n",
      "Top 5 important features for LightGBM:\n",
      "  FirmID: 430.0000\n",
      "  FirmDailyTraffic: 391.0000\n",
      "  Sales_lag7: 322.0000\n",
      "  Sales_lag1: 274.0000\n",
      "  DayOfWeek: 135.0000\n",
      "LightGBM feature importance visualization saved to 'lightgbm_feature_importance.html'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:04 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM actual vs predicted plot saved\n",
      "Current memory usage: 315.40 MB\n",
      "\n",
      "Training Prophet models for each firm with simplified regressors...\n",
      "\n",
      "Training Prophet model for Firm 0.0...\n",
      "  Using firm-specific Prophet parameters: {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:04 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 0.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 1.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:04 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 1.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 2.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:05 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 2.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 3.0...\n",
      "  Using firm-specific Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:05 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 3.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 4.0...\n",
      "  Using firm-specific Prophet parameters: {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:05 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 4.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 5.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:05 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:05 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 5.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 6.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:06 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 6.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 7.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:06 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 7.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 8.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:06 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 8.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 9.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:07 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 9.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 10.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:07 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 10.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 11.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:07 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 11.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 12.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:07 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 12.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 13.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:08 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 13.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 14.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:08 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 14.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 15.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:08 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 15.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 16.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:08 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 16.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 17.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:53:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:09 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 17.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 18.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:09 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 18.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 19.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:10 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 19.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 20.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:10 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 20.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 21.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:10 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 21.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 22.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:11 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 22.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 23.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:11 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 23.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 24.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:11 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 24.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 25.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:11 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 25.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 26.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:12 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 26.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 27.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:12 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 27.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 28.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:12 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 28.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 29.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:13 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 29.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 30.0...\n",
      "  Using firm-specific Prophet parameters: {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:13 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 30.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 31.0...\n",
      "  Using firm-specific Prophet parameters: {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:13 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 31.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 32.0...\n",
      "  Using firm-specific Prophet parameters: {'changepoint_prior_scale': 0.1, 'seasonality_prior_scale': 1.0, 'holidays_prior_scale': 0.1, 'seasonality_mode': 'additive', 'changepoint_range': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:13 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 32.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 33.0...\n",
      "  Using firm-specific Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:14 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 33.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 34.0...\n",
      "  Using firm-specific Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 0.1, 'holidays_prior_scale': 0.01, 'seasonality_mode': 'additive', 'changepoint_range': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:14 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 34.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 35.0...\n",
      "  Using firm-specific Prophet parameters: {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 1.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:14 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 35.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 36.0...\n",
      "  Using firm-specific Prophet parameters: {'changepoint_prior_scale': 0.001, 'seasonality_prior_scale': 0.01, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:15 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 36.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 37.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:15 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 37.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 38.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:15 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 38.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 39.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:15 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 39.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 40.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:15 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:16 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 40.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 41.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:16 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 41.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 42.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:16 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 42.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 43.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:17 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 43.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 44.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:17 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 44.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 45.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:17 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 45.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 46.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:17 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 46.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 47.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:18 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 47.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 48.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "11:53:18 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 48.0 successfully trained\n",
      "\n",
      "Training Prophet model for Firm 49.0...\n",
      "  Using default Prophet parameters: {'changepoint_prior_scale': 0.05, 'seasonality_prior_scale': 10.0, 'holidays_prior_scale': 10.0, 'seasonality_mode': 'additive', 'changepoint_range': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:53:18 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Prophet model for Firm 49.0 successfully trained\n",
      "Prophet components visualization saved to 'prophet_components.html'\n",
      "\n",
      "Combining Prophet forecasts...\n",
      "Prophet models trained for all firms with simplified regressors\n",
      "Current memory usage: 316.16 MB\n",
      "\n",
      "Integrating OLS model predictions...\n",
      "Using firm-specific OLS models\n",
      "OLS predictions available for 100.0% of forecast rows\n",
      "Current memory usage: 316.16 MB\n",
      "\n",
      "Creating ensemble forecast...\n",
      "Models available for ensemble: ['Sales_pred_ols', 'Sales_pred_lgbm', 'Sales_pred_prophet']\n",
      "Using weights from hyperparameter tuning for ensemble\n",
      "Ensemble predictions available for 100.0% of forecast rows\n",
      "Current memory usage: 316.16 MB\n",
      "\n",
      "Comparing model performance...\n",
      "No actual Sales data in forecast period for evaluation.\n",
      "Current memory usage: 316.16 MB\n",
      "\n",
      "Creating forecast visualizations...\n",
      "Sales forecast visualization saved for firm 43.0\n",
      "Sales forecast visualization saved for firm 46.0\n",
      "Sales forecast visualization saved for firm 37.0\n",
      "Sales forecast visualization saved for firm 17.0\n",
      "Sales forecast visualization saved for firm 15.0\n",
      "Current memory usage: 316.16 MB\n",
      "\n",
      "Creating combined forecast dashboard...\n",
      "Combined sales forecast dashboard saved to 'sales_forecast_dashboard.html'\n",
      "Current memory usage: 317.06 MB\n",
      "\n",
      "Preparing final forecast output for submission...\n",
      "Final model (Sales_pred_ensemble) has predictions for 100.0% of forecast rows\n",
      "Final forecasts saved to 'Sales_with_forecasts.csv'\n",
      "Forecast-only data saved to 'Forecasts_only.csv'\n",
      "\n",
      "All model training and visualization complete!\n",
      "Current memory usage: 320.87 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPreparing forecast dataframe...\")\n",
    "df_forecast = df_test.copy()\n",
    "\n",
    "# Use the simplified feature set to match the training approach\n",
    "volatility_features = [\n",
    "    'FirmID', 'FirmDailyTraffic',  # Core features\n",
    "    'DayOfWeek', 'Month', 'IsWeekend',  # Time components\n",
    "    'Sales_lag1', 'Sales_lag7'  # Minimal lag features\n",
    "]\n",
    "X_forecast = df_forecast[volatility_features]\n",
    "\n",
    "cutoff_date = df_train['Date'].max()\n",
    "print(f\"Cutoff date for forecasting: {cutoff_date}\")\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# 9.1. LightGBM Model with Simplified Features\n",
    "#-----------------------------------------------------------\n",
    "print(\"\\nTraining LGBM model with simplified features...\")\n",
    "print_memory_usage()\n",
    "\n",
    "try:\n",
    "    # Prepare categorical features\n",
    "    categorical_columns = ['FirmID', 'DayOfWeek', 'Month', 'IsWeekend']\n",
    "    for col in categorical_columns:\n",
    "        if col in X_train.columns:\n",
    "            X_train[col] = X_train[col].astype(int)\n",
    "            X_forecast[col] = X_forecast[col].astype(int)\n",
    "    \n",
    "    categorical_features = [X_train.columns.get_loc(col) for col in categorical_columns \n",
    "                          if col in X_train.columns]\n",
    "    \n",
    "    lgbm_model = lgb.LGBMRegressor(**lgb_best_params, \n",
    "                                   random_state=42, \n",
    "                                   categorical_feature=categorical_features)\n",
    "    lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "    #-----------------------------------------------------------\n",
    "    # Implement rolling forecast approach for LightGBM\n",
    "    #-----------------------------------------------------------\n",
    "    print(\"\\nImplementing rolling forecast for LightGBM...\")\n",
    "    for firm_id in df_forecast['FirmID'].unique():\n",
    "        print(f\"Processing firm {firm_id} with rolling forecast...\")\n",
    "        \n",
    "        # Get firm-specific data sorted by date\n",
    "        firm_forecast = df_forecast[df_forecast['FirmID'] == firm_id].sort_values('Date')\n",
    "        \n",
    "        if len(firm_forecast) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Get latest values from training data\n",
    "        firm_train = df_train[df_train['FirmID'] == firm_id].sort_values('Date')\n",
    "        \n",
    "        if len(firm_train) > 0:\n",
    "            # Initialize lag values from training data\n",
    "            last_sales = firm_train['Sales'].iloc[-1] if len(firm_train) > 0 else 0\n",
    "            last_sales_7 = firm_train['Sales'].iloc[-7] if len(firm_train) >= 7 else last_sales\n",
    "        else:\n",
    "            last_sales = 0\n",
    "            last_sales_7 = 0\n",
    "            \n",
    "        # Process each date in sequence to update lag features\n",
    "        for i, (idx, row) in enumerate(firm_forecast.iterrows()):\n",
    "            # Update lag1 feature with the previous prediction or training value\n",
    "            if i == 0:  # First forecast day\n",
    "                df_forecast.loc[idx, 'Sales_lag1'] = last_sales\n",
    "                df_forecast.loc[idx, 'Sales_lag7'] = last_sales_7\n",
    "            else:\n",
    "                # Use previous day's prediction as lag1\n",
    "                prev_idx = firm_forecast.index[i-1]\n",
    "                df_forecast.loc[idx, 'Sales_lag1'] = df_forecast.loc[prev_idx, 'Sales_pred_lgbm']\n",
    "                \n",
    "                # Update lag7 feature if available\n",
    "                if i >= 7:\n",
    "                    week_ago_idx = firm_forecast.index[i-7]\n",
    "                    df_forecast.loc[idx, 'Sales_lag7'] = df_forecast.loc[week_ago_idx, 'Sales_pred_lgbm']\n",
    "                elif len(firm_train) > 0:\n",
    "                    # Use training data for the lag7 if available\n",
    "                    day_diff = (row['Date'] - firm_train['Date'].iloc[-1]).days\n",
    "                    if day_diff <= 7:\n",
    "                        lag7_idx = day_diff - 7\n",
    "                        if abs(lag7_idx) < len(firm_train):\n",
    "                            df_forecast.loc[idx, 'Sales_lag7'] = firm_train['Sales'].iloc[lag7_idx]\n",
    "            \n",
    "            # Make prediction for current row with updated features\n",
    "            X_row = df_forecast.loc[idx:idx, volatility_features]\n",
    "            df_forecast.loc[idx, 'Sales_pred_lgbm'] = lgbm_model.predict(X_row)[0]\n",
    "            \n",
    "            # Store prediction for use in next iteration\n",
    "            last_sales = df_forecast.loc[idx, 'Sales_pred_lgbm']\n",
    "\n",
    "    lgbm_cv_scores = []\n",
    "    for train_idx, val_idx in tscv.split(X_train):\n",
    "        X_train_cv, X_val_cv = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_train_cv, y_val_cv = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        lgbm_cv = lgb.LGBMRegressor(**lgb_best_params, \n",
    "                                    random_state=42, \n",
    "                                    categorical_feature=categorical_features)\n",
    "        lgbm_cv.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "        y_pred_cv = lgbm_cv.predict(X_val_cv)\n",
    "        mae = mean_absolute_error(y_val_cv, y_pred_cv)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val_cv, y_pred_cv))\n",
    "        r2 = r2_score(y_val_cv, y_pred_cv)\n",
    "        lgbm_cv_scores.append((mae, rmse, r2))\n",
    "\n",
    "    lgbm_mean_mae = np.mean([score[0] for score in lgbm_cv_scores])\n",
    "    lgbm_mean_rmse = np.mean([score[1] for score in lgbm_cv_scores])\n",
    "    lgbm_mean_r2 = np.mean([score[2] for score in lgbm_cv_scores])\n",
    "    print(f\"LightGBM cross-validation - Mean MAE: {lgbm_mean_mae:.4f}, Mean RMSE: {lgbm_mean_rmse:.4f}, Mean R: {lgbm_mean_r2:.4f}\")\n",
    "\n",
    "    model_results['LightGBM'] = {\n",
    "        'cv_mae': lgbm_mean_mae,\n",
    "        'cv_rmse': lgbm_mean_rmse,\n",
    "        'cv_r2': lgbm_mean_r2,\n",
    "        'feature_importance': dict(zip(volatility_features, lgbm_model.feature_importances_))\n",
    "    }\n",
    "\n",
    "    importances = model_results['LightGBM']['feature_importance']\n",
    "    top_features = sorted(importances.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    print(\"Top 5 important features for LightGBM:\")\n",
    "    for feature, importance in top_features:\n",
    "        print(f\"  {feature}: {importance:.4f}\")\n",
    "\n",
    "    try:\n",
    "        sorted_features = sorted(importances.items(), key=lambda x: x[1], reverse=True)\n",
    "        features_names = [item[0] for item in sorted_features]\n",
    "        feature_values = [item[1] for item in sorted_features]\n",
    "\n",
    "        fig = px.bar(\n",
    "            x=feature_values,\n",
    "            y=features_names,\n",
    "            orientation='h',\n",
    "            title='LightGBM Feature Importance',\n",
    "            labels={'x': 'Importance', 'y': 'Feature'},\n",
    "            color=feature_values,\n",
    "            color_continuous_scale='Viridis'\n",
    "        )\n",
    "\n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            width=1000,\n",
    "            template='plotly_white',\n",
    "            yaxis={'categoryorder': 'total ascending'}\n",
    "        )\n",
    "\n",
    "        fig.write_html('lightgbm_feature_importance.html')\n",
    "        print(\"LightGBM feature importance visualization saved to 'lightgbm_feature_importance.html'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating feature importance visualization: {e}\")\n",
    "\n",
    "    try:\n",
    "        sample_size = min(1000, len(X_train))\n",
    "        sample_idx = np.random.choice(len(X_train), sample_size, replace=False)\n",
    "        X_sample = X_train.iloc[sample_idx]\n",
    "        y_sample = y_train.iloc[sample_idx]\n",
    "        y_pred_sample = lgbm_model.predict(X_sample)\n",
    "        \n",
    "        fig = px.scatter(\n",
    "            x=y_sample,\n",
    "            y=y_pred_sample,\n",
    "            opacity=0.6,\n",
    "            title=f'LightGBM: Actual vs Predicted Sales (R = {lgbm_mean_r2:.4f})',\n",
    "            labels={'x': 'Actual Sales', 'y': 'Predicted Sales'}\n",
    "        )\n",
    "\n",
    "        max_val = max(max(y_sample), max(y_pred_sample))\n",
    "        min_val = min(min(y_sample), min(y_pred_sample))\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[min_val, max_val],\n",
    "                y=[min_val, max_val],\n",
    "                mode='lines',\n",
    "                line=dict(color='red', dash='dash'),\n",
    "                name='Perfect Prediction'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(template='plotly_white')\n",
    "        fig.write_html('lightgbm_actual_vs_predicted.html')\n",
    "        print(\"LightGBM actual vs predicted plot saved\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating LightGBM actual vs predicted plot: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in LightGBM model training: {e}\")\n",
    "\n",
    "    df_forecast['Sales_pred_lgbm'] = np.nan\n",
    "    lgbm_mean_r2 = 0\n",
    "    \n",
    "print_memory_usage()\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# 9.2. Prophet Model with Simplified Regressors\n",
    "#-----------------------------------------------------------\n",
    "print(\"\\nTraining Prophet models for each firm with simplified regressors...\")\n",
    "\n",
    "prophet_forecasts = []\n",
    "prophet_components = []\n",
    "\n",
    "# Simplified regressors list\n",
    "prophet_regressors = [\n",
    "    'FirmDailyTraffic',\n",
    "    'TrafficPctChange'\n",
    "]\n",
    "\n",
    "for firm_id in df_model['FirmID'].unique():\n",
    "    try:\n",
    "        print(f\"\\nTraining Prophet model for Firm {firm_id}...\")\n",
    "        firm_train = df_train[df_train['FirmID'] == firm_id].copy()\n",
    "        firm_forecast = df_forecast[df_forecast['FirmID'] == firm_id].copy()\n",
    "\n",
    "        if len(firm_train) < 10:\n",
    "            print(f\"  Skipping Firm {firm_id}: insufficient data ({len(firm_train)} rows)\")\n",
    "            continue\n",
    "\n",
    "        prophet_train = firm_train[['Date', 'Sales']].rename(columns={'Date': 'ds', 'Sales': 'y'})\n",
    "\n",
    "        for regressor in prophet_regressors:\n",
    "            if regressor in firm_train.columns:\n",
    "                prophet_train[regressor] = firm_train[regressor].fillna(firm_train[regressor].median())\n",
    "\n",
    "        if firm_id in prophet_firm_results:\n",
    "            firm_params = prophet_firm_results[firm_id]['best_params']\n",
    "            print(f\"  Using firm-specific Prophet parameters: {firm_params}\")\n",
    "        else:\n",
    "            firm_params = {\n",
    "                'changepoint_prior_scale': 0.05,\n",
    "                'seasonality_prior_scale': 10.0, \n",
    "                'holidays_prior_scale': 10.0,\n",
    "                'seasonality_mode': 'additive',\n",
    "                'changepoint_range': 0.8\n",
    "            }\n",
    "            print(f\"  Using default Prophet parameters: {firm_params}\")\n",
    "\n",
    "        prophet_model = Prophet(\n",
    "            changepoint_prior_scale=firm_params['changepoint_prior_scale'],\n",
    "            seasonality_prior_scale=firm_params['seasonality_prior_scale'],\n",
    "            holidays_prior_scale=firm_params['holidays_prior_scale'],\n",
    "            seasonality_mode=firm_params['seasonality_mode'],\n",
    "            changepoint_range=firm_params['changepoint_range'],\n",
    "            yearly_seasonality=True,\n",
    "            weekly_seasonality=True,\n",
    "            daily_seasonality=False\n",
    "        )\n",
    "\n",
    "        for regressor in prophet_regressors:\n",
    "            if regressor in prophet_train.columns:\n",
    "                prophet_model.add_regressor(regressor)\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            prophet_model.fit(prophet_train)\n",
    "\n",
    "        # Create future dataframe for predictions\n",
    "        future = firm_forecast[['Date']].rename(columns={'Date': 'ds'})\n",
    "\n",
    "        for regressor in prophet_regressors:\n",
    "            if regressor in firm_forecast.columns:\n",
    "                future[regressor] = firm_forecast[regressor].values\n",
    "\n",
    "        forecast = prophet_model.predict(future)\n",
    "\n",
    "        firm_forecast['Sales_pred_prophet'] = forecast['yhat'].values\n",
    "        prophet_forecasts.append(firm_forecast)\n",
    "\n",
    "        if len(prophet_components) < 3: \n",
    "            components_df = pd.DataFrame({\n",
    "                'Date': forecast['ds'],\n",
    "                'Trend': forecast['trend'],\n",
    "                'Weekly': forecast['weekly'],\n",
    "                'Yearly': forecast['yearly'] if 'yearly' in forecast.columns else 0,\n",
    "                'FirmID': firm_id\n",
    "            })\n",
    "\n",
    "            for regressor in prophet_regressors:\n",
    "                regressor_effect = f'{regressor}_effect'\n",
    "                if regressor_effect in forecast.columns:\n",
    "                    components_df[f'{regressor}_Effect'] = forecast[regressor_effect]\n",
    "            \n",
    "            prophet_components.append(components_df)\n",
    "            \n",
    "        print(f\"  Prophet model for Firm {firm_id} successfully trained\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error training Prophet model for Firm {firm_id}: {e}\")\n",
    "        if 'firm_forecast' in locals():\n",
    "            firm_forecast['Sales_pred_prophet'] = np.nan\n",
    "            prophet_forecasts.append(firm_forecast)\n",
    "\n",
    "if prophet_components:\n",
    "    try:\n",
    "        from plotly.subplots import make_subplots\n",
    "        import plotly.graph_objects as go\n",
    "        \n",
    "        components_df = pd.concat(prophet_components)\n",
    "\n",
    "        component_cols = [col for col in components_df.columns if col not in ['Date', 'FirmID']]\n",
    "\n",
    "        fig = make_subplots(\n",
    "            rows=len(component_cols), cols=1,\n",
    "            subplot_titles=component_cols,\n",
    "            vertical_spacing=0.1,\n",
    "            shared_xaxes=True\n",
    "        )\n",
    "        \n",
    "        for i, component in enumerate(component_cols):\n",
    "            for firm_id in components_df['FirmID'].unique():\n",
    "                firm_data = components_df[components_df['FirmID'] == firm_id]\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=firm_data['Date'],\n",
    "                        y=firm_data[component],\n",
    "                        mode='lines',\n",
    "                        name=f'Firm {firm_id} - {component}',\n",
    "                        line=dict(width=2)\n",
    "                    ),\n",
    "                    row=i+1, col=1\n",
    "                )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            height=300 * len(component_cols),\n",
    "            width=1000,\n",
    "            title_text=\"Prophet Model Components\",\n",
    "            template='plotly_white',\n",
    "            showlegend=True,\n",
    "            legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
    "        )\n",
    "        \n",
    "        fig.write_html('prophet_components.html')\n",
    "        print(f\"Prophet components visualization saved to 'prophet_components.html'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating Prophet components visualization: {e}\")\n",
    "\n",
    "print(\"\\nCombining Prophet forecasts...\")\n",
    "try:\n",
    "    df_prophet_forecast = pd.concat(prophet_forecasts)\n",
    "    df_forecast = df_forecast.merge(\n",
    "        df_prophet_forecast[['FirmID', 'Date', 'Sales_pred_prophet']],\n",
    "        on=['FirmID', 'Date'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    missing_prophet_firms = df_forecast[df_forecast['Sales_pred_prophet'].isna()]['FirmID'].unique()\n",
    "    if len(missing_prophet_firms) > 0:\n",
    "        print(f\"Warning: {len(missing_prophet_firms)} firms have no Prophet predictions.\")\n",
    "        print(f\"First few firms without Prophet predictions: {missing_prophet_firms[:5]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error combining Prophet forecasts: {e}\")\n",
    "    df_forecast['Sales_pred_prophet'] = np.nan\n",
    "\n",
    "print(\"Prophet models trained for all firms with simplified regressors\")\n",
    "print_memory_usage()\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# 9.3. OLS Model Predictions\n",
    "#-----------------------------------------------------------\n",
    "print(\"\\nIntegrating OLS model predictions...\")\n",
    "\n",
    "if 'Sales_pred_ols' not in df_forecast.columns:\n",
    "    try:\n",
    "        if 'firm_models' in globals() or 'firm_models' in locals():\n",
    "            print(\"Using firm-specific OLS models\")\n",
    "\n",
    "            df_forecast['Sales_pred_ols'] = np.nan\n",
    "\n",
    "            for firm_id, model in firm_models.items():\n",
    "                firm_mask = df_forecast['FirmID'] == firm_id\n",
    "                if firm_mask.sum() > 0:\n",
    "                    try:\n",
    "                        firm_forecast = df_forecast[firm_mask]\n",
    "                        df_forecast.loc[firm_mask, 'Sales_pred_ols'] = model.predict(firm_forecast)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error predicting with OLS model for Firm {firm_id}: {e}\")\n",
    "\n",
    "            if 'ols_model' in globals() or 'ols_model' in locals():\n",
    "                missing_mask = df_forecast['Sales_pred_ols'].isna()\n",
    "                if missing_mask.sum() > 0:\n",
    "                    print(f\"Using global OLS model for {missing_mask.sum()} rows without firm-specific predictions\")\n",
    "                    df_forecast.loc[missing_mask, 'Sales_pred_ols'] = ols_model.predict(df_forecast[missing_mask])\n",
    "        \n",
    "        elif 'ols_model' in globals() or 'ols_model' in locals():\n",
    "            print(\"Using global OLS model for all firms\")\n",
    "            df_forecast['Sales_pred_ols'] = ols_model.predict(df_forecast)\n",
    "        \n",
    "        else:\n",
    "            print(\"No OLS models available, skipping OLS predictions\")\n",
    "            df_forecast['Sales_pred_ols'] = np.nan\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating OLS predictions: {e}\")\n",
    "        df_forecast['Sales_pred_ols'] = np.nan\n",
    "else:\n",
    "    print(\"OLS predictions already present in forecast dataframe\")\n",
    "\n",
    "ols_coverage = (df_forecast['Sales_pred_ols'].notna().sum() / len(df_forecast)) * 100\n",
    "print(f\"OLS predictions available for {ols_coverage:.1f}% of forecast rows\")\n",
    "print_memory_usage()\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# 9.4. Ensemble Model: Combining Predictions\n",
    "#-----------------------------------------------------------\n",
    "print(\"\\nCreating ensemble forecast...\")\n",
    "\n",
    "models_to_ensemble = []\n",
    "for model_col in ['Sales_pred_ols', 'Sales_pred_lgbm', 'Sales_pred_prophet']:\n",
    "    if model_col in df_forecast.columns and df_forecast[model_col].notna().any():\n",
    "        models_to_ensemble.append(model_col)\n",
    "\n",
    "print(f\"Models available for ensemble: {models_to_ensemble}\")\n",
    "\n",
    "df_forecast['Sales_pred_ensemble'] = np.nan\n",
    "\n",
    "if 'ensemble_weights' in globals() or 'ensemble_weights' in locals():\n",
    "    print(\"Using weights from hyperparameter tuning for ensemble\")\n",
    "\n",
    "    for firm_id in df_forecast['FirmID'].unique():\n",
    "        firm_mask = df_forecast['FirmID'] == firm_id\n",
    "        \n",
    "        if firm_id in ensemble_weights and firm_id != 'global':\n",
    "            weights = ensemble_weights[firm_id]['weights']\n",
    "            print(f\"Using firm-specific weights for Firm {firm_id}: {weights}\")\n",
    "        else:\n",
    "            weights = ensemble_weights['global']['weights']\n",
    "\n",
    "        firm_preds = df_forecast.loc[firm_mask]\n",
    "\n",
    "        lgbm_available = 'Sales_pred_lgbm' in firm_preds.columns and not firm_preds['Sales_pred_lgbm'].isna().all()\n",
    "        prophet_available = 'Sales_pred_prophet' in firm_preds.columns and not firm_preds['Sales_pred_prophet'].isna().all()\n",
    "        \n",
    "        if lgbm_available and prophet_available:\n",
    "            df_forecast.loc[firm_mask, 'Sales_pred_ensemble'] = (\n",
    "                weights['lightgbm'] * firm_preds['Sales_pred_lgbm'] +\n",
    "                weights['prophet'] * firm_preds['Sales_pred_prophet']\n",
    "            )\n",
    "        elif lgbm_available:\n",
    "            df_forecast.loc[firm_mask, 'Sales_pred_ensemble'] = firm_preds['Sales_pred_lgbm']\n",
    "        elif prophet_available:\n",
    "            df_forecast.loc[firm_mask, 'Sales_pred_ensemble'] = firm_preds['Sales_pred_prophet']\n",
    "        else:\n",
    "            if 'Sales_pred_ols' in firm_preds.columns and not firm_preds['Sales_pred_ols'].isna().all():\n",
    "                df_forecast.loc[firm_mask, 'Sales_pred_ensemble'] = firm_preds['Sales_pred_ols']\n",
    "else:\n",
    "    print(\"No ensemble weights from hyperparameter tuning, using default weights\")\n",
    "\n",
    "    model_weights = {\n",
    "        'Sales_pred_ols': 0.1,      \n",
    "        'Sales_pred_lgbm': 0.4,     \n",
    "        'Sales_pred_prophet': 0.5,  \n",
    "    }\n",
    "\n",
    "    available_models = [m for m in model_weights.keys() if m in models_to_ensemble]\n",
    "    weights_sum = sum([model_weights[m] for m in available_models])\n",
    "    normalized_weights = {m: model_weights[m]/weights_sum for m in available_models}\n",
    "    \n",
    "    print(\"Default ensemble weights:\")\n",
    "    for model, weight in normalized_weights.items():\n",
    "        print(f\"  {model}: {weight:.2f}\")\n",
    "\n",
    "    df_forecast['Sales_pred_ensemble'] = 0.0\n",
    "    for model, weight in normalized_weights.items():\n",
    "        temp_values = df_forecast[model].fillna(0)\n",
    "        df_forecast['Sales_pred_ensemble'] += temp_values * weight\n",
    "\n",
    "    all_nan_mask = True\n",
    "    for model in available_models:\n",
    "        all_nan_mask = all_nan_mask & df_forecast[model].isna()\n",
    "    df_forecast.loc[all_nan_mask, 'Sales_pred_ensemble'] = np.nan\n",
    "\n",
    "ensemble_coverage = (df_forecast['Sales_pred_ensemble'].notna().sum() / len(df_forecast)) * 100\n",
    "print(f\"Ensemble predictions available for {ensemble_coverage:.1f}% of forecast rows\")\n",
    "print_memory_usage()\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# 9.5. Model Evaluation and Comparison\n",
    "#-----------------------------------------------------------\n",
    "print(\"\\nComparing model performance...\")\n",
    "\n",
    "if 'Sales' in df_forecast.columns and df_forecast['Sales'].notna().any():\n",
    "    metrics = {}\n",
    "    for model in ['Sales_pred_ols', 'Sales_pred_lgbm', 'Sales_pred_prophet', 'Sales_pred_ensemble']:\n",
    "        if model in df_forecast.columns and df_forecast[model].notna().any():\n",
    "            mask = df_forecast['Sales'].notna() & df_forecast[model].notna()\n",
    "            if mask.sum() > 0:\n",
    "                y_true = df_forecast.loc[mask, 'Sales']\n",
    "                y_pred = df_forecast.loc[mask, model]\n",
    "                \n",
    "                mae = mean_absolute_error(y_true, y_pred)\n",
    "                rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "                r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "                mape = np.mean(np.abs((y_true - y_pred) / y_true.clip(lower=1))) * 100  # Avoid div by 0\n",
    "                median_ae = median_absolute_error(y_true, y_pred)\n",
    "                \n",
    "                metrics[model] = {\n",
    "                    'MAE': mae, \n",
    "                    'RMSE': rmse, \n",
    "                    'R': r2,\n",
    "                    'MAPE': mape,\n",
    "                    'MedianAE': median_ae\n",
    "                }\n",
    "\n",
    "    print(\"\\nForecast period evaluation metrics:\")\n",
    "    print(f\"{'Model':<20} {'MAE':<12} {'RMSE':<12} {'R':<8} {'MAPE':<8} {'MedianAE':<8}\")\n",
    "    print(\"-\" * 65)\n",
    "    for model, values in metrics.items():\n",
    "        print(f\"{model:<20} {values['MAE']:<12.4f} {values['RMSE']:<12.4f} {values['R']:<8.4f} {values['MAPE']:<8.2f}% {values['MedianAE']:<8.4f}\")\n",
    "\n",
    "    if metrics:\n",
    "        best_model = min(metrics.items(), key=lambda x: x[1]['RMSE'])[0]\n",
    "        print(f\"\\nBest performing model: {best_model}\")\n",
    "        final_model = best_model\n",
    "    else:\n",
    "        print(\"No metrics available for evaluation, defaulting to ensemble\")\n",
    "        final_model = 'Sales_pred_ensemble'\n",
    "\n",
    "    try:\n",
    "        metrics_long = []\n",
    "        for model, metrics_dict in metrics.items():\n",
    "            for metric, value in metrics_dict.items():\n",
    "                if metric not in ['MAPE', 'MedianAE']:  \n",
    "                    metrics_long.append({\n",
    "                        'Model': model.replace('Sales_pred_', ''),\n",
    "                        'Metric': metric,\n",
    "                        'Value': value\n",
    "                    })\n",
    "        \n",
    "        metrics_df = pd.DataFrame(metrics_long)\n",
    "\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=(\"RMSE (lower is better)\", \"MAE (lower is better)\", \n",
    "                          \"R (higher is better)\", \"Model Comparison\"),\n",
    "            specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
    "                  [{\"type\": \"bar\"}, {\"type\": \"scatter\"}]]\n",
    "        )\n",
    "\n",
    "        rmse_data = metrics_df[metrics_df['Metric'] == 'RMSE']\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=rmse_data['Model'],\n",
    "                y=rmse_data['Value'],\n",
    "                marker_color='red',\n",
    "                name='RMSE',\n",
    "                text=[f\"{v:.2f}\" for v in rmse_data['Value']],\n",
    "                textposition=\"outside\"\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "\n",
    "        mae_data = metrics_df[metrics_df['Metric'] == 'MAE']\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=mae_data['Model'],\n",
    "                y=mae_data['Value'],\n",
    "                marker_color='blue',\n",
    "                name='MAE',\n",
    "                text=[f\"{v:.2f}\" for v in mae_data['Value']],\n",
    "                textposition=\"outside\"\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "\n",
    "        r2_data = metrics_df[metrics_df['Metric'] == 'R']\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=r2_data['Model'],\n",
    "                y=r2_data['Value'],\n",
    "                marker_color='green',\n",
    "                name='R',\n",
    "                text=[f\"{v:.3f}\" for v in r2_data['Value']],\n",
    "                textposition=\"outside\"\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "\n",
    "        model_names = list(metrics.keys())\n",
    "        model_display_names = [name.replace('Sales_pred_', '') for name in model_names]\n",
    "\n",
    "        max_rmse = max([m['RMSE'] for m in metrics.values()]) * 1.1 \n",
    "        max_mae = max([m['MAE'] for m in metrics.values()]) * 1.1\n",
    "        \n",
    "        normalized_metrics = []\n",
    "        for model in model_names:\n",
    "            normalized_metrics.append({\n",
    "                'Model': model.replace('Sales_pred_', ''),\n",
    "                'RMSE': 1 - (metrics[model]['RMSE'] / max_rmse),  \n",
    "                'MAE': 1 - (metrics[model]['MAE'] / max_mae),     \n",
    "                'R': max(0, metrics[model]['R'])               \n",
    "            })\n",
    "\n",
    "        for model_data in normalized_metrics:\n",
    "            model_name = model_data['Model']\n",
    "            fig.add_trace(\n",
    "                go.Scatterpolar(\n",
    "                    r=[model_data['RMSE'], model_data['MAE'], model_data['R']],\n",
    "                    theta=['RMSE', 'MAE', 'R'],\n",
    "                    fill='toself',\n",
    "                    name=model_name\n",
    "                ),\n",
    "                row=2, col=2\n",
    "            )\n",
    "\n",
    "        fig.update_layout(\n",
    "            polar=dict(\n",
    "                radialaxis=dict(\n",
    "                    visible=True,\n",
    "                    range=[0, 1]\n",
    "                )\n",
    "            ),\n",
    "            showlegend=True\n",
    "        )\n",
    "\n",
    "        best_model_name = best_model.replace('Sales_pred_', '')\n",
    "\n",
    "        for i, metric in enumerate(['RMSE', 'MAE', 'R']):\n",
    "            best_for_metric = min(metrics.items(), key=lambda x: x[1][metric])[0] if metric in ['RMSE', 'MAE'] else max(metrics.items(), key=lambda x: x[1][metric])[0]\n",
    "            best_name = best_for_metric.replace('Sales_pred_', '')\n",
    "            \n",
    "            row, col = (1, 1) if metric == 'RMSE' else ((1, 2) if metric == 'MAE' else (2, 1))\n",
    "            \n",
    "            fig.add_annotation(\n",
    "                x=best_name,\n",
    "                y=metrics[best_for_metric][metric] * 1.1,  # Position above the chart\n",
    "                text=\"Best for \" + metric,\n",
    "                showarrow=True,\n",
    "                arrowhead=1,\n",
    "                xref=f\"x{i+1}\" if i > 0 else \"x\",\n",
    "                yref=f\"y{i+1}\" if i > 0 else \"y\",\n",
    "                font=dict(size=12, color=\"green\")\n",
    "            )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            width=1000,\n",
    "            template='plotly_white',\n",
    "            title_text=\"Model Performance Comparison\",\n",
    "            showlegend=True\n",
    "        )\n",
    "        \n",
    "        fig.write_html('model_performance_comparison.html')\n",
    "        print(\"Model performance comparison visualization saved to 'model_performance_comparison.html'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating model comparison visualization: {e}\")    \n",
    "else:\n",
    "    print(\"No actual Sales data in forecast period for evaluation.\")\n",
    "    final_model = 'Sales_pred_ensemble'\n",
    "\n",
    "print_memory_usage()\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# 9.6. Time Series Forecast Visualization\n",
    "#-----------------------------------------------------------\n",
    "print(\"\\nCreating forecast visualizations...\")\n",
    "\n",
    "try:\n",
    "    vis_firms = np.random.choice(df_forecast['FirmID'].unique(), \n",
    "                                min(5, len(df_forecast['FirmID'].unique())), \n",
    "                                replace=False)\n",
    "\n",
    "    for firm_id in vis_firms:\n",
    "        try:\n",
    "            firm_train = df_train[df_train['FirmID'] == firm_id].copy().sort_values('Date')\n",
    "            firm_forecast = df_forecast[df_forecast['FirmID'] == firm_id].copy().sort_values('Date')\n",
    "\n",
    "            fig = make_subplots(\n",
    "                rows=2, cols=1, \n",
    "                shared_xaxes=True,\n",
    "                subplot_titles=(\"Sales Forecast\", \"Shopper Traffic\"),\n",
    "                vertical_spacing=0.1,\n",
    "                specs=[[{\"secondary_y\": False}], [{\"secondary_y\": True}]]\n",
    "            )\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=firm_train['Date'], \n",
    "                    y=firm_train['Sales'],\n",
    "                    mode='lines+markers',\n",
    "                    name='Actual Sales (Historical)',\n",
    "                    line=dict(color='black')\n",
    "                ),\n",
    "                row=1, col=1\n",
    "            )\n",
    "\n",
    "            model_colors = {\n",
    "                'Sales_pred_ols': 'blue',\n",
    "                'Sales_pred_lgbm': 'red',\n",
    "                'Sales_pred_prophet': 'green',\n",
    "                'Sales_pred_ensemble': 'orange'\n",
    "            }\n",
    "            \n",
    "            model_names = {\n",
    "                'Sales_pred_ols': 'OLS Baseline',\n",
    "                'Sales_pred_lgbm': 'LightGBM',\n",
    "                'Sales_pred_prophet': 'Prophet',\n",
    "                'Sales_pred_ensemble': 'Ensemble'\n",
    "            }\n",
    "            \n",
    "            for model, color in model_colors.items():\n",
    "                if model in firm_forecast.columns and firm_forecast[model].notna().any():\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=firm_forecast['Date'], \n",
    "                            y=firm_forecast[model],\n",
    "                            mode='lines+markers',\n",
    "                            name=model_names[model],\n",
    "                            line=dict(color=color)\n",
    "                        ),\n",
    "                        row=1, col=1\n",
    "                    )\n",
    "\n",
    "            if 'Sales' in firm_forecast.columns and firm_forecast['Sales'].notna().any():\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=firm_forecast['Date'], \n",
    "                        y=firm_forecast['Sales'],\n",
    "                        mode='lines+markers',\n",
    "                        name='Actual Sales (Forecast)',\n",
    "                        line=dict(color='black', dash='dash')\n",
    "                    ),\n",
    "                    row=1, col=1\n",
    "                )\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=firm_train['Date'], \n",
    "                    y=firm_train['FirmDailyTraffic'],\n",
    "                    mode='lines',\n",
    "                    name='Training Traffic',\n",
    "                    line=dict(color='darkblue')\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=firm_forecast['Date'], \n",
    "                    y=firm_forecast['FirmDailyTraffic'],\n",
    "                    mode='lines',\n",
    "                    name='Forecast Traffic',\n",
    "                    line=dict(color='darkblue', dash='dash')\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "\n",
    "            fig.add_vline(x=cutoff_date, line_dash=\"dash\", line_color=\"gray\", row=1, col=1)\n",
    "            fig.add_vline(x=cutoff_date, line_dash=\"dash\", line_color=\"gray\", row=2, col=1)\n",
    "\n",
    "            fig.update_layout(\n",
    "                title_text=f\"Sales Forecast and Traffic for Firm {firm_id}\",\n",
    "                height=800,\n",
    "                width=1200,\n",
    "                legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1),\n",
    "                template=\"plotly_white\"\n",
    "            )\n",
    "\n",
    "            fig.update_yaxes(title_text=\"Sales\", row=1, col=1)\n",
    "            fig.update_yaxes(title_text=\"Traffic\", row=2, col=1)\n",
    "            fig.update_xaxes(title_text=\"Date\", row=2, col=1)\n",
    "\n",
    "            fig.write_html(f\"forecast_firm{firm_id}.html\")\n",
    "            print(f\"Sales forecast visualization saved for firm {firm_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating visualization for firm {firm_id}: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in forecast visualization: {e}\")\n",
    "\n",
    "print_memory_usage()\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# 9.7. Create Combined Forecast Dashboard\n",
    "#-----------------------------------------------------------\n",
    "print(\"\\nCreating combined forecast dashboard...\")\n",
    "\n",
    "try:\n",
    "    if 'vis_firms' not in locals():\n",
    "\n",
    "        vis_firms = np.random.choice(df_forecast['FirmID'].unique(), \n",
    "                                   min(6, len(df_forecast['FirmID'].unique())), \n",
    "                                   replace=False)\n",
    "    \n",
    "    dashboard_firms = vis_firms[:min(6, len(vis_firms))]\n",
    "\n",
    "    agg_data = []\n",
    "\n",
    "    for firm_id in dashboard_firms:\n",
    "        firm_train = df_train[df_train['FirmID'] == firm_id].copy()\n",
    "        firm_forecast = df_forecast[df_forecast['FirmID'] == firm_id].copy()\n",
    "\n",
    "        for _, row in firm_train.iterrows():\n",
    "            agg_data.append({\n",
    "                'FirmID': firm_id,\n",
    "                'Date': row['Date'],\n",
    "                'Period': 'Training',\n",
    "                'Traffic': row['FirmDailyTraffic'],\n",
    "                'Actual_Sales': row['Sales'],\n",
    "                'OLS': None,\n",
    "                'LightGBM': None,\n",
    "                'Prophet': None,\n",
    "                'Ensemble': None\n",
    "            })\n",
    "\n",
    "        for _, row in firm_forecast.iterrows():\n",
    "            forecast_row = {\n",
    "                'FirmID': firm_id,\n",
    "                'Date': row['Date'],\n",
    "                'Period': 'Forecast',\n",
    "                'Traffic': row['FirmDailyTraffic'],\n",
    "                'Actual_Sales': row['Sales'] if 'Sales' in row and not pd.isna(row['Sales']) else None,\n",
    "                'OLS': row['Sales_pred_ols'] if 'Sales_pred_ols' in row else None,\n",
    "                'LightGBM': row['Sales_pred_lgbm'] if 'Sales_pred_lgbm' in row else None,\n",
    "                'Prophet': row['Sales_pred_prophet'] if 'Sales_pred_prophet' in row else None,\n",
    "                'Ensemble': row['Sales_pred_ensemble'] if 'Sales_pred_ensemble' in row else None\n",
    "            }\n",
    "            agg_data.append(forecast_row)\n",
    "\n",
    "    agg_df = pd.DataFrame(agg_data)\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=len(dashboard_firms), \n",
    "        cols=1,\n",
    "        subplot_titles=[f\"Firm {firm_id} Forecast\" for firm_id in dashboard_firms],\n",
    "        vertical_spacing=0.1,\n",
    "        shared_xaxes=True\n",
    "    )\n",
    "\n",
    "    row_idx = 1\n",
    "    for firm_id in dashboard_firms:\n",
    "        firm_data = agg_df[agg_df['FirmID'] == firm_id]\n",
    "\n",
    "        train_data = firm_data[firm_data['Period'] == 'Training']\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=train_data['Date'],\n",
    "                y=train_data['Actual_Sales'],\n",
    "                mode='lines',\n",
    "                name=f'Actual (Firm {firm_id})',\n",
    "                line=dict(color='black', width=2),\n",
    "                showlegend=(row_idx == 1) \n",
    "            ),\n",
    "            row=row_idx, col=1\n",
    "        )\n",
    "\n",
    "        forecast_data = firm_data[firm_data['Period'] == 'Forecast']\n",
    "\n",
    "        if not forecast_data['Actual_Sales'].isna().all():\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=forecast_data['Date'],\n",
    "                    y=forecast_data['Actual_Sales'],\n",
    "                    mode='lines+markers',\n",
    "                    name=f'Actual (Forecast)',\n",
    "                    line=dict(color='black', width=2, dash='dash'),\n",
    "                    showlegend=(row_idx == 1)\n",
    "                ),\n",
    "                row=row_idx, col=1\n",
    "            )\n",
    "\n",
    "        model_colors = {\n",
    "            'OLS': 'blue',\n",
    "            'LightGBM': 'red',\n",
    "            'Prophet': 'green',\n",
    "            'Ensemble': 'orange'\n",
    "        }\n",
    "        \n",
    "        for model, color in model_colors.items():\n",
    "            if model in forecast_data.columns and not forecast_data[model].isna().all():\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=forecast_data['Date'],\n",
    "                        y=forecast_data[model],\n",
    "                        mode='lines+markers',\n",
    "                        name=model,\n",
    "                        line=dict(color=color),\n",
    "                        showlegend=(row_idx == 1)\n",
    "                    ),\n",
    "                    row=row_idx, col=1\n",
    "                )\n",
    "\n",
    "        fig.add_vline(x=cutoff_date, line_dash=\"dash\", line_color=\"gray\", row=row_idx, col=1)\n",
    "\n",
    "        fig.update_yaxes(title_text=\"Sales\", row=row_idx, col=1)\n",
    "        \n",
    "        row_idx += 1\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text=\"Multi-Firm Sales Forecasting Dashboard\",\n",
    "        height=300 * len(dashboard_firms),\n",
    "        width=1200,\n",
    "        template=\"plotly_white\",\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(title_text=\"Date\", row=len(dashboard_firms), col=1)\n",
    "\n",
    "    fig.write_html(\"sales_forecast_dashboard.html\")\n",
    "    print(\"Combined sales forecast dashboard saved to 'sales_forecast_dashboard.html'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating combined forecast dashboard: {e}\")\n",
    "\n",
    "print_memory_usage()\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# 9.8. Create Final Forecast Output for Submission\n",
    "#-----------------------------------------------------------\n",
    "print(\"\\nPreparing final forecast output for submission...\")\n",
    "\n",
    "try:\n",
    "    if final_model not in df_forecast.columns:\n",
    "        print(f\"Warning: {final_model} not found in forecast dataframe. Falling back to ensemble.\")\n",
    "        final_model = 'Sales_pred_ensemble'\n",
    "\n",
    "    final_coverage = (df_forecast[final_model].notna().sum() / len(df_forecast)) * 100\n",
    "    print(f\"Final model ({final_model}) has predictions for {final_coverage:.1f}% of forecast rows\")\n",
    "\n",
    "    if final_coverage < 100:\n",
    "        print(\"Filling missing predictions using fallback models\")\n",
    "\n",
    "        df_forecast['Sales_pred_final'] = df_forecast[final_model]\n",
    "\n",
    "        fallback_models = [m for m in ['Sales_pred_ensemble', 'Sales_pred_lgbm', 'Sales_pred_prophet', 'Sales_pred_ols'] \n",
    "                          if m in df_forecast.columns and m != final_model]\n",
    "\n",
    "        for model in fallback_models:\n",
    "            missing_mask = df_forecast['Sales_pred_final'].isna()\n",
    "            if missing_mask.sum() > 0:\n",
    "                df_forecast.loc[missing_mask, 'Sales_pred_final'] = df_forecast.loc[missing_mask, model]\n",
    "                filled = missing_mask.sum() - df_forecast['Sales_pred_final'].isna().sum()\n",
    "                if filled > 0:\n",
    "                    print(f\"  Filled {filled} missing values using {model}\")\n",
    "    else:\n",
    "        df_forecast['Sales_pred_final'] = df_forecast[final_model]\n",
    "\n",
    "    still_missing = df_forecast['Sales_pred_final'].isna().sum()\n",
    "    if still_missing > 0:\n",
    "        print(f\"Warning: {still_missing} rows still have missing predictions.\")\n",
    "        print(\"Filling remaining missing values with the global median prediction\")\n",
    "\n",
    "        global_median = df_forecast['Sales_pred_final'].median()\n",
    "\n",
    "        df_forecast['Sales_pred_final'] = df_forecast['Sales_pred_final'].fillna(global_median)\n",
    "\n",
    "    sales_with_forecasts = df_sales.copy()\n",
    "\n",
    "    forecast_data = df_forecast[['FirmID', 'Date', 'Sales_pred_final']]\n",
    "    sales_with_forecasts = sales_with_forecasts.merge(\n",
    "        forecast_data,\n",
    "        on=['FirmID', 'Date'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    sales_with_forecasts['Sales'] = sales_with_forecasts['Sales'].fillna(\n",
    "        sales_with_forecasts['Sales_pred_final']\n",
    "    )\n",
    "\n",
    "    sales_with_forecasts = sales_with_forecasts.drop(columns=['Sales_pred_final'])\n",
    "\n",
    "    sales_with_forecasts.to_csv('Sales_with_forecasts.csv', index=False)\n",
    "    print(\"Final forecasts saved to 'Sales_with_forecasts.csv'\")\n",
    "\n",
    "    df_forecast[['FirmID', 'Date', 'Sales_pred_final']].to_csv('Forecasts_only.csv', index=False)\n",
    "    print(\"Forecast-only data saved to 'Forecasts_only.csv'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error preparing final forecast output: {e}\")\n",
    "\n",
    "print(\"\\nAll model training and visualization complete!\")\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d38092-7954-4401-87ed-aa7446a738a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
